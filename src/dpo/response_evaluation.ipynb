{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the Combined Evaluation Responses along with Question/Answer Pairs for Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('response_generation_data/all_model_responses.pkl', 'rb') as file:\n",
    "    all_model_responses = pickle.load(file)\n",
    "\n",
    "all_model_responses.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a new dataframe to record the evaluation results\n",
    "\n",
    "Calculating the word length of the response of each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_response_evaluation = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_response_evaluation['gpt_responses_base_len'] = all_model_responses['gpt_responses_base'].apply(lambda x: len(x.split()))\n",
    "df_response_evaluation['gpt_responses_ft_len'] = all_model_responses['gpt_responses_ft'].apply(lambda x: len(x.split()))\n",
    "df_response_evaluation['llama_responses_base_len'] = all_model_responses['llama_responses_base'].apply(lambda x: len(x.split()))\n",
    "df_response_evaluation['llama_responses_ft_len'] = all_model_responses['llama_responses_ft'].apply(lambda x: len(x.split()))\n",
    "df_response_evaluation.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI Configuration and Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../api.key\", 'r') as file:\n",
    "    openai_api_key = file.read()\n",
    "\n",
    "openai_client = OpenAI(api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_openai_response(system_prompt: str, user_prompt: str) -> str:\n",
    "        \n",
    "    completion = openai_client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    openai_response = completion.choices[0].message.content\n",
    "    \n",
    "    return openai_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the responses by GPT based on CTRS Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_eval = 'You are an expert mental-health counsellor'\n",
    "user_prompt_eval = '''You are given a conversation between a patient and therapist. Your job is to evaluate the response of therapist against the problem described by the patient as per the criterion of\n",
    "Understanding, Interpersonal Effectiveness, Collaboration, Guided Discovery, Focus and Strategy. The definition for each of these criterion is mentioned below:\n",
    "\n",
    "Understanding: How accurately does the therapist demonstrate understanding of the client’s issues and concerns?\n",
    "Interpersonal Effectiveness: How effective is the therapist in maintaining a positive and therapeutic relationship with the client?\n",
    "Collaboration: To what extent does the therapist engage the client in collaborative goalsetting and decision-making?\n",
    "Guided Discovery: How effectively does the therapist use guided discovery techniques to facilitate client self-reflection and insight?\n",
    "Focus: How well does the therapist identify and address the client’s key cognitions or behaviors that need change?\n",
    "Strategy: How appropriate and coherent is the therapist’s strategy for promoting change in the client’s problematic behaviors or thoughts?\n",
    "\n",
    "For each of these criterion, you need to assign a rating of 0 to 6 based on how well the therapist response fulfills the definition of the specific criterion.\n",
    "\n",
    "You final response must not contain any description about any criterion and it must ONLY be in a json serializable format as following {{\n",
    "    Understanding: understanding_rating,\n",
    "    Interpersonal Effectiveness: interpersonal_effectiveness_rating,\n",
    "    Collaboration: collaboration_rating,\n",
    "    Guided Discovery: guided_discovery_rating,\n",
    "    Focus: focus_rating,\n",
    "    Strategy: strategy_rating,\n",
    "}}\n",
    "\n",
    "[PATIENT Problem]:\n",
    "{patient}\n",
    "\n",
    "[THERAPIST Response]:\n",
    "{therapist}\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the Original Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_responses_eval = []\n",
    "\n",
    "for index, row in tqdm(all_model_responses.iterrows(), total=len(all_model_responses)):\n",
    "    \n",
    "    patient_problem = row['question']\n",
    "    therapist_response = row['answerText'] #Original Response\n",
    "    \n",
    "    response_eval = get_openai_response(system_prompt=system_prompt_eval, user_prompt=user_prompt_eval.format(patient=patient_problem, therapist=therapist_response))\n",
    "    \n",
    "    original_responses_eval.append(response_eval)\n",
    "\n",
    "with open('response_evaluation_data/original_responses_eval.pkl', 'wb') as file:\n",
    "    pickle.dump(original_responses_eval, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('response_evaluation_data/original_responses_eval.pkl', 'rb') as file:\n",
    "    original_responses_eval = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "understanding = []\n",
    "interpersonal_effectiveness  = []\n",
    "collaboration = []\n",
    "guided_discovery = []\n",
    "focus = []\n",
    "strategy = []\n",
    "\n",
    "for index, response in enumerate(original_responses_eval):\n",
    "    response = response.replace('json','').replace(\"```\",'')\n",
    "    try:\n",
    "        json_response = json.loads(response)\n",
    "        understanding.append(json_response['Understanding'])\n",
    "        interpersonal_effectiveness.append(json_response['Interpersonal Effectiveness'])\n",
    "        collaboration.append(json_response['Interpersonal Effectiveness'])\n",
    "        guided_discovery.append(json_response['Guided Discovery'])\n",
    "        focus.append(json_response['Focus'])\n",
    "        strategy.append(json_response['Strategy'])\n",
    "    except:\n",
    "        print(index)\n",
    "        understanding.append(None)\n",
    "        interpersonal_effectiveness.append(None)\n",
    "        collaboration.append(None)\n",
    "        guided_discovery.append(None)\n",
    "        focus.append(None)\n",
    "        strategy.append(None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_response_evaluation['undr_orig'] = understanding\n",
    "df_response_evaluation['inter_pers_orig'] = interpersonal_effectiveness\n",
    "df_response_evaluation['colab_orig'] = collaboration\n",
    "df_response_evaluation['guid_disc_orig'] = guided_discovery\n",
    "df_response_evaluation['foc_orig'] = focus\n",
    "df_response_evaluation['strat_orig'] = strategy\n",
    "df_response_evaluation.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the LLaMA Base Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_base_responses_eval = []\n",
    "\n",
    "for index, row in tqdm(all_model_responses.iterrows(), total=len(all_model_responses)):\n",
    "    \n",
    "    patient_problem = row['question']\n",
    "    therapist_response = row['llama_responses_base'] #LLaMA Response Base\n",
    "    \n",
    "    response_eval = get_openai_response(system_prompt=system_prompt_eval, user_prompt=user_prompt_eval.format(patient=patient_problem, therapist=therapist_response))\n",
    "    \n",
    "    llama_base_responses_eval.append(response_eval)\n",
    "\n",
    "with open('response_evaluation_data/llama_base_responses_eval.pkl', 'wb') as file:\n",
    "    pickle.dump(llama_base_responses_eval, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('response_evaluation_data/llama_base_responses_eval.pkl', 'rb') as file:\n",
    "    llama_base_responses_eval = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "understanding = []\n",
    "interpersonal_effectiveness  = []\n",
    "collaboration = []\n",
    "guided_discovery = []\n",
    "focus = []\n",
    "strategy = []\n",
    "\n",
    "for index, response in enumerate(llama_base_responses_eval):\n",
    "    response = response.replace('json','').replace(\"```\",'')\n",
    "    try:\n",
    "        json_response = json.loads(response)\n",
    "        understanding.append(json_response['Understanding'])\n",
    "        interpersonal_effectiveness.append(json_response['Interpersonal Effectiveness'])\n",
    "        collaboration.append(json_response['Interpersonal Effectiveness'])\n",
    "        guided_discovery.append(json_response['Guided Discovery'])\n",
    "        focus.append(json_response['Focus'])\n",
    "        strategy.append(json_response['Strategy'])\n",
    "    except:\n",
    "        print(index)\n",
    "        understanding.append(None)\n",
    "        interpersonal_effectiveness.append(None)\n",
    "        collaboration.append(None)\n",
    "        guided_discovery.append(None)\n",
    "        focus.append(None)\n",
    "        strategy.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_response_evaluation['undr_llama_base'] = understanding\n",
    "df_response_evaluation['inter_pers_llama_base'] = interpersonal_effectiveness\n",
    "df_response_evaluation['colab_llama_base'] = collaboration\n",
    "df_response_evaluation['guid_disc_llama_base'] = guided_discovery\n",
    "df_response_evaluation['foc_llama_base'] = focus\n",
    "df_response_evaluation['strat_llama_base'] = strategy\n",
    "df_response_evaluation.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the LLaMA Finetuned Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_ft_responses_eval = []\n",
    "\n",
    "for index, row in tqdm(all_model_responses.iterrows(), total=len(all_model_responses)):\n",
    "    \n",
    "    patient_problem = row['question']\n",
    "    therapist_response = row['llama_responses_ft'] #LLaMA Responses Finetuned\n",
    "    \n",
    "    response_eval = get_openai_response(system_prompt=system_prompt_eval, user_prompt=user_prompt_eval.format(patient=patient_problem, therapist=therapist_response))\n",
    "    \n",
    "    llama_ft_responses_eval.append(response_eval)\n",
    "\n",
    "with open('response_evaluation_data/llama_ft_responses_eval.pkl', 'wb') as file:\n",
    "    pickle.dump(llama_ft_responses_eval, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('response_evaluation_data/llama_ft_responses_eval.pkl', 'rb') as file:\n",
    "    llama_ft_responses_eval = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "understanding = []\n",
    "interpersonal_effectiveness  = []\n",
    "collaboration = []\n",
    "guided_discovery = []\n",
    "focus = []\n",
    "strategy = []\n",
    "\n",
    "for index, response in enumerate(llama_ft_responses_eval):\n",
    "    response = response.replace('json','').replace(\"```\",'')\n",
    "    try:\n",
    "        json_response = json.loads(response)\n",
    "        understanding.append(json_response['Understanding'])\n",
    "        interpersonal_effectiveness.append(json_response['Interpersonal Effectiveness'])\n",
    "        collaboration.append(json_response['Interpersonal Effectiveness'])\n",
    "        guided_discovery.append(json_response['Guided Discovery'])\n",
    "        focus.append(json_response['Focus'])\n",
    "        strategy.append(json_response['Strategy'])\n",
    "    except:\n",
    "        print(index)\n",
    "        understanding.append(None)\n",
    "        interpersonal_effectiveness.append(None)\n",
    "        collaboration.append(None)\n",
    "        guided_discovery.append(None)\n",
    "        focus.append(None)\n",
    "        strategy.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_response_evaluation['undr_llama_ft'] = understanding\n",
    "df_response_evaluation['inter_pers_llama_ft'] = interpersonal_effectiveness\n",
    "df_response_evaluation['colab_llama_ft'] = collaboration\n",
    "df_response_evaluation['guid_disc_llama_ft'] = guided_discovery\n",
    "df_response_evaluation['foc_llama_ft'] = focus\n",
    "df_response_evaluation['strat_llama_ft'] = strategy\n",
    "df_response_evaluation.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the Base GPT Reponses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_base_responses_eval = []\n",
    "\n",
    "for index, row in tqdm(all_model_responses.iterrows(), total=len(all_model_responses)):\n",
    "    \n",
    "    patient_problem = row['question']\n",
    "    therapist_response = row['gpt_responses_base'] #GPT Response Base\n",
    "    \n",
    "    response_eval = get_openai_response(system_prompt=system_prompt_eval, user_prompt=user_prompt_eval.format(patient=patient_problem, therapist=therapist_response))\n",
    "    \n",
    "    gpt_base_responses_eval.append(response_eval)\n",
    "\n",
    "with open('response_evaluation_data/gpt_base_responses_eval.pkl', 'wb') as file:\n",
    "    pickle.dump(gpt_base_responses_eval, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('response_evaluation_data/gpt_base_responses_eval.pkl', 'rb') as file:\n",
    "    gpt_base_responses_eval = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "understanding = []\n",
    "interpersonal_effectiveness  = []\n",
    "collaboration = []\n",
    "guided_discovery = []\n",
    "focus = []\n",
    "strategy = []\n",
    "\n",
    "for index, response in enumerate(gpt_base_responses_eval):\n",
    "    response = response.replace('json','').replace(\"```\",'')\n",
    "    try:\n",
    "        json_response = json.loads(response)\n",
    "        understanding.append(json_response['Understanding'])\n",
    "        interpersonal_effectiveness.append(json_response['Interpersonal Effectiveness'])\n",
    "        collaboration.append(json_response['Interpersonal Effectiveness'])\n",
    "        guided_discovery.append(json_response['Guided Discovery'])\n",
    "        focus.append(json_response['Focus'])\n",
    "        strategy.append(json_response['Strategy'])\n",
    "    except:\n",
    "        print(index)\n",
    "        understanding.append(None)\n",
    "        interpersonal_effectiveness.append(None)\n",
    "        collaboration.append(None)\n",
    "        guided_discovery.append(None)\n",
    "        focus.append(None)\n",
    "        strategy.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_response_evaluation['undr_gpt_base'] = understanding\n",
    "df_response_evaluation['inter_pers_gpt_base'] = interpersonal_effectiveness\n",
    "df_response_evaluation['colab_gpt_base'] = collaboration\n",
    "df_response_evaluation['guid_disc_gpt_base'] = guided_discovery\n",
    "df_response_evaluation['foc_gpt_base'] = focus\n",
    "df_response_evaluation['strat_gpt_base'] = strategy\n",
    "df_response_evaluation.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the GPT Fine-Tuned Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_ft_responses_eval = []\n",
    "\n",
    "for index, row in tqdm(all_model_responses.iterrows(), total=len(all_model_responses)):\n",
    "    \n",
    "    patient_problem = row['question']\n",
    "    therapist_response = row['gpt_responses_ft'] #GPT Response Finetuned\n",
    "    \n",
    "    response_eval = get_openai_response(system_prompt=system_prompt_eval, user_prompt=user_prompt_eval.format(patient=patient_problem, therapist=therapist_response))\n",
    "    \n",
    "    gpt_ft_responses_eval.append(response_eval)\n",
    "\n",
    "with open('response_evaluation_data/gpt_ft_responses_eval.pkl', 'wb') as file:\n",
    "    pickle.dump(gpt_ft_responses_eval, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('response_evaluation_data/gpt_ft_responses_eval.pkl', 'rb') as file:\n",
    "    gpt_ft_responses_eval = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "understanding = []\n",
    "interpersonal_effectiveness  = []\n",
    "collaboration = []\n",
    "guided_discovery = []\n",
    "focus = []\n",
    "strategy = []\n",
    "\n",
    "for index, response in enumerate(gpt_ft_responses_eval):\n",
    "    response = response.replace('json','').replace(\"```\",'')\n",
    "    try:\n",
    "        json_response = json.loads(response)\n",
    "        understanding.append(json_response['Understanding'])\n",
    "        interpersonal_effectiveness.append(json_response['Interpersonal Effectiveness'])\n",
    "        collaboration.append(json_response['Interpersonal Effectiveness'])\n",
    "        guided_discovery.append(json_response['Guided Discovery'])\n",
    "        focus.append(json_response['Focus'])\n",
    "        strategy.append(json_response['Strategy'])\n",
    "    except:\n",
    "        print(index)\n",
    "        understanding.append(None)\n",
    "        interpersonal_effectiveness.append(None)\n",
    "        collaboration.append(None)\n",
    "        guided_discovery.append(None)\n",
    "        focus.append(None)\n",
    "        strategy.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_response_evaluation['undr_gpt_ft'] = understanding\n",
    "df_response_evaluation['inter_pers_gpt_ft'] = interpersonal_effectiveness\n",
    "df_response_evaluation['colab_gpt_ft'] = collaboration\n",
    "df_response_evaluation['guid_disc_gpt_ft'] = guided_discovery\n",
    "df_response_evaluation['foc_gpt_ft'] = focus\n",
    "df_response_evaluation['strat_gpt_ft'] = strategy\n",
    "df_response_evaluation.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating comparisons of evaluation between different answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_df = df_response_evaluation[['topic','undr_orig', 'undr_llama_ft', 'undr_llama_base', 'undr_gpt_base', 'undr_gpt_ft',\n",
    "                                'inter_pers_orig', 'inter_pers_llama_ft', 'inter_pers_llama_base', 'inter_pers_gpt_base', 'inter_pers_gpt_ft',\n",
    "                                'colab_orig', 'colab_llama_ft', 'colab_llama_base', 'colab_gpt_base', 'colab_gpt_ft',\n",
    "                                'guid_disc_orig', 'guid_disc_llama_ft', 'guid_disc_llama_base', 'guid_disc_gpt_base', 'guid_disc_gpt_ft',\n",
    "                                'foc_orig', 'foc_llama_ft', 'foc_llama_base', 'foc_gpt_base', 'foc_gpt_ft',\n",
    "                                'strat_orig', 'strat_llama_ft', 'strat_llama_base', 'strat_gpt_base', 'strat_gpt_ft']].set_index('topic')\n",
    "\n",
    "viz_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_metrics = viz_df.agg('mean')\n",
    "avg_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,6))\n",
    "plt.bar(avg_metrics.index.to_list(), avg_metrics.values.tolist(), color=['r','b','g','k','c',\n",
    "                                                                         'r','b','g','k','c',\n",
    "                                                                         'r','b','g','k','c',\n",
    "                                                                         'r','b','g','k','c',\n",
    "                                                                         'r','b','g','k','c',\n",
    "                                                                         'r','b','g','k','c'])\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can fine-tune the evaluator. Question/Answer and then we provide the rating (he issue is that A question related to Anxiety might be more popular than some other topics)\n",
    "\n",
    "A pair-wise comparison could be better\n",
    "\n",
    "At any time, we provide two answers (one with most votes, one with least votes). We tell them which one's better. We can use all the samples and make more pairs\n",
    "\n",
    "For LLaMA and GPT, the length is affecting our results. So, we might want to restrict their length based on the average length of the answers in counselChat\n",
    "\n",
    "THe bias in COunselChat could come from long answers by the evaluators or voters themselves. Maybe we can test out the length of the answers having high votes versus low votes\n",
    "\n",
    "Print the average length of each of these"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mental-health-agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

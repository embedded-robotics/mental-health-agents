{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the Combined Evaluation Responses along with Question/Answer Pairs for Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('response_generation_data/all_model_responses.pkl', 'rb') as file:\n",
    "    all_model_responses = pickle.load(file)\n",
    "\n",
    "all_model_responses.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a new dataframe to record the evaluation results\n",
    "\n",
    "Calculating the word length of the response of each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_response_evaluation = pd.DataFrame(all_model_responses['topic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_response_evaluation['gpt_responses_base_len'] = all_model_responses['gpt_responses_base'].apply(lambda x: len(x.split()))\n",
    "df_response_evaluation['gpt_responses_ft_len'] = all_model_responses['gpt_responses_ft'].apply(lambda x: len(x.split()))\n",
    "df_response_evaluation['llama_responses_base_len'] = all_model_responses['llama_responses_base'].apply(lambda x: len(x.split()))\n",
    "df_response_evaluation['llama_responses_ft_len'] = all_model_responses['llama_responses_ft'].apply(lambda x: len(x.split()))\n",
    "df_response_evaluation.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI Configuration and Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../api.key\", 'r') as file:\n",
    "    openai_api_key = file.read()\n",
    "\n",
    "openai_client = OpenAI(api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_openai_response(system_prompt: str, user_prompt: str) -> str:\n",
    "        \n",
    "    completion = openai_client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    openai_response = completion.choices[0].message.content\n",
    "    \n",
    "    return openai_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the responses by GPT based on CTRS Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_eval = 'You are an expert mental-health counsellor'\n",
    "user_prompt_eval = '''You are given a conversation between a patient and therapist. Your job is to assess the response of therapist on a scale from 0 to 6 as per each of the following criterion.\n",
    "\n",
    "<<AGENDA>>\n",
    "0 = Therapist did not set agenda.\n",
    "2 = Therapist set agenda that was vague or incomplete.\n",
    "4 = Therapist worked with patient to set a mutually satisfactory agenda that included specific target problems (e.g., anxiety at work, dissatisfaction with marriage).\n",
    "6 = Therapist worked with patient to set an appropriate agenda with target problems, suitable for the available time. Established priorities and then followed agenda\n",
    "\n",
    "<<FEEDBACK>>\n",
    "0 = Therapist did not ask for feedback to determine patient’s understanding of, or response to, the session.\n",
    "2 = Therapist elicited some feedback from the patient, but did not ask enough questions to be sure the patient understood the therapist’s line of reasoning during the session or to ascertain whether the patient was satisfied with the session.\n",
    "4 = Therapist asked enough questions to be sure that the patient understood the therapist’s line of reasoning throughout the session and to determine the patient’s reactions to the session. The therapist adjusted his/her behavior in response to the feedback when appropriate.\n",
    "6 = Therapist was especially adept at eliciting and responding to verbal and nonverbal feedback throughout the session (e.g., elicited reactions to session, regularly checked for understanding, helped summarize main points at end of session).\n",
    "\n",
    "<<UNDERSTANDING>>\n",
    "0 = Therapist repeatedly failed to understand what the patient explicitly said and thus consistently missed the point. Poor empathic skills.\n",
    "2 = Therapist was usually able to reflect or rephrase what the patient explicitly said, but repeatedly failed to respond to more subtle communication. Limited ability to listen and empathize.\n",
    "4 = Therapist generally seemed to grasp the patient’s “internal reality” as reflected by both what the patient explicitly said and what the patient communicated in more subtle ways. Good ability to listen and empathize.\n",
    "6 = Therapist seemed to understand the patient’s “internal reality” thoroughly and was adept at communicating this understanding through appropriate verbal and nonverbal responses to the patient (e.g., the tone of the therapist’s response conveyed a sympathetic understanding of the patient’s “message”). Excellent listening and empathic skills.\n",
    "\n",
    "<<INTERPERSONAL EFFECTIVENESS>>\n",
    "0 = Therapist had poor interpersonal skills. Seemed hostile, demeaning, or in some other way destructive to the patient.\n",
    "2 = Therapist did not seem destructive, but had significant interpersonal problems. At times, therapist appeared unnecessarily impatient, aloof, insincere or had difficulty conveying confidence and competence.\n",
    "4 = Therapist displayed a satisfactory degree of warmth, concern, confidence, genuineness, and professionalism. No significant interpersonal problems.\n",
    "6 = Therapist displayed optimal levels of warmth, concern, confidence, genuineness, and professionalism, appropriate for this particular patient in this session.\n",
    "\n",
    "<<COLLABORATION>>\n",
    "0 = Therapist did not attempt to collaborate with patient.\n",
    "2 = Therapist attempted to collaborate with patient, but had difficulty either defining a problem that the patient considered important or establishing rapport.\n",
    "4 = Therapist was able to collaborate with patient, focus on a problem that both patient and therapist considered important, and establish rapport.\n",
    "6 = Collaboration seemed excellent; therapist encouraged patient as much as possible to take an active role during the session (e.g., by offering choices) so they could function as a “team.”\n",
    "\n",
    "<<PACING AND EFFICIENT USE OF TIME>>\n",
    "0 = Therapist made no attempt to structure therapy time. Session seemed aimless.\n",
    "2 = Session had some direction, but the therapist had significant problems with structuring or pacing (e.g., too little structure, inflexible about structure, too slowly paced, too rapidly paced).\n",
    "4 = Therapist was reasonably successful at using time efficiently. Therapist maintained appropriate control over flow of discussion and pacing.\n",
    "6 = Therapist used time efficiently by tactfully limiting peripheral and unproductive discussion and by pacing the session as rapidly as was appropriate for the patient.\n",
    "\n",
    "<<GUIDED DISCOVERY>>\n",
    "0 = Therapist relied primarily on debate, persuasion, or “lecturing.” Therapist seemed to be “cross-examining” patient, putting the patient on the defensive, or forcing his/her point of view on the patient.\n",
    "2 = Therapist relied too heavily on persuasion and debate, rather than guided discovery. However, therapist’s style was supportive enough that patient did not seem to feel attacked or defensive.\n",
    "4 = Therapist, for the most part, helped patient see new perspectives through guided discovery (e.g., examining evidence, considering alternatives, weighing advantages and disadvantages) rather than through debate. Used questioning appropriately.\n",
    "6 = Therapist was especially adept at using guided discovery during the session to explore problems and help patient draw his/ her own conclusions. Achieved an excellent balance between skillful questioning and other modes of intervention.\n",
    "\n",
    "<<FOCUSING ON KEY COGNITIONS OR BEHAVIORS>>\n",
    "0 = Therapist did not attempt to elicit specific thoughts, assumptions, images, meanings, or behaviors.\n",
    "2 = Therapist used appropriate techniques to elicit cognitions or behaviors; however, therapist had difficulty finding a focus or focused on cognitions/behaviors that were irrelevant to the patient’s key problems.\n",
    "4 = Therapist focused on specific cognitions or behaviors relevant to the target problem. However, therapist could have focused on more central cognitions or behaviors that offered greater promise for progress.\n",
    "6 = Therapist very skillfully focused on key thoughts, assumptions, behaviors, etc., that were most relevant to the problem area and offered considerable promise for progress.\n",
    "\n",
    "<<STRATEGY FOR CHANGE>> (Note: For this item, focus on the quality of the therapist’s strategy for change, not on how effectively the strategy was implemented or whether change actually occurred.)\n",
    "0 = Therapist did not select cognitive-behavioral techniques.\n",
    "2 = Therapist selected cognitive-behavioral techniques; however, either the overall strategy for bringing about change seemed vague or did not seem promising in helping the patient.\n",
    "4 = Therapist seemed to have a generally coherent strategy for change that showed reasonable promise and incorporated cognitive-behavioral techniques.\n",
    "6 = Therapist followed a consistent strategy for change that seemed very promising and incorporated the most appropriate cognitive-behavioral techniques.\n",
    "\n",
    "<<APPLICATION OF COGNITIVE-BEHAVIORAL TECHNIQUES>> (Note: For this item, focus on how skillfully the techniques were applied, not on how appropriate they were for the target problem or whether change actually occurred.)\n",
    "0 = Therapist did not apply any cognitive-behavioral techniques.\n",
    "2 = Therapist used cognitive-behavioral techniques, but there were significant flaws in the way they were applied.\n",
    "4 = Therapist applied cognitive-behavioral techniques with moderate skill.\n",
    "6 = Therapist very skillfully and resourcefully employed cognitive-behavioral techniques.\n",
    "\n",
    "<<HOMEWORK>>\n",
    "0 = Therapist did not attempt to incorporate homework relevant to cognitive therapy.\n",
    "2 = Therapist had significant difficulties incorporating homework (e.g., did not review previous homework, did not explain homework in sufficient detail, assigned inappropriate homework).\n",
    "4 = Therapist reviewed previous homework and assigned “standard” cognitive therapy homework generally relevant to issues dealt with in session. Homework was explained in sufficient detail.\n",
    "6 = Therapist reviewed previous homework and carefully assigned homework drawn from cognitive therapy for the coming week. Assignment seemed “custom-tailored” to help patient incorporate new perspectives, test hypotheses, experiment with new behaviors discussed during session, etc.\n",
    "\n",
    "For each criterion, descriptions are provided for even-numbered scale points. If you believe the therapist falls between two of the descriptors, select the intervening odd number (1, 3, 5). For example, if the therapist set a very good agenda but did not establish priorities, assign a rating of a 5 rather than a 4 or 6.\n",
    "\n",
    "Do not leave any criterion blank. For all criterion, focus on the skill of the therapist, taking into account how difficult the patient seems to be.\n",
    "\n",
    "You final response must not contain any description about any criterion and it must ONLY be in a json serializable format as following {{\n",
    "    <<AGENDA>>: <<AGENDA>> rating,\n",
    "    <<FEEDBACK>>: <<FEEDBACK>> rating,\n",
    "    <<UNDERSTANDING>>: <<UNDERSTANDING>> rating,\n",
    "    <<INTERPERSONAL EFFECTIVENESS>>: <<INTERPERSONAL EFFECTIVENESS>> rating,\n",
    "    <<COLLABORATION>>: <<COLLABORATION>> rating,\n",
    "    <<PACING AND EFFICIENT USE OF TIME>>: <<PACING AND EFFICIENT USE OF TIME>> rating,\n",
    "    <<GUIDED DISCOVERY>>: <<GUIDED DISCOVERY>> rating,\n",
    "    <<FOCUSING ON KEY COGNITIONS OR BEHAVIORS>>: <<FOCUSING ON KEY COGNITIONS OR BEHAVIORS>> rating, \n",
    "    <<STRATEGY FOR CHANGE>>: <<STRATEGY FOR CHANGE>> rating,\n",
    "    <<APPLICATION OF COGNITIVE-BEHAVIORAL TECHNIQUES>>: <<APPLICATION OF COGNITIVE-BEHAVIORAL TECHNIQUES>> rating,\n",
    "    <<HOMEWORK>>: <<HOMEWORK>> rating\n",
    "}}\n",
    "\n",
    "[PATIENT Problem]:\n",
    "{patient}\n",
    "\n",
    "[THERAPIST Response]:\n",
    "{therapist}\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the Original Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_responses_eval = []\n",
    "\n",
    "for index, row in tqdm(all_model_responses.iterrows(), total=len(all_model_responses)):\n",
    "    \n",
    "    patient_problem = row['question']\n",
    "    therapist_response = row['answerText'] #Original Response\n",
    "    \n",
    "    response_eval = get_openai_response(system_prompt=system_prompt_eval, user_prompt=user_prompt_eval.format(patient=patient_problem, therapist=therapist_response))\n",
    "    \n",
    "    original_responses_eval.append(response_eval)\n",
    "\n",
    "with open('response_evaluation_data/original_responses_eval.pkl', 'wb') as file:\n",
    "    pickle.dump(original_responses_eval, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('response_evaluation_data/original_responses_eval.pkl', 'rb') as file:\n",
    "    original_responses_eval = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# understanding = []\n",
    "# interpersonal_effectiveness  = []\n",
    "# collaboration = []\n",
    "# guided_discovery = []\n",
    "# focus = []\n",
    "# strategy = []\n",
    "\n",
    "# for index, response in enumerate(original_responses_eval):\n",
    "#     response = response.replace('json','').replace(\"```\",'')\n",
    "#     try:\n",
    "#         json_response = json.loads(response)\n",
    "#         understanding.append(json_response['Understanding'])\n",
    "#         interpersonal_effectiveness.append(json_response['Interpersonal Effectiveness'])\n",
    "#         collaboration.append(json_response['Interpersonal Effectiveness'])\n",
    "#         guided_discovery.append(json_response['Guided Discovery'])\n",
    "#         focus.append(json_response['Focus'])\n",
    "#         strategy.append(json_response['Strategy'])\n",
    "#     except:\n",
    "#         print(index)\n",
    "#         understanding.append(None)\n",
    "#         interpersonal_effectiveness.append(None)\n",
    "#         collaboration.append(None)\n",
    "#         guided_discovery.append(None)\n",
    "#         focus.append(None)\n",
    "#         strategy.append(None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_response_evaluation['undr_orig'] = understanding\n",
    "# df_response_evaluation['inter_pers_orig'] = interpersonal_effectiveness\n",
    "# df_response_evaluation['colab_orig'] = collaboration\n",
    "# df_response_evaluation['guid_disc_orig'] = guided_discovery\n",
    "# df_response_evaluation['foc_orig'] = focus\n",
    "# df_response_evaluation['strat_orig'] = strategy\n",
    "# df_response_evaluation.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the LLaMA Base Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_base_responses_eval = []\n",
    "\n",
    "for index, row in tqdm(all_model_responses.iterrows(), total=len(all_model_responses)):\n",
    "    \n",
    "    patient_problem = row['question']\n",
    "    therapist_response = row['llama_responses_base'] #LLaMA Response Base\n",
    "    \n",
    "    response_eval = get_openai_response(system_prompt=system_prompt_eval, user_prompt=user_prompt_eval.format(patient=patient_problem, therapist=therapist_response))\n",
    "    \n",
    "    llama_base_responses_eval.append(response_eval)\n",
    "\n",
    "with open('response_evaluation_data/llama_base_responses_eval.pkl', 'wb') as file:\n",
    "    pickle.dump(llama_base_responses_eval, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('response_evaluation_data/llama_base_responses_eval.pkl', 'rb') as file:\n",
    "    llama_base_responses_eval = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# understanding = []\n",
    "# interpersonal_effectiveness  = []\n",
    "# collaboration = []\n",
    "# guided_discovery = []\n",
    "# focus = []\n",
    "# strategy = []\n",
    "\n",
    "# for index, response in enumerate(llama_base_responses_eval):\n",
    "#     response = response.replace('json','').replace(\"```\",'')\n",
    "#     try:\n",
    "#         json_response = json.loads(response)\n",
    "#         understanding.append(json_response['Understanding'])\n",
    "#         interpersonal_effectiveness.append(json_response['Interpersonal Effectiveness'])\n",
    "#         collaboration.append(json_response['Interpersonal Effectiveness'])\n",
    "#         guided_discovery.append(json_response['Guided Discovery'])\n",
    "#         focus.append(json_response['Focus'])\n",
    "#         strategy.append(json_response['Strategy'])\n",
    "#     except:\n",
    "#         print(index)\n",
    "#         understanding.append(None)\n",
    "#         interpersonal_effectiveness.append(None)\n",
    "#         collaboration.append(None)\n",
    "#         guided_discovery.append(None)\n",
    "#         focus.append(None)\n",
    "#         strategy.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_response_evaluation['undr_llama_base'] = understanding\n",
    "# df_response_evaluation['inter_pers_llama_base'] = interpersonal_effectiveness\n",
    "# df_response_evaluation['colab_llama_base'] = collaboration\n",
    "# df_response_evaluation['guid_disc_llama_base'] = guided_discovery\n",
    "# df_response_evaluation['foc_llama_base'] = focus\n",
    "# df_response_evaluation['strat_llama_base'] = strategy\n",
    "# df_response_evaluation.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the LLaMA Finetuned Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_ft_responses_eval = []\n",
    "\n",
    "for index, row in tqdm(all_model_responses.iterrows(), total=len(all_model_responses)):\n",
    "    \n",
    "    patient_problem = row['question']\n",
    "    therapist_response = row['llama_responses_ft'] #LLaMA Responses Finetuned\n",
    "    \n",
    "    response_eval = get_openai_response(system_prompt=system_prompt_eval, user_prompt=user_prompt_eval.format(patient=patient_problem, therapist=therapist_response))\n",
    "    \n",
    "    llama_ft_responses_eval.append(response_eval)\n",
    "\n",
    "with open('response_evaluation_data/llama_ft_responses_eval.pkl', 'wb') as file:\n",
    "    pickle.dump(llama_ft_responses_eval, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('response_evaluation_data/llama_ft_responses_eval.pkl', 'rb') as file:\n",
    "    llama_ft_responses_eval = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# understanding = []\n",
    "# interpersonal_effectiveness  = []\n",
    "# collaboration = []\n",
    "# guided_discovery = []\n",
    "# focus = []\n",
    "# strategy = []\n",
    "\n",
    "# for index, response in enumerate(llama_ft_responses_eval):\n",
    "#     response = response.replace('json','').replace(\"```\",'')\n",
    "#     try:\n",
    "#         json_response = json.loads(response)\n",
    "#         understanding.append(json_response['Understanding'])\n",
    "#         interpersonal_effectiveness.append(json_response['Interpersonal Effectiveness'])\n",
    "#         collaboration.append(json_response['Interpersonal Effectiveness'])\n",
    "#         guided_discovery.append(json_response['Guided Discovery'])\n",
    "#         focus.append(json_response['Focus'])\n",
    "#         strategy.append(json_response['Strategy'])\n",
    "#     except:\n",
    "#         print(index)\n",
    "#         understanding.append(None)\n",
    "#         interpersonal_effectiveness.append(None)\n",
    "#         collaboration.append(None)\n",
    "#         guided_discovery.append(None)\n",
    "#         focus.append(None)\n",
    "#         strategy.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_response_evaluation['undr_llama_ft'] = understanding\n",
    "# df_response_evaluation['inter_pers_llama_ft'] = interpersonal_effectiveness\n",
    "# df_response_evaluation['colab_llama_ft'] = collaboration\n",
    "# df_response_evaluation['guid_disc_llama_ft'] = guided_discovery\n",
    "# df_response_evaluation['foc_llama_ft'] = focus\n",
    "# df_response_evaluation['strat_llama_ft'] = strategy\n",
    "# df_response_evaluation.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the Base GPT Reponses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_base_responses_eval = []\n",
    "\n",
    "for index, row in tqdm(all_model_responses.iterrows(), total=len(all_model_responses)):\n",
    "    \n",
    "    patient_problem = row['question']\n",
    "    therapist_response = row['gpt_responses_base'] #GPT Response Base\n",
    "    \n",
    "    response_eval = get_openai_response(system_prompt=system_prompt_eval, user_prompt=user_prompt_eval.format(patient=patient_problem, therapist=therapist_response))\n",
    "    \n",
    "    gpt_base_responses_eval.append(response_eval)\n",
    "\n",
    "with open('response_evaluation_data/gpt_base_responses_eval.pkl', 'wb') as file:\n",
    "    pickle.dump(gpt_base_responses_eval, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('response_evaluation_data/gpt_base_responses_eval.pkl', 'rb') as file:\n",
    "    gpt_base_responses_eval = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# understanding = []\n",
    "# interpersonal_effectiveness  = []\n",
    "# collaboration = []\n",
    "# guided_discovery = []\n",
    "# focus = []\n",
    "# strategy = []\n",
    "\n",
    "# for index, response in enumerate(gpt_base_responses_eval):\n",
    "#     response = response.replace('json','').replace(\"```\",'')\n",
    "#     try:\n",
    "#         json_response = json.loads(response)\n",
    "#         understanding.append(json_response['Understanding'])\n",
    "#         interpersonal_effectiveness.append(json_response['Interpersonal Effectiveness'])\n",
    "#         collaboration.append(json_response['Interpersonal Effectiveness'])\n",
    "#         guided_discovery.append(json_response['Guided Discovery'])\n",
    "#         focus.append(json_response['Focus'])\n",
    "#         strategy.append(json_response['Strategy'])\n",
    "#     except:\n",
    "#         print(index)\n",
    "#         understanding.append(None)\n",
    "#         interpersonal_effectiveness.append(None)\n",
    "#         collaboration.append(None)\n",
    "#         guided_discovery.append(None)\n",
    "#         focus.append(None)\n",
    "#         strategy.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_response_evaluation['undr_gpt_base'] = understanding\n",
    "# df_response_evaluation['inter_pers_gpt_base'] = interpersonal_effectiveness\n",
    "# df_response_evaluation['colab_gpt_base'] = collaboration\n",
    "# df_response_evaluation['guid_disc_gpt_base'] = guided_discovery\n",
    "# df_response_evaluation['foc_gpt_base'] = focus\n",
    "# df_response_evaluation['strat_gpt_base'] = strategy\n",
    "# df_response_evaluation.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the GPT Fine-Tuned Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_ft_responses_eval = []\n",
    "\n",
    "for index, row in tqdm(all_model_responses.iterrows(), total=len(all_model_responses)):\n",
    "    \n",
    "    patient_problem = row['question']\n",
    "    therapist_response = row['gpt_responses_ft'] #GPT Response Finetuned\n",
    "    \n",
    "    response_eval = get_openai_response(system_prompt=system_prompt_eval, user_prompt=user_prompt_eval.format(patient=patient_problem, therapist=therapist_response))\n",
    "    \n",
    "    gpt_ft_responses_eval.append(response_eval)\n",
    "\n",
    "with open('response_evaluation_data/gpt_ft_responses_eval.pkl', 'wb') as file:\n",
    "    pickle.dump(gpt_ft_responses_eval, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('response_evaluation_data/gpt_ft_responses_eval.pkl', 'rb') as file:\n",
    "    gpt_ft_responses_eval = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# understanding = []\n",
    "# interpersonal_effectiveness  = []\n",
    "# collaboration = []\n",
    "# guided_discovery = []\n",
    "# focus = []\n",
    "# strategy = []\n",
    "\n",
    "# for index, response in enumerate(gpt_ft_responses_eval):\n",
    "#     response = response.replace('json','').replace(\"```\",'')\n",
    "#     try:\n",
    "#         json_response = json.loads(response)\n",
    "#         understanding.append(json_response['Understanding'])\n",
    "#         interpersonal_effectiveness.append(json_response['Interpersonal Effectiveness'])\n",
    "#         collaboration.append(json_response['Interpersonal Effectiveness'])\n",
    "#         guided_discovery.append(json_response['Guided Discovery'])\n",
    "#         focus.append(json_response['Focus'])\n",
    "#         strategy.append(json_response['Strategy'])\n",
    "#     except:\n",
    "#         print(index)\n",
    "#         understanding.append(None)\n",
    "#         interpersonal_effectiveness.append(None)\n",
    "#         collaboration.append(None)\n",
    "#         guided_discovery.append(None)\n",
    "#         focus.append(None)\n",
    "#         strategy.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_response_evaluation['undr_gpt_ft'] = understanding\n",
    "# df_response_evaluation['inter_pers_gpt_ft'] = interpersonal_effectiveness\n",
    "# df_response_evaluation['colab_gpt_ft'] = collaboration\n",
    "# df_response_evaluation['guid_disc_gpt_ft'] = guided_discovery\n",
    "# df_response_evaluation['foc_gpt_ft'] = focus\n",
    "# df_response_evaluation['strat_gpt_ft'] = strategy\n",
    "# df_response_evaluation.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating comparisons of evaluation between different answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_df = df_response_evaluation[['topic','undr_orig', 'undr_llama_ft', 'undr_llama_base', 'undr_gpt_base', 'undr_gpt_ft',\n",
    "                                'inter_pers_orig', 'inter_pers_llama_ft', 'inter_pers_llama_base', 'inter_pers_gpt_base', 'inter_pers_gpt_ft',\n",
    "                                'colab_orig', 'colab_llama_ft', 'colab_llama_base', 'colab_gpt_base', 'colab_gpt_ft',\n",
    "                                'guid_disc_orig', 'guid_disc_llama_ft', 'guid_disc_llama_base', 'guid_disc_gpt_base', 'guid_disc_gpt_ft',\n",
    "                                'foc_orig', 'foc_llama_ft', 'foc_llama_base', 'foc_gpt_base', 'foc_gpt_ft',\n",
    "                                'strat_orig', 'strat_llama_ft', 'strat_llama_base', 'strat_gpt_base', 'strat_gpt_ft']].set_index('topic')\n",
    "\n",
    "viz_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_metrics = viz_df.agg('mean')\n",
    "avg_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,6))\n",
    "plt.bar(avg_metrics.index.to_list(), avg_metrics.values.tolist(), color=['r','b','g','k','c',\n",
    "                                                                         'r','b','g','k','c',\n",
    "                                                                         'r','b','g','k','c',\n",
    "                                                                         'r','b','g','k','c',\n",
    "                                                                         'r','b','g','k','c',\n",
    "                                                                         'r','b','g','k','c'])\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.xlabel('Metrics (Understanding, Interpersonal Effectiveness, Collaboation, Guided Discovery, Focus, Strategy)')\n",
    "plt.ylabel('Evaluation Scores (0-6)')\n",
    "plt.title('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can fine-tune the evaluator. Question/Answer and then we provide the rating (he issue is that A question related to Anxiety might be more popular than some other topics)\n",
    "\n",
    "A pair-wise comparison could be better\n",
    "\n",
    "At any time, we provide two answers (one with most votes, one with least votes). We tell them which one's better. We can use all the samples and make more pairs\n",
    "\n",
    "For LLaMA and GPT, the length is affecting our results. So, we might want to restrict their length based on the average length of the answers in counselChat\n",
    "\n",
    "THe bias in COunselChat could come from long answers by the evaluators or voters themselves. Maybe we can test out the length of the answers having high votes versus low votes\n",
    "\n",
    "Print the average length of each of these"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mental-health-agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the Combined Evaluation Responses along with Question/Answer Pairs for Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>question</th>\n",
       "      <th>answerText</th>\n",
       "      <th>gpt_responses_base</th>\n",
       "      <th>gpt_responses_ft</th>\n",
       "      <th>llama_responses_base</th>\n",
       "      <th>llama_responses_ft</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>relationships</td>\n",
       "      <td>I had to go to the emergency room today to get...</td>\n",
       "      <td>It is extremely frustrating when our significa...</td>\n",
       "      <td>I'm sorry to hear about your experience. It's ...</td>\n",
       "      <td>Be straightforward with how you felt at that s...</td>\n",
       "      <td>It's completely understandable that you're fee...</td>\n",
       "      <td>It's hard to understand why you're mad. It's p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>marriage</td>\n",
       "      <td>What makes a healthy marriage last? What makes...</td>\n",
       "      <td>This is a fantastic question. In one sentence,...</td>\n",
       "      <td>A healthy and lasting marriage often hinges on...</td>\n",
       "      <td>What a happy, healthy marriage looks or feels ...</td>\n",
       "      <td>A healthy marriage is built on a foundation of...</td>\n",
       "      <td>Good question! There are many things that can ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>relationships</td>\n",
       "      <td>I'm a female freshman in high school, and this...</td>\n",
       "      <td>First off, I think it is great that you are wi...</td>\n",
       "      <td>It's great that you want to support your frien...</td>\n",
       "      <td>Hi there. It sounds like you care a lot for Ti...</td>\n",
       "      <td>I'm glad you're taking the initiative to suppo...</td>\n",
       "      <td>Hi. You're asking a lot of questions, and I'll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>intimacy</td>\n",
       "      <td>My wife and I are newly married, about 2 month...</td>\n",
       "      <td>You are newly married, you Have a hectic sched...</td>\n",
       "      <td>Navigating intimacy issues in a new marriage c...</td>\n",
       "      <td>You have most of the knowledge you need alread...</td>\n",
       "      <td>I can sense the frustration and concern in you...</td>\n",
       "      <td>I think you're being overly critical of your w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>legal-regulatory</td>\n",
       "      <td>I think I have depression, anxiety, bipolar di...</td>\n",
       "      <td>It can be difficult to get counseling if you d...</td>\n",
       "      <td>I'm sorry to hear you're going through this. I...</td>\n",
       "      <td>There are a number of resources you can utiliz...</td>\n",
       "      <td>I'm so sorry to hear that you're struggling wi...</td>\n",
       "      <td>I'm so sorry to hear that you are struggling w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              topic                                           question  \\\n",
       "0     relationships  I had to go to the emergency room today to get...   \n",
       "1          marriage  What makes a healthy marriage last? What makes...   \n",
       "2     relationships  I'm a female freshman in high school, and this...   \n",
       "3          intimacy  My wife and I are newly married, about 2 month...   \n",
       "4  legal-regulatory  I think I have depression, anxiety, bipolar di...   \n",
       "\n",
       "                                          answerText  \\\n",
       "0  It is extremely frustrating when our significa...   \n",
       "1  This is a fantastic question. In one sentence,...   \n",
       "2  First off, I think it is great that you are wi...   \n",
       "3  You are newly married, you Have a hectic sched...   \n",
       "4  It can be difficult to get counseling if you d...   \n",
       "\n",
       "                                  gpt_responses_base  \\\n",
       "0  I'm sorry to hear about your experience. It's ...   \n",
       "1  A healthy and lasting marriage often hinges on...   \n",
       "2  It's great that you want to support your frien...   \n",
       "3  Navigating intimacy issues in a new marriage c...   \n",
       "4  I'm sorry to hear you're going through this. I...   \n",
       "\n",
       "                                    gpt_responses_ft  \\\n",
       "0  Be straightforward with how you felt at that s...   \n",
       "1  What a happy, healthy marriage looks or feels ...   \n",
       "2  Hi there. It sounds like you care a lot for Ti...   \n",
       "3  You have most of the knowledge you need alread...   \n",
       "4  There are a number of resources you can utiliz...   \n",
       "\n",
       "                                llama_responses_base  \\\n",
       "0  It's completely understandable that you're fee...   \n",
       "1  A healthy marriage is built on a foundation of...   \n",
       "2  I'm glad you're taking the initiative to suppo...   \n",
       "3  I can sense the frustration and concern in you...   \n",
       "4  I'm so sorry to hear that you're struggling wi...   \n",
       "\n",
       "                                  llama_responses_ft  \n",
       "0  It's hard to understand why you're mad. It's p...  \n",
       "1  Good question! There are many things that can ...  \n",
       "2  Hi. You're asking a lot of questions, and I'll...  \n",
       "3  I think you're being overly critical of your w...  \n",
       "4  I'm so sorry to hear that you are struggling w...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('response_generation_data/all_model_responses.pkl', 'rb') as file:\n",
    "    all_model_responses = pickle.load(file)\n",
    "\n",
    "all_model_responses.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a new dataframe to record the evaluation results\n",
    "\n",
    "Calculating the word length of the response of each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_response_evaluation = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gpt_responses_base_len</th>\n",
       "      <th>gpt_responses_ft_len</th>\n",
       "      <th>llama_responses_base_len</th>\n",
       "      <th>llama_responses_ft_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>186</td>\n",
       "      <td>175</td>\n",
       "      <td>197</td>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>211</td>\n",
       "      <td>282</td>\n",
       "      <td>225</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>209</td>\n",
       "      <td>204</td>\n",
       "      <td>333</td>\n",
       "      <td>217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>203</td>\n",
       "      <td>229</td>\n",
       "      <td>325</td>\n",
       "      <td>206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>192</td>\n",
       "      <td>159</td>\n",
       "      <td>207</td>\n",
       "      <td>162</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gpt_responses_base_len  gpt_responses_ft_len  llama_responses_base_len  \\\n",
       "0                     186                   175                       197   \n",
       "1                     211                   282                       225   \n",
       "2                     209                   204                       333   \n",
       "3                     203                   229                       325   \n",
       "4                     192                   159                       207   \n",
       "\n",
       "   llama_responses_ft_len  \n",
       "0                     189  \n",
       "1                      67  \n",
       "2                     217  \n",
       "3                     206  \n",
       "4                     162  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_response_evaluation['gpt_responses_base_len'] = all_model_responses['gpt_responses_base'].apply(lambda x: len(x.split()))\n",
    "df_response_evaluation['gpt_responses_ft_len'] = all_model_responses['gpt_responses_ft'].apply(lambda x: len(x.split()))\n",
    "df_response_evaluation['llama_responses_base_len'] = all_model_responses['llama_responses_base'].apply(lambda x: len(x.split()))\n",
    "df_response_evaluation['llama_responses_ft_len'] = all_model_responses['llama_responses_ft'].apply(lambda x: len(x.split()))\n",
    "df_response_evaluation.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI Configuration and Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../api.key\", 'r') as file:\n",
    "    openai_api_key = file.read()\n",
    "\n",
    "openai_client = OpenAI(api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_openai_response(system_prompt: str, user_prompt: str) -> str:\n",
    "        \n",
    "    completion = openai_client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    openai_response = completion.choices[0].message.content\n",
    "    \n",
    "    return openai_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the responses by GPT based on CTRS Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_eval = 'You are an expert mental-health counsellor'\n",
    "user_prompt_eval = '''You are given a conversation between a patient and therapist. Your job is to evaluate the response of therapist against the problem described by the patient as per the criterion of\n",
    "Understanding, Interpersonal Effectiveness, Collaboration, Guided Discovery, Focus and Strategy. The definition for each of these criterion is mentioned below:\n",
    "\n",
    "Understanding: How accurately does the therapist demonstrate understanding of the client’s issues and concerns?\n",
    "Interpersonal Effectiveness: How effective is the therapist in maintaining a positive and therapeutic relationship with the client?\n",
    "Collaboration: To what extent does the therapist engage the client in collaborative goalsetting and decision-making?\n",
    "Guided Discovery: How effectively does the therapist use guided discovery techniques to facilitate client self-reflection and insight?\n",
    "Focus: How well does the therapist identify and address the client’s key cognitions or behaviors that need change?\n",
    "Strategy: How appropriate and coherent is the therapist’s strategy for promoting change in the client’s problematic behaviors or thoughts?\n",
    "\n",
    "For each of these criterion, you need to assign a rating of 0 to 6 based on how well the therapist response fulfills the definition of the specific criterion.\n",
    "\n",
    "You final response must not contain any description about any criterion and it must ONLY be in a json serializable format as following {{\n",
    "    Understanding: understanding_rating,\n",
    "    Interpersonal Effectiveness: interpersonal_effectiveness_rating,\n",
    "    Collaboration: collaboration_rating,\n",
    "    Guided Discovery: guided_discovery_rating,\n",
    "    Focus: focus_rating,\n",
    "    Strategy: strategy_rating,\n",
    "}}\n",
    "\n",
    "[PATIENT Problem]:\n",
    "{patient}\n",
    "\n",
    "[THERAPIST Response]:\n",
    "{therapist}\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the Original Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_responses_eval = []\n",
    "\n",
    "for index, row in tqdm(all_model_responses.iterrows(), total=len(all_model_responses)):\n",
    "    \n",
    "    patient_problem = row['question']\n",
    "    therapist_response = row['answerText'] #Original Response\n",
    "    \n",
    "    response_eval = get_openai_response(system_prompt=system_prompt_eval, user_prompt=user_prompt_eval.format(patient=patient_problem, therapist=therapist_response))\n",
    "    \n",
    "    original_responses_eval.append(response_eval)\n",
    "\n",
    "with open('response_evaluation_data/original_responses_eval.pkl', 'wb') as file:\n",
    "    pickle.dump(original_responses_eval, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('response_evaluation_data/original_responses_eval.pkl', 'rb') as file:\n",
    "    original_responses_eval = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "understanding = []\n",
    "interpersonal_effectiveness  = []\n",
    "collaboration = []\n",
    "guided_discovery = []\n",
    "focus = []\n",
    "strategy = []\n",
    "\n",
    "for index, response in enumerate(original_responses_eval):\n",
    "    response = response.replace('json','').replace(\"```\",'')\n",
    "    try:\n",
    "        json_response = json.loads(response)\n",
    "        understanding.append(json_response['Understanding'])\n",
    "        interpersonal_effectiveness.append(json_response['Interpersonal Effectiveness'])\n",
    "        collaboration.append(json_response['Interpersonal Effectiveness'])\n",
    "        guided_discovery.append(json_response['Guided Discovery'])\n",
    "        focus.append(json_response['Focus'])\n",
    "        strategy.append(json_response['Strategy'])\n",
    "    except:\n",
    "        print(index)\n",
    "        understanding.append(None)\n",
    "        interpersonal_effectiveness.append(None)\n",
    "        collaboration.append(None)\n",
    "        guided_discovery.append(None)\n",
    "        focus.append(None)\n",
    "        strategy.append(None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_response_evaluation['undr_orig'] = understanding\n",
    "df_response_evaluation['inter_pers_orig'] = interpersonal_effectiveness\n",
    "df_response_evaluation['colab_orig'] = collaboration\n",
    "df_response_evaluation['guid_disc_orig'] = guided_discovery\n",
    "df_response_evaluation['foc_orig'] = focus\n",
    "df_response_evaluation['strat_orig'] = strategy\n",
    "df_response_evaluation.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the LLaMA Base Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_base_responses_eval = []\n",
    "\n",
    "for index, row in tqdm(all_model_responses.iterrows(), total=len(all_model_responses)):\n",
    "    \n",
    "    patient_problem = row['question']\n",
    "    therapist_response = row['llama_responses_base'] #LLaMA Response Base\n",
    "    \n",
    "    response_eval = get_openai_response(system_prompt=system_prompt_eval, user_prompt=user_prompt_eval.format(patient=patient_problem, therapist=therapist_response))\n",
    "    \n",
    "    llama_base_responses_eval.append(response_eval)\n",
    "\n",
    "with open('response_evaluation_data/llama_base_responses_eval.pkl', 'wb') as file:\n",
    "    pickle.dump(llama_base_responses_eval, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('response_evaluation_data/llama_base_responses_eval.pkl', 'rb') as file:\n",
    "    llama_base_responses_eval = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "understanding = []\n",
    "interpersonal_effectiveness  = []\n",
    "collaboration = []\n",
    "guided_discovery = []\n",
    "focus = []\n",
    "strategy = []\n",
    "\n",
    "for index, response in enumerate(llama_base_responses_eval):\n",
    "    response = response.replace('json','').replace(\"```\",'')\n",
    "    try:\n",
    "        json_response = json.loads(response)\n",
    "        understanding.append(json_response['Understanding'])\n",
    "        interpersonal_effectiveness.append(json_response['Interpersonal Effectiveness'])\n",
    "        collaboration.append(json_response['Interpersonal Effectiveness'])\n",
    "        guided_discovery.append(json_response['Guided Discovery'])\n",
    "        focus.append(json_response['Focus'])\n",
    "        strategy.append(json_response['Strategy'])\n",
    "    except:\n",
    "        print(index)\n",
    "        understanding.append(None)\n",
    "        interpersonal_effectiveness.append(None)\n",
    "        collaboration.append(None)\n",
    "        guided_discovery.append(None)\n",
    "        focus.append(None)\n",
    "        strategy.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_response_evaluation['undr_llama_base'] = understanding\n",
    "df_response_evaluation['inter_pers_llama_base'] = interpersonal_effectiveness\n",
    "df_response_evaluation['colab_llama_base'] = collaboration\n",
    "df_response_evaluation['guid_disc_llama_base'] = guided_discovery\n",
    "df_response_evaluation['foc_llama_base'] = focus\n",
    "df_response_evaluation['strat_llama_base'] = strategy\n",
    "df_response_evaluation.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the LLaMA Finetuned Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_ft_responses_eval = []\n",
    "\n",
    "for index, row in tqdm(all_model_responses.iterrows(), total=len(all_model_responses)):\n",
    "    \n",
    "    patient_problem = row['question']\n",
    "    therapist_response = row['llama_responses_ft'] #LLaMA Responses Finetuned\n",
    "    \n",
    "    response_eval = get_openai_response(system_prompt=system_prompt_eval, user_prompt=user_prompt_eval.format(patient=patient_problem, therapist=therapist_response))\n",
    "    \n",
    "    llama_ft_responses_eval.append(response_eval)\n",
    "\n",
    "with open('response_evaluation_data/llama_ft_responses_eval.pkl', 'wb') as file:\n",
    "    pickle.dump(llama_ft_responses_eval, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('response_evaluation_data/llama_ft_responses_eval.pkl', 'rb') as file:\n",
    "    llama_ft_responses_eval = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "understanding = []\n",
    "interpersonal_effectiveness  = []\n",
    "collaboration = []\n",
    "guided_discovery = []\n",
    "focus = []\n",
    "strategy = []\n",
    "\n",
    "for index, response in enumerate(llama_ft_responses_eval):\n",
    "    response = response.replace('json','').replace(\"```\",'')\n",
    "    try:\n",
    "        json_response = json.loads(response)\n",
    "        understanding.append(json_response['Understanding'])\n",
    "        interpersonal_effectiveness.append(json_response['Interpersonal Effectiveness'])\n",
    "        collaboration.append(json_response['Interpersonal Effectiveness'])\n",
    "        guided_discovery.append(json_response['Guided Discovery'])\n",
    "        focus.append(json_response['Focus'])\n",
    "        strategy.append(json_response['Strategy'])\n",
    "    except:\n",
    "        print(index)\n",
    "        understanding.append(None)\n",
    "        interpersonal_effectiveness.append(None)\n",
    "        collaboration.append(None)\n",
    "        guided_discovery.append(None)\n",
    "        focus.append(None)\n",
    "        strategy.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_response_evaluation['undr_llama_ft'] = understanding\n",
    "df_response_evaluation['inter_pers_llama_ft'] = interpersonal_effectiveness\n",
    "df_response_evaluation['colab_llama_ft'] = collaboration\n",
    "df_response_evaluation['guid_disc_llama_ft'] = guided_discovery\n",
    "df_response_evaluation['foc_llama_ft'] = focus\n",
    "df_response_evaluation['strat_llama_ft'] = strategy\n",
    "df_response_evaluation.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the Base GPT Reponses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_base_responses_eval = []\n",
    "\n",
    "for index, row in tqdm(all_model_responses.iterrows(), total=len(all_model_responses)):\n",
    "    \n",
    "    patient_problem = row['question']\n",
    "    therapist_response = row['gpt_responses_base'] #GPT Response Base\n",
    "    \n",
    "    response_eval = get_openai_response(system_prompt=system_prompt_eval, user_prompt=user_prompt_eval.format(patient=patient_problem, therapist=therapist_response))\n",
    "    \n",
    "    gpt_base_responses_eval.append(response_eval)\n",
    "\n",
    "with open('response_evaluation_data/gpt_base_responses_eval.pkl', 'wb') as file:\n",
    "    pickle.dump(gpt_base_responses_eval, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('response_evaluation_data/gpt_base_responses_eval.pkl', 'rb') as file:\n",
    "    gpt_base_responses_eval = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "understanding = []\n",
    "interpersonal_effectiveness  = []\n",
    "collaboration = []\n",
    "guided_discovery = []\n",
    "focus = []\n",
    "strategy = []\n",
    "\n",
    "for index, response in enumerate(gpt_base_responses_eval):\n",
    "    response = response.replace('json','').replace(\"```\",'')\n",
    "    try:\n",
    "        json_response = json.loads(response)\n",
    "        understanding.append(json_response['Understanding'])\n",
    "        interpersonal_effectiveness.append(json_response['Interpersonal Effectiveness'])\n",
    "        collaboration.append(json_response['Interpersonal Effectiveness'])\n",
    "        guided_discovery.append(json_response['Guided Discovery'])\n",
    "        focus.append(json_response['Focus'])\n",
    "        strategy.append(json_response['Strategy'])\n",
    "    except:\n",
    "        print(index)\n",
    "        understanding.append(None)\n",
    "        interpersonal_effectiveness.append(None)\n",
    "        collaboration.append(None)\n",
    "        guided_discovery.append(None)\n",
    "        focus.append(None)\n",
    "        strategy.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_response_evaluation['undr_gpt_base'] = understanding\n",
    "df_response_evaluation['inter_pers_gpt_base'] = interpersonal_effectiveness\n",
    "df_response_evaluation['colab_gpt_base'] = collaboration\n",
    "df_response_evaluation['guid_disc_gpt_base'] = guided_discovery\n",
    "df_response_evaluation['foc_gpt_base'] = focus\n",
    "df_response_evaluation['strat_gpt_base'] = strategy\n",
    "df_response_evaluation.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the GPT Fine-Tuned Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_ft_responses_eval = []\n",
    "\n",
    "for index, row in tqdm(all_model_responses.iterrows(), total=len(all_model_responses)):\n",
    "    \n",
    "    patient_problem = row['question']\n",
    "    therapist_response = row['gpt_responses_ft'] #GPT Response Finetuned\n",
    "    \n",
    "    response_eval = get_openai_response(system_prompt=system_prompt_eval, user_prompt=user_prompt_eval.format(patient=patient_problem, therapist=therapist_response))\n",
    "    \n",
    "    gpt_ft_responses_eval.append(response_eval)\n",
    "\n",
    "with open('response_evaluation_data/gpt_ft_responses_eval.pkl', 'wb') as file:\n",
    "    pickle.dump(gpt_ft_responses_eval, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('response_evaluation_data/gpt_ft_responses_eval.pkl', 'rb') as file:\n",
    "    gpt_ft_responses_eval = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "understanding = []\n",
    "interpersonal_effectiveness  = []\n",
    "collaboration = []\n",
    "guided_discovery = []\n",
    "focus = []\n",
    "strategy = []\n",
    "\n",
    "for index, response in enumerate(gpt_ft_responses_eval):\n",
    "    response = response.replace('json','').replace(\"```\",'')\n",
    "    try:\n",
    "        json_response = json.loads(response)\n",
    "        understanding.append(json_response['Understanding'])\n",
    "        interpersonal_effectiveness.append(json_response['Interpersonal Effectiveness'])\n",
    "        collaboration.append(json_response['Interpersonal Effectiveness'])\n",
    "        guided_discovery.append(json_response['Guided Discovery'])\n",
    "        focus.append(json_response['Focus'])\n",
    "        strategy.append(json_response['Strategy'])\n",
    "    except:\n",
    "        print(index)\n",
    "        understanding.append(None)\n",
    "        interpersonal_effectiveness.append(None)\n",
    "        collaboration.append(None)\n",
    "        guided_discovery.append(None)\n",
    "        focus.append(None)\n",
    "        strategy.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_response_evaluation['undr_gpt_ft'] = understanding\n",
    "df_response_evaluation['inter_pers_gpt_ft'] = interpersonal_effectiveness\n",
    "df_response_evaluation['colab_gpt_ft'] = collaboration\n",
    "df_response_evaluation['guid_disc_gpt_ft'] = guided_discovery\n",
    "df_response_evaluation['foc_gpt_ft'] = focus\n",
    "df_response_evaluation['strat_gpt_ft'] = strategy\n",
    "df_response_evaluation.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating comparisons of evaluation between different answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_df = df_response_evaluation[['topic','undr_orig', 'undr_llama_ft', 'undr_llama_base', 'undr_gpt_base', 'undr_gpt_ft',\n",
    "                                'inter_pers_orig', 'inter_pers_llama_ft', 'inter_pers_llama_base', 'inter_pers_gpt_base', 'inter_pers_gpt_ft',\n",
    "                                'colab_orig', 'colab_llama_ft', 'colab_llama_base', 'colab_gpt_base', 'colab_gpt_ft',\n",
    "                                'guid_disc_orig', 'guid_disc_llama_ft', 'guid_disc_llama_base', 'guid_disc_gpt_base', 'guid_disc_gpt_ft',\n",
    "                                'foc_orig', 'foc_llama_ft', 'foc_llama_base', 'foc_gpt_base', 'foc_gpt_ft',\n",
    "                                'strat_orig', 'strat_llama_ft', 'strat_llama_base', 'strat_gpt_base', 'strat_gpt_ft']].set_index('topic')\n",
    "\n",
    "viz_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_metrics = viz_df.agg('mean')\n",
    "avg_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,6))\n",
    "plt.bar(avg_metrics.index.to_list(), avg_metrics.values.tolist(), color=['r','b','g','k','c',\n",
    "                                                                         'r','b','g','k','c',\n",
    "                                                                         'r','b','g','k','c',\n",
    "                                                                         'r','b','g','k','c',\n",
    "                                                                         'r','b','g','k','c',\n",
    "                                                                         'r','b','g','k','c'])\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can fine-tune the evaluator. Question/Answer and then we provide the rating (he issue is that A question related to Anxiety might be more popular than some other topics)\n",
    "\n",
    "A pair-wise comparison could be better\n",
    "\n",
    "At any time, we provide two answers (one with most votes, one with least votes). We tell them which one's better. We can use all the samples and make more pairs\n",
    "\n",
    "For LLaMA and GPT, the length is affecting our results. So, we might want to restrict their length based on the average length of the answers in counselChat\n",
    "\n",
    "THe bias in COunselChat could come from long answers by the evaluators or voters themselves. Maybe we can test out the length of the answers having high votes versus low votes\n",
    "\n",
    "Print the average length of each of these"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mental-health-agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/mn27889/miniconda3/envs/mental-health/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from transformers.trainer import TrainingArguments\n",
    "from peft import LoraConfig, AutoPeftModelForCausalLM, get_peft_model, PeftModel\n",
    "from trl import SFTTrainer, DPOTrainer\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resolving data files: 100%|██████████| 72/72 [00:00<00:00, 520.76it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\n",
    "    path=\"lvwerra/stack-exchange-paired\",\n",
    "    split = \"train\"\n",
    ")\n",
    "\n",
    "dataset = dataset.shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hf_token.key', 'r') as f:\n",
    "    hf_token = f.read()\n",
    "\n",
    "base_model = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "new_model = \"llama-3-8b-stack-exchange\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model, padding='max_length', truncation=True, token = hf_token)\n",
    "# Adding a special token for pad token so that eos token can be recognized \n",
    "# (https://github.com/unslothai/unsloth/issues/416)\n",
    "# https://github.com/huggingface/transformers/issues/22794\n",
    "# https://github.com/huggingface/transformers/issues/23230\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"<|reserved_special_token_0|>\"})\n",
    "tokenizer.padding_side = \"right\"\n",
    "tokenizer.model_max_length = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_chat_template(row):\n",
    "    row_json = [\n",
    "        {\"role\" : \"user\", \"content\": row['question']},\n",
    "        {\"role\" : \"assistant\", \"content\": row['response_j']}\n",
    "    ]\n",
    "\n",
    "    row[\"text\"] = tokenizer.apply_chat_template(row_json, tokenize=False)\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(\n",
    "            format_chat_template,\n",
    "            num_proc=8\n",
    "        )\n",
    "\n",
    "dataset = dataset.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QLoRA Config for 4-bit quntization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "# # For 8 bit quantization\n",
    "# bnb_config = BitsAndBytesConfig(load_in_8bit=True, llm_int8_threshold=200.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config = bnb_config,\n",
    "    torch_dtype = torch.bfloat16,\n",
    "    device_map={'':torch.cuda.current_device()}\n",
    ")\n",
    "\n",
    "model.config.use_cache=False\n",
    "model.config.pad_token_id = tokenizer.pad_token_id # Updating the model config to use the special pad token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type = \"CAUSAL_LM\",\n",
    "    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    output_dir=new_model,\n",
    "    overwrite_output_dir=True,\n",
    "    bf16=True,\n",
    "    do_eval=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=0.1,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=5,\n",
    "    logging_strategy=\"steps\",\n",
    "    log_level=\"info\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    warmup_steps=10,\n",
    "    group_by_length=True,\n",
    "    report_to=\"none\",\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    dataset_text_field=\"text\",\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=tokenizer.model_max_length,\n",
    "    packing= False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging the base model with the adapter to get full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "new_model = \"llama-3-8b-stack-exchange\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model\n",
    "base_model_reload = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    return_dict=True,\n",
    "    low_cpu_mem_usage=True,    \n",
    "    trust_remote_code=True,\n",
    "    torch_dtype = torch.bfloat16,\n",
    "    device_map={\"\":torch.cuda.current_device()}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model, padding='max_length', truncation=True, token = hf_token)\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"<|reserved_special_token_0|>\"})\n",
    "tokenizer.padding_side = \"right\"\n",
    "tokenizer.model_max_length = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge adapter with the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PeftModel.from_pretrained(base_model_reload, new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.merge_and_unload()\n",
    "model.config.pad_token_id = tokenizer.pad_token_id # Updating the model config to use the special pad token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"llama-3-8b-stack-exchange-sft\")\n",
    "tokenizer.save_pretrained(\"llama-3-8b-stack-exchange-sft\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load merged Model and Tokenizer for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"llama-3-8b-stack-exchange-sft\",\n",
    "    torch_dtype = torch.bfloat16,\n",
    "    device_map={'':torch.cuda.current_device()}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"llama-3-8b-stack-exchange-sft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.use_cache = True\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Background: \\n\\nMy DB need to store YYYY-MM-DD and HH:MM:SS data generated from a Machine. Data will be inserted every few minutes, **every day** will have **Thousands of** records\\n\\nQuestions: \\n\\nShould I separate Date column into another table with DateID and DateName?\\n\\nWhat about time? HH:MM:SS, should it be another table or just a column?\\n\\nHow about query performance?? Should I index Date and Time Column with FK?\\n\\nWhat's the best practices for Date and Time stamp?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "\n",
    "generation_config = model.generation_config\n",
    "generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "generation_config.repetition_penalty = 1.5\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=1024,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    "    num_return_sequences=1,\n",
    "    generation_config=generation_config\n",
    ")\n",
    "\n",
    "text = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "\n",
    "print(text.split(\"assistant\")[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direct Preference Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer from pre-trained model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"llama-3-8b-stack-exchange-sft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QLoRA Config for 4-bit quntization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "# # For 8 bit quantization\n",
    "# bnb_config = BitsAndBytesConfig(load_in_8bit=True, llm_int8_threshold=200.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"llama-3-8b-stack-exchange-sft\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    quantization_config = bnb_config,\n",
    "    torch_dtype = torch.bfloat16,\n",
    "    device_map={'':torch.cuda.current_device()}\n",
    ")\n",
    "\n",
    "model.config.use_cache=False\n",
    "# model.config.pad_token_id = tokenizer.pad_token_id # Updating the model config to use the special pad token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type = \"CAUSAL_LM\",\n",
    "    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_chat_template_dpo(row):\n",
    "    row_json = [\n",
    "        {\"role\" : \"user\", \"content\": row['question']}\n",
    "    ]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(row_json, tokenize=False, add_generation_prompt=True)\n",
    "    chosen = row['response_j'] + tokenizer.eos_token\n",
    "    rejected = row['response_k'] + tokenizer.eos_token\n",
    "\n",
    "    return {\n",
    "        \"prompt\": prompt,\n",
    "        \"chosen\": chosen,\n",
    "        \"rejected\": rejected\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stack_exchange_paired(sanity_check=False, cache_dir=None, num_proc=24):\n",
    "    \n",
    "    dataset = load_dataset(\n",
    "        path=\"lvwerra/stack-exchange-paired\",\n",
    "        split = \"train\"\n",
    "    )\n",
    "\n",
    "    dataset = dataset.shuffle(seed=42).select(range(1000))\n",
    "\n",
    "    original_columns=dataset.column_names\n",
    "    \n",
    "    dataset = dataset.map(\n",
    "                    format_chat_template_dpo,\n",
    "                    num_proc=24,\n",
    "                    remove_columns=original_columns\n",
    "                )\n",
    "\n",
    "    dataset = dataset.train_test_split(test_size=0.1)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_stack_exchange_paired()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"llama-3-8b-stack-exchange-dpo\",\n",
    "    overwrite_output_dir=True,\n",
    "    bf16=True,\n",
    "    do_eval=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=0.1,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=5,\n",
    "    logging_strategy=\"steps\",\n",
    "    log_level=\"info\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    remove_unused_columns=False,\n",
    "    warmup_steps=10,\n",
    "    report_to=\"none\",\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = DPOTrainer(\n",
    "    model=model,\n",
    "    ref_model=None,\n",
    "    args=training_arguments,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    peft_config=peft_config,\n",
    "    max_prompt_length=tokenizer.model_max_length,\n",
    "    max_length=tokenizer.model_max_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(\"llama-3-8b-stack-exchange-dpo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging the base model with the adapter to get full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = \"llama-3-8b-stack-exchange-sft\"\n",
    "new_model = \"llama-3-8b-stack-exchange-dpo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model\n",
    "base_model_reload = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    return_dict=True,\n",
    "    low_cpu_mem_usage=True,    \n",
    "    trust_remote_code=True,\n",
    "    torch_dtype = torch.bfloat16,\n",
    "    device_map={\"\":torch.cuda.current_device()}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer from pre-trained model\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PeftModel.from_pretrained(base_model_reload, new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"llama-3-8b-stack-exchange-dpo-merged\")\n",
    "tokenizer.save_pretrained(\"llama-3-8b-stack-exchange-dpo-merged\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the merged model for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.81s/it]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"llama-3-8b-stack-exchange-dpo-merged\",\n",
    "    torch_dtype = torch.bfloat16,\n",
    "    device_map={'':torch.cuda.current_device()}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"llama-3-8b-stack-exchange-dpo-merged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|end_header_id|>\n",
      "\n",
      "You use the `SELECT` keyword and separate each column with commas:\n",
      "\n",
      "```\n",
      "  SELECT col1, \n",
      "         col2,\n",
      "         etc\n",
      "    FROM table_name;\n",
      "\n",
      "```  \n",
      "\n",
      "If you want to select all of them (as opposed to selecting some), just type out an asterisk: \n",
      "\n",
      "```\n",
      "   SELECT *  \n",
      "     from mytable;   \n",
      "\n",
      "```\n",
      "\n",
      "Edit:\n",
      "---\n",
      "\n",
      "The OP asked about [the syntax](http://msdn.microsoft.com/en-us/library/ms189499%28v=sql.105%29.aspx) which is used when there are more than one value specified.\n",
      "\n",
      "> The ALL or ANY operator must be preceded by =, < >, <= >=!= <> but not IN. You cannot combine these operators as follows : any!<>\n",
      "\n",
      "So if we have this data set :\n",
      "\n",
      "| id | val |\n",
      "---------+-----\n",
      "      0       A   \n",
      "        -3 B    \n",
      "          +4 C     \n",
      "           NULL D      \n",
      "             E      \n",
      "\n",
      "Then using \"IN\" will work fine ([SQL Fiddle Demo here ](https://www.sqlfiddle.com/#!6/d41d8ced59daa)\n",
      "\n",
      "[![enter image description here][10]][5]\n",
      "\n",
      "But trying it like so won't give us what most people expect... **Any** means that at least ONE element needs match our condition...\n",
      "\n",
      "* AND / OR don' t care how many elements do NOT meet your conditions.\n",
      "*\n",
      "\n",
      "And thus,\n",
      "\n",
      "> If @x int='A'\n",
      ">\n",
      ">> \n",
      ">>\n",
      ">>> declare @y sysname;\n",
      ">>>> SET NOCOUNT ON;\n",
      ">>>>>>> DECLARE y CURSOR FOR\n",
      ">>>>>>>>                SELECT distinct case WHEN 'B'=b THEN cast('Y' AS bit)\n",
      "<<<<<<<<<<<<<<<<<                 ELSE CAST(N'' AS XML).query('.')\n",
      ">>>>>>>>               END \n",
      ">>>>>>>>              As x,y INTO #temp From MyTable where b IS null Or ('C'<>'D')\n",
      ">>>>>>>> BEGIN TRY\n",
      ">>>>>>>>SET XACT_ABORT OFF            \n",
      ">>>>>>>>BEGIN TRAN T\n",
      " >>>>> >>             \n",
      "------------------------\n",
      "\n",
      "This gives me no error message because none need occur during execution time! But since every row has either `'E',NULL,'X','F`, then nothing matches `(CASE... ) When '' Then Cast....`. So neither does anything else except maybe something outside control flow (`RAISERROR`)!\n",
      "\n",
      "There's only two ways around such behavior :\n",
      "- Use subqueries instead...\n",
      "or\n",
      "Use common tables expressions! Which doesn't matter much whether they're defined within another function call though.\n",
      "\n",
      "\n",
      "\n",
      "Final Edit:\n",
      "\n",
      "I found why I couldn’t get “ANY” working… Because Oracle uses ANSI standard terminology on their site http://docs.oracle.com/cpljs/index.htm#STHRDT23613 whereas Microsoft’s documentation says otherwise https://support.microsoft.com/kb/q141904 – In short MSFTs Any == All while theirs equals Some\n",
      "\n",
      "\n",
      "\n",
      "Source:http://stackoverflow.com/questions/1759967/sql-any-vs-all-and-in-with-multiple-values\n",
      "\n",
      "Please correct yourself accordingly before voting down answers based solely upon personal experience without understanding WHY things happen differently between DBMS systems :) And please also consider reading up on different database management system types rather simply assuming everyone knows everything about yours!!! Your question was edited after mine answered therefore i did answer according to its initial state ;-) However feel free asking new questions regarding those topics anytime.)</p></pre><h3>Edit:</h3>\n",
      "\n",
      "It seems his problem really stems form lack off knowledge concerning boolean logic combined together inside statements involving logical operations (**AND**, **OR**) as well as confusion over equality checks vs inequality ones `<`,`>` compared against other comparison symbols `-=`,**`= ='**\n",
      "\n",
      "To explain briefly:**All values being true mean result=true***Some/all/maybe/noone know exactly howmany/values may fulfill given criteria -> result=True***\n",
      "\n",
      "He probably thinks he could somehow magically make oracle behave similarly although both databases follow exact same rules derived directly fro mathematical concepts involved[^1]. He should learn basic math first prior attempting complex problems &mdash;&gt; Learn Boolean algebra basics.[^1] Though possibly helpful sources would include [Wikipedia article linked above](http://en.wikipedia.org/wiki:Boolean_algebra) especially section titled ***Basic Operations.*** There exists even better resources online available however once knowing terms explained therein.)\n",
      "\n",
      "### Update following comments below ###\n",
      "\n",
      "\n",
      "Well thank goodness someone finally understood what went wrong&#8239;:-( It wasn&rsquo;t until now I realized myself too didn&lsquo;t understand correctly initially -- Thankyou very kindly pointing that part out!(Although still confused slightly see edit history).\n",
      "\n",
      "In simple words:&nbsp;%27ALL %22in&quot;&#160;;means&nbsp;<em>&ldquoevery single item among list provided MUST fit into rule described.&rdquo;</em>; While `%27SOME &#39;in)&quot;++---is equivalent++to+++(&amp;amp__;every_ _single_item_among_list_provided_MUST_fit_into_rule_described___but_at_least_one_must_match);`+\n",
      "\n",
      "As per request added info related to usage limitations:<br />\n",
      "<br />\n",
      "\n",
      "Here is taken straightly copied information (&copy;) from official\n"
     ]
    }
   ],
   "source": [
    "model.config.use_cache = True\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"How can I write a Select query for multiple columns in SQL?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "\n",
    "generation_config = model.generation_config\n",
    "generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "generation_config.repetition_penalty = 1.5\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=1024,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    "    num_return_sequences=1,\n",
    "    generation_config=generation_config\n",
    ")\n",
    "\n",
    "text = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "\n",
    "print(text.split(\"assistant\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "How can I write a Select query for multiple columns in SQL?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "You use the `SELECT` keyword and separate each column with commas:\n",
      "\n",
      "```\n",
      "  SELECT col1, \n",
      "         col2,\n",
      "         etc\n",
      "    FROM table_name;\n",
      "\n",
      "```  \n",
      "\n",
      "If you want to select all of them (as opposed to selecting some), just type out an asterisk: \n",
      "\n",
      "```\n",
      "   SELECT *  \n",
      "     from mytable;   \n",
      "\n",
      "```\n",
      "\n",
      "Edit:\n",
      "---\n",
      "\n",
      "The OP asked about [the syntax](http://msdn.microsoft.com/en-us/library/ms189499%28v=sql.105%29.aspx) which is used when there are more than one value specified.\n",
      "\n",
      "> The ALL or ANY operator must be preceded by =, < >, <= >=!= <> but not IN. You cannot combine these operators as follows : any!<>\n",
      "\n",
      "So if we have this data set :\n",
      "\n",
      "| id | val |\n",
      "---------+-----\n",
      "      0       A   \n",
      "        -3 B    \n",
      "          +4 C     \n",
      "           NULL D      \n",
      "             E      \n",
      "\n",
      "Then using \"IN\" will work fine ([SQL Fiddle Demo here ](https://www.sqlfiddle.com/#!6/d41d8ced59daa)\n",
      "\n",
      "[![enter image description here][10]][5]\n",
      "\n",
      "But trying it like so won't give us what most people expect... **Any** means that at least ONE element needs match our condition...\n",
      "\n",
      "* AND / OR don' t care how many elements do NOT meet your conditions.\n",
      "*\n",
      "\n",
      "And thus,\n",
      "\n",
      "> If @x int='A'\n",
      ">\n",
      ">> \n",
      ">>\n",
      ">>> declare @y sysname;\n",
      ">>>> SET NOCOUNT ON;\n",
      ">>>>>>> DECLARE y CURSOR FOR\n",
      ">>>>>>>>                SELECT distinct case WHEN 'B'=b THEN cast('Y' AS bit)\n",
      "<<<<<<<<<<<<<<<<<                 ELSE CAST(N'' AS XML).query('.')\n",
      ">>>>>>>>               END \n",
      ">>>>>>>>              As x,y INTO #temp From MyTable where b IS null Or ('C'<>'D')\n",
      ">>>>>>>> BEGIN TRY\n",
      ">>>>>>>>SET XACT_ABORT OFF            \n",
      ">>>>>>>>BEGIN TRAN T\n",
      " >>>>> >>             \n",
      "------------------------\n",
      "\n",
      "This gives me no error message because none need occur during execution time! But since every row has either `'E',NULL,'X','F`, then nothing matches `(CASE... ) When '' Then Cast....`. So neither does anything else except maybe something outside control flow (`RAISERROR`)!\n",
      "\n",
      "There's only two ways around such behavior :\n",
      "- Use subqueries instead...\n",
      "or\n",
      "Use common tables expressions! Which doesn't matter much whether they're defined within another function call though.\n",
      "\n",
      "\n",
      "\n",
      "Final Edit:\n",
      "\n",
      "I found why I couldn’t get “ANY” working… Because Oracle uses ANSI standard terminology on their site http://docs.oracle.com/cpljs/index.htm#STHRDT23613 whereas Microsoft’s documentation says otherwise https://support.microsoft.com/kb/q141904 – In short MSFTs Any == All while theirs equals Some\n",
      "\n",
      "\n",
      "\n",
      "Source:http://stackoverflow.com/questions/1759967/sql-any-vs-all-and-in-with-multiple-values\n",
      "\n",
      "Please correct yourself accordingly before voting down answers based solely upon personal experience without understanding WHY things happen differently between DBMS systems :) And please also consider reading up on different database management system types rather simply assuming everyone knows everything about yours!!! Your question was edited after mine answered therefore i did answer according to its initial state ;-) However feel free asking new questions regarding those topics anytime.)</p></pre><h3>Edit:</h3>\n",
      "\n",
      "It seems his problem really stems form lack off knowledge concerning boolean logic combined together inside statements involving logical operations (**AND**, **OR**) as well as confusion over equality checks vs inequality ones `<`,`>` compared against other comparison symbols `-=`,**`= ='**\n",
      "\n",
      "To explain briefly:**All values being true mean result=true***Some/all/maybe/noone know exactly howmany/values may fulfill given criteria -> result=True***\n",
      "\n",
      "He probably thinks he could somehow magically make oracle behave similarly although both databases follow exact same rules derived directly fro mathematical concepts involved[^1]. He should learn basic math first prior attempting complex problems &mdash;&gt; Learn Boolean algebra basics.[^1] Though possibly helpful sources would include [Wikipedia article linked above](http://en.wikipedia.org/wiki:Boolean_algebra) especially section titled ***Basic Operations.*** There exists even better resources online available however once knowing terms explained therein.)\n",
      "\n",
      "### Update following comments below ###\n",
      "\n",
      "\n",
      "Well thank goodness someone finally understood what went wrong&#8239;:-( It wasn&rsquo;t until now I realized myself too didn&lsquo;t understand correctly initially -- Thankyou very kindly pointing that part out!(Although still confused slightly see edit history).\n",
      "\n",
      "In simple words:&nbsp;%27ALL %22in&quot;&#160;;means&nbsp;<em>&ldquoevery single item among list provided MUST fit into rule described.&rdquo;</em>; While `%27SOME &#39;in)&quot;++---is equivalent++to+++(&amp;amp__;every_ _single_item_among_list_provided_MUST_fit_into_rule_described___but_at_least_one_must_match);`+\n",
      "\n",
      "As per request added info related to usage limitations:<br />\n",
      "<br />\n",
      "\n",
      "Here is taken straightly copied information (&copy;) from official\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mental-health",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

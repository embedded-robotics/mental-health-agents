{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/mn27889/miniconda3/envs/mental-health/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from transformers.trainer import TrainingArguments\n",
    "from peft import LoraConfig, AutoPeftModelForCausalLM, get_peft_model, prepare_model_for_int8_training, PeftModel\n",
    "from trl import SFTTrainer, DPOTrainer\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Glan Prompting Data in preference pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glan_prompt_answers = \"../data/glan_prompt_data/a/answers.json\"\n",
    "glan_prompt_questions = \"../data/glan_prompt_data/q/questions.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(glan_prompt_questions, 'rb') as file:\n",
    "    questions = json.load(file)\n",
    "    \n",
    "with open(glan_prompt_answers, 'rb') as file:\n",
    "    answers = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_list = []\n",
    "preferred_answer = []\n",
    "rejected_answer = []\n",
    "\n",
    "for key in questions.keys():\n",
    "    question_list.append(questions[key]['q'])\n",
    "    preferred_answer.append(answers[key][0]['better'])\n",
    "    rejected_answer.append(answers[key][0]['worse'])\n",
    "\n",
    "glan_data_pairs = {   \n",
    "                        'question': question_list,\n",
    "                        'preferred_answer': preferred_answer,\n",
    "                        'rejected_answer': rejected_answer\n",
    "                    }\n",
    "\n",
    "glan_dataset = Dataset.from_dict(glan_data_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the tokenizer and prepare the data in chat template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hf_token.key', 'r') as f:\n",
    "    hf_token = f.read()\n",
    "\n",
    "base_model = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "new_model = \"llama-3-8b-glan-sft\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model, padding='max_length', truncation=True, token = hf_token)\n",
    "# Adding a special token for pad token so that eos token can be recognized \n",
    "# (https://github.com/unslothai/unsloth/issues/416)\n",
    "# https://github.com/huggingface/transformers/issues/22794\n",
    "# https://github.com/huggingface/transformers/issues/23230\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"<|reserved_special_token_0|>\"})\n",
    "tokenizer.padding_side = \"right\"\n",
    "tokenizer.model_max_length = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_chat_template(row):\n",
    "    row_json = [\n",
    "        {\"role\" : \"user\", \"content\": row['question']},\n",
    "        {\"role\" : \"assistant\", \"content\": row['preferred_answer']}\n",
    "    ]\n",
    "\n",
    "    row[\"text\"] = tokenizer.apply_chat_template(row_json, tokenize=False)\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = glan_dataset.map(\n",
    "                        format_chat_template,\n",
    "                        num_proc=8\n",
    "                    )\n",
    "\n",
    "dataset = dataset.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QLoRA Config for 4-bit quntization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "# # For 8 bit quantization\n",
    "# bnb_config = BitsAndBytesConfig(load_in_8bit=True, llm_int8_threshold=200.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config = bnb_config,\n",
    "    torch_dtype = torch.bfloat16,\n",
    "    device_map={'':torch.cuda.current_device()}\n",
    ")\n",
    "\n",
    "model.config.use_cache=False\n",
    "model.config.pad_token_id = tokenizer.pad_token_id # Updating the model config to use the special pad token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type = \"CAUSAL_LM\",\n",
    "    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    output_dir=new_model,\n",
    "    overwrite_output_dir=True,\n",
    "    bf16=True,\n",
    "    do_eval=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=0.1,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=5,\n",
    "    logging_strategy=\"steps\",\n",
    "    log_level=\"info\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    warmup_steps=10,\n",
    "    group_by_length=True,\n",
    "    report_to=\"none\",\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    dataset_text_field=\"text\",\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=tokenizer.model_max_length,\n",
    "    packing= False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging the base model with the adapter to get full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "new_model = \"llama-3-8b-glan-sft\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model\n",
    "base_model_reload = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    return_dict=True,\n",
    "    low_cpu_mem_usage=True,    \n",
    "    trust_remote_code=True,\n",
    "    torch_dtype = torch.bfloat16,\n",
    "    device_map={\"\":torch.cuda.current_device()}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model, padding='max_length', truncation=True, token = hf_token)\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"<|reserved_special_token_0|>\"})\n",
    "tokenizer.padding_side = \"right\"\n",
    "tokenizer.model_max_length = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge adapter with the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PeftModel.from_pretrained(base_model_reload, new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.merge_and_unload()\n",
    "model.config.pad_token_id = tokenizer.pad_token_id # Updating the model config to use the special pad token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"llama-3-8b-glan-sft-merged\")\n",
    "tokenizer.save_pretrained(\"llama-3-8b-glan-sft-merged\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load merged Model and Tokenizer for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.65s/it]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"llama-3-8b-glan-sft-merged\",\n",
    "    torch_dtype = torch.bfloat16,\n",
    "    device_map={'':torch.cuda.current_device()}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"llama-3-8b-glan-sft-merged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|end_header_id|>\n",
      "\n",
      "Breaking up from a relationship can be incredibly painful and lead to significant emotional trauma. It's important to allow yourself time to grieve the loss of the partnership while also focusing on self-care activities that promote healing such as:\n",
      "\n",
      "1. Seeking support: Reach out to trusted friends, family members or join a support group for people going through similar experiences.\n",
      "2. Processing your emotions: Write in a journal, engage in expressive arts therapy like painting or writing poetry about how you feel without judgment.\n",
      "\n",
      "3.Celebrate past memories positively rather than dwelling negative aspects of ex-partnership which may trigger further pain\n",
      "4.Set boundaries around social media use related to former partners \n",
      "5.Focus now more forward by setting goals and working towards personal growth & development \n",
      "\n",
      "Remember it takes different amount times heal process so patience compassion understanding is key during this journey back independence empowerment You deserve love care too! If needed consider seeking professional help therapist counselor guidance throughout recovery. #BreakUpTraumaSupport #HealingJourney<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "model.config.use_cache = True\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"How do I overcome emotional trauma after breaking up with my partner?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "\n",
    "generation_config = model.generation_config\n",
    "generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "generation_config.repetition_penalty = 1.5\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=512,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    "    num_return_sequences=1,\n",
    "    generation_config=generation_config\n",
    ")\n",
    "\n",
    "text = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "\n",
    "print(text.split(\"assistant\")[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direct Preference Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = \"llama-3-8b-glan-sft-merged\"\n",
    "new_model = \"llama-3-8b-glan-dpo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer from pre-trained model\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QLoRA Config for 4-bit quntization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "# # For 8 bit quantization\n",
    "# bnb_config = BitsAndBytesConfig(load_in_8bit=True, llm_int8_threshold=200.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    low_cpu_mem_usage=True,\n",
    "    quantization_config = bnb_config,\n",
    "    torch_dtype = torch.bfloat16,\n",
    "    device_map={'':torch.cuda.current_device()}\n",
    ")\n",
    "\n",
    "model.config.use_cache=False\n",
    "# model.config.pad_token_id = tokenizer.pad_token_id # Updating the model config to use the special pad token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type = \"CAUSAL_LM\",\n",
    "    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_chat_template_dpo(row):\n",
    "    row_json = [\n",
    "        {\"role\" : \"user\", \"content\": row['question']}\n",
    "    ]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(row_json, tokenize=False, add_generation_prompt=True)\n",
    "    chosen = str(row['preferred_answer']) + tokenizer.eos_token\n",
    "    rejected = str(row['rejected_answer']) + tokenizer.eos_token\n",
    "\n",
    "    return {\n",
    "        \"prompt\": prompt,\n",
    "        \"chosen\": chosen,\n",
    "        \"rejected\": rejected\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_counsel_chat_paired(sanity_check=False, cache_dir=None, num_proc=24):\n",
    "\n",
    "    original_columns=glan_dataset.column_names\n",
    "    \n",
    "    dataset = glan_dataset.map(\n",
    "                        format_chat_template_dpo,\n",
    "                        num_proc=24,\n",
    "                        remove_columns=original_columns\n",
    "                    )\n",
    "\n",
    "    dataset = dataset.train_test_split(test_size=0.1)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_counsel_chat_paired()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    output_dir=new_model,\n",
    "    overwrite_output_dir=True,\n",
    "    bf16=True,\n",
    "    do_eval=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=0.1,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=5,\n",
    "    logging_strategy=\"steps\",\n",
    "    log_level=\"info\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    remove_unused_columns=False,\n",
    "    warmup_steps=10,\n",
    "    report_to=\"none\",\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = DPOTrainer(\n",
    "    model=model,\n",
    "    ref_model=None,\n",
    "    args=training_arguments,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    peft_config=peft_config,\n",
    "    max_prompt_length=1024,\n",
    "    max_length=1024\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging the base model with the adapter to get full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = \"llama-3-8b-glan-sft-merged\"\n",
    "new_model = \"llama-3-8b-glan-dpo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model\n",
    "base_model_reload = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    return_dict=True,\n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype = torch.bfloat16,\n",
    "    device_map={\"\":torch.cuda.current_device()}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer from pre-trained model\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PeftModel.from_pretrained(base_model_reload, new_model)\n",
    "model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"llama-3-8b-glan-dpo-merged\")\n",
    "tokenizer.save_pretrained(\"llama-3-8b-glan-dpo-merged\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the merged model for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.30s/it]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"llama-3-8b-glan-dpo-merged\",\n",
    "    torch_dtype = torch.bfloat16,\n",
    "    device_map={'':torch.cuda.current_device()}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"llama-3-8b-glan-dpo-merged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|end_header_id|>\n",
      "\n",
      "Overcoming emotional trauma after a breakup can be a challenging and painful process, but it is absolutely possible with the right support and guidance. Some strategies that may help you heal include:\n",
      "\n",
      "1. Allow yourself to grieve the loss: It's important to acknowledge and process the feelings of loss and grief that come with the end of a relationship.\n",
      "2. Seek a safe space: Surround yourself with a supportive network of friends and family, or consider seeking therapy to provide a safe space for your emotions.\n",
      "3. Practice self-care: Take care of your physical and emotional needs by ensuring you get enough sleep, eating healthily, and engaging in regular exercise.\n",
      "4. Process your emotions: Write in a journal, create art, or engage in any other activity that helps you express and make sense of your emotions.\n",
      "5. Give yourself time: Healing from emotional trauma takes time, so be patient and compassionate with yourself throughout the journey.\n",
      "6. Consider therapy: A therapist can provide you with a safe and non-judgmental space to explore your feelings, identify any underlying patterns or triggers, and develop coping strategies to help you move forward.\n",
      "\n",
      "Remember, healing from emotional trauma is a unique and individual journey, and it's okay to seek help and support along the way.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "model.config.use_cache = True\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"How do I overcome emotional trauma after breaking up with my partner?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "\n",
    "generation_config = model.generation_config\n",
    "generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "# generation_config.repetition_penalty = 1.5\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=512,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    "    num_return_sequences=1,\n",
    "    generation_config=generation_config\n",
    ")\n",
    "\n",
    "text = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "\n",
    "print(text.split(\"assistant\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mental-health",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

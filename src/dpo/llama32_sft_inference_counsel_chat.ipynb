{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the Counsel Chat Dataset\n",
    "\n",
    "Extract 5 questions from each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"nbertagnolli/counsel-chat\"\n",
    "dataset = load_dataset(dataset_name, split=\"all\")\n",
    "dataset = dataset.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df = dataset.to_pandas()\n",
    "dataset_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df_filt = dataset_df[['topic', 'questionTitle', 'questionText', 'answerText', 'upvotes']].groupby('topic', group_keys=False).apply(lambda x: x.sort_values(['upvotes'], ascending=False)[:5]).reset_index(drop=True)\n",
    "dataset_df_filt = dataset_df_filt.fillna('')\n",
    "dataset_df_filt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use OpenAI to generate synthetic data\n",
    "\n",
    "We need to generate a question pair from OpenAI by giving a relevant example from the CounselChat Dataset as one-shot instruction tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../api.key\", 'r') as file:\n",
    "    openai_api_key = file.read()\n",
    "    \n",
    "openai_client = OpenAI(api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_openai_response(system_prompt: str, user_prompt: str) -> str:\n",
    "    \n",
    "    completion = openai_client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=1,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    openai_response = completion.choices[0].message.content\n",
    "    \n",
    "    return openai_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = 'You are an expert mental-health counsellor'\n",
    "user_prompt = '''You are given a broad topic which covers a specific area in which humans suffer from ill mental health.\n",
    "You job is to generate a topic relevant question/answer pair with question describing the mental state of the patient and answer describing the counselling advice given to the patient.\n",
    "\n",
    "Topic: {topic}\n",
    "\n",
    "Example:\n",
    "Question-> {question}\n",
    "Answer-> {answer}\n",
    "\n",
    "You must return response in a json serializable format as following {{question: question_text, answer:answer_text}}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# openai_responses = []\n",
    "\n",
    "# for index, row in tqdm(dataset_df_filt.iterrows(), total=len(dataset_df_filt)):\n",
    "    \n",
    "#     topic = row['topic']\n",
    "#     question = row['questionText'] + row['questionTitle']\n",
    "#     answer = row['answerText']\n",
    "    \n",
    "#     response = get_openai_response(system_prompt=system_prompt, user_prompt=user_prompt.format(topic=topic, question=question, answer=answer))\n",
    "    \n",
    "#     openai_responses.append(response)\n",
    "\n",
    "# with open('openai_response.pkl', 'wb') as file:\n",
    "#     pickle.dump(openai_responses, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('openai_response.pkl', 'rb') as file:\n",
    "    openai_responses = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference from already fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"llama32-sft-fine-tune-counselchat\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.model_max_length = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\") # Must be float32 for MacBooks!\n",
    "model.config.pad_token_id = tokenizer.pad_token_id # Updating the model config to use the special pad token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input0 = [{\"role\": \"user\", \"content\": dataset[0]['questionText']}]\n",
    "input1 = [{\"role\": \"user\", \"content\": dataset[1]['questionText']}]\n",
    "input2 = [{\"role\": \"user\", \"content\": dataset[2]['questionText']}]\n",
    "input3 = [{\"role\": \"user\", \"content\": dataset[3]['questionText']}]\n",
    "input4 = [{\"role\": \"user\", \"content\": dataset[4]['questionText']}]\n",
    "\n",
    "\n",
    "texts = tokenizer.apply_chat_template([input0, input1, input2, input3, input4], tokenize=False, add_generation_prompt=True)\n",
    "inputs = tokenizer(texts, padding=\"longest\", truncation=True, return_tensors=\"pt\")\n",
    "inputs = {key: val.to(model.device) for key, val in inputs.items()}\n",
    "temp_texts = tokenizer.batch_decode(inputs['input_ids'], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_tokens = model.generate(\n",
    "    **inputs, \n",
    "    max_new_tokens=2048, \n",
    "    pad_token_id=tokenizer.pad_token_id, \n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_text = tokenizer.batch_decode(gen_tokens, skip_special_tokens=True)\n",
    "gen_text = [i[len(temp_texts[idx]):] for idx, i in enumerate(gen_text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mental-health",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

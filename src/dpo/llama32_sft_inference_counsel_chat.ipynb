{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import json\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the Counsel Chat Dataset\n",
    "\n",
    "Extract 5 questions from each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"nbertagnolli/counsel-chat\"\n",
    "dataset = load_dataset(dataset_name, split=\"all\")\n",
    "dataset = dataset.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df = dataset.to_pandas()\n",
    "dataset_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df_filt = dataset_df[['topic', 'questionTitle', 'questionText', 'answerText', 'upvotes']].groupby('topic', group_keys=False).apply(lambda x: x.sort_values(['upvotes'], ascending=False)[:5]).reset_index(drop=True)\n",
    "dataset_df_filt = dataset_df_filt.fillna('')\n",
    "dataset_df_filt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use OpenAI to generate synthetic data\n",
    "\n",
    "We need to generate a question pair from OpenAI by giving a relevant example from the CounselChat Dataset as one-shot instruction tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../api.key\", 'r') as file:\n",
    "    openai_api_key = file.read()\n",
    "    \n",
    "openai_client = OpenAI(api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_openai_response(system_prompt: str, user_prompt: str) -> str:\n",
    "    \n",
    "    completion = openai_client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=1,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    openai_response = completion.choices[0].message.content\n",
    "    \n",
    "    return openai_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_qa = 'You are an expert mental-health counsellor'\n",
    "user_prompt_qa = '''You are given a broad topic which covers a specific area in which humans suffer from ill mental health.\n",
    "You job is to generate a topic relevant question/answer pair with question describing the mental state of the patient and answer describing the counselling advice given to the patient.\n",
    "\n",
    "Topic: {topic}\n",
    "\n",
    "Example:\n",
    "Question-> {question}\n",
    "Answer-> {answer}\n",
    "\n",
    "You must return response in a json serializable format as following {{question: question_text, answer:answer_text}}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# openai_responses_qa = []\n",
    "\n",
    "# for index, row in tqdm(dataset_df_filt.iterrows(), total=len(dataset_df_filt)):\n",
    "    \n",
    "#     topic = row['topic']\n",
    "#     question = row['questionText'] + row['questionTitle']\n",
    "#     answer = row['answerText']\n",
    "    \n",
    "#     response_qa = get_openai_response(system_prompt=system_prompt_qa, user_prompt=user_prompt_qa.format(topic=topic, question=question, answer=answer))\n",
    "    \n",
    "#     openai_responses_qa.append(response_qa)\n",
    "\n",
    "# with open('openai_responses_qa.pkl', 'wb') as file:\n",
    "#     pickle.dump(openai_responses_qa, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the OpenAI Response files and converting to json for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('openai_responses_qa.pkl', 'rb') as file:\n",
    "    openai_responses_qa = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting to json responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_responses = []\n",
    "for index, response in enumerate(openai_responses_qa):\n",
    "    response = response.replace('json','').replace(\"```\",\"\")\n",
    "    try:\n",
    "        json_responses.append(json.loads(response))\n",
    "    except:\n",
    "        print(index)\n",
    "        json_responses.append(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually processing the ones with errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_responses[3] = json.loads(openai_responses_qa[3].split(\"```\")[1].replace('json',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df_topic_list = dataset_df_filt['topic'].to_list()\n",
    "\n",
    "for index, topic in enumerate(dataset_df_topic_list):\n",
    "    json_responses[index]['topic'] = topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the topics to each json response for later processing or analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference from already fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"llama32-sft-fine-tune-counselchat\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.model_max_length = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\") # Must be float32 for MacBooks!\n",
    "model.config.pad_token_id = tokenizer.pad_token_id # Updating the model config to use the special pad token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing a list of questions from GPT QA Responses for batch inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_inputs = [[{\"role\": \"user\", \"content\": response['question']}] for response in json_responses]\n",
    "\n",
    "texts = tokenizer.apply_chat_template(llama_inputs, tokenize=False, add_generation_prompt=True)\n",
    "inputs = tokenizer(texts, padding=\"longest\", truncation=True, return_tensors=\"pt\")\n",
    "inputs = {key: val.to(model.device) for key, val in inputs.items()}\n",
    "temp_texts = tokenizer.batch_decode(inputs['input_ids'], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen_tokens = model.generate(\n",
    "#     **inputs, \n",
    "#     max_new_tokens=2048, \n",
    "#     pad_token_id=tokenizer.pad_token_id, \n",
    "#     eos_token_id=terminators,\n",
    "#     do_sample=True,\n",
    "#     temperature=0.6,\n",
    "#     top_p=0.9\n",
    "# )\n",
    "\n",
    "# gen_text = tokenizer.batch_decode(gen_tokens, skip_special_tokens=True)\n",
    "# gen_text = [i[len(temp_texts[idx]):] for idx, i in enumerate(gen_text)]\n",
    "\n",
    "# with open('llama_responses_qa.pkl', 'wb') as file:\n",
    "#     pickle.dump(gen_text, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('llama_responses_qa.pkl', 'rb') as file:\n",
    "    llama_response = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrating the LLaMA Response within the json responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, response in enumerate(llama_response):\n",
    "    json_responses[index]['llama_answer'] = response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asking the GPT to rate the responses based on CTRS evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_eval = 'You are an expert mental-health counsellor'\n",
    "user_prompt_eval = '''You are given a conversation between a patient and therapist. Your job is to evaluate the response of therapist against the problem described by the patient as per the criterion of\n",
    "Understanding, Interpersonal Effectiveness, Collaboration, Guided Discovery, Focus and Strategy. The definition for each of these criterion is mentioned below:\n",
    "\n",
    "Understanding: How accurately does the therapist demonstrate understanding of the client’s issues and concerns?\n",
    "Interpersonal Effectiveness: How effective is the therapist in maintaining a positive and therapeutic relationship with the client?\n",
    "Collaboration: To what extent does the therapist engage the client in collaborative goalsetting and decision-making?\n",
    "Guided Discovery: How effectively does the therapist use guided discovery techniques to facilitate client self-reflection and insight?\n",
    "Focus: How well does the therapist identify and address the client’s key cognitions or behaviors that need change?\n",
    "Strategy: How appropriate and coherent is the therapist’s strategy for promoting change in the client’s problematic behaviors or thoughts?\n",
    "\n",
    "For each of these criterion, you need to assign a rating of 0 to 6 based on how well the therapist response fulfills the definition of the specific criterion.\n",
    "\n",
    "You final response must not contain any description about any criterion and it must ONLY be in a json serializable format as following {{\n",
    "    Understanding: understanding_rating,\n",
    "    Interpersonal Effectiveness: interpersonal_effectiveness_rating,\n",
    "    Collaboration: collaboration_rating,\n",
    "    Guided Discovery: guided_discovery_rating,\n",
    "    Focus: focus_rating,\n",
    "    Strategy: strategy_rating,\n",
    "}}\n",
    "\n",
    "[PATIENT Problem]:\n",
    "{patient}\n",
    "\n",
    "[THERAPIST Response]:\n",
    "{therapist}\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the Responses of LLaMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# openai_responses_eval_llama = []\n",
    "\n",
    "# for response in tqdm(json_responses):\n",
    "    \n",
    "#     patient_problem = response['question']\n",
    "#     therapist_response = response['llama_answer'] #LLaMA Response\n",
    "    \n",
    "#     response_eval = get_openai_response(system_prompt=system_prompt_eval, user_prompt=user_prompt_eval.format(patient=patient_problem, therapist=therapist_response))\n",
    "    \n",
    "#     openai_responses_eval_llama.append(response_eval)\n",
    "\n",
    "# with open('openai_responses_eval_llama.pkl', 'wb') as file:\n",
    "#     pickle.dump(openai_responses_eval_llama, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('openai_responses_eval_llama.pkl', 'rb') as file:\n",
    "    openai_responses_eval_llama = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding to Json Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, response in enumerate(openai_responses_eval_llama):\n",
    "    response = response.replace('json','').replace(\"```\",'')\n",
    "    try:\n",
    "        json_responses[index]['llama_answer_eval'] = json.loads(response)\n",
    "    except:\n",
    "        print(index)\n",
    "        json_responses[index]['llama_answer_eval'] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the Responses of GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# openai_responses_eval_gpt = []\n",
    "\n",
    "# for response in tqdm(json_responses):\n",
    "    \n",
    "#     patient_problem = response['question']\n",
    "#     therapist_response = response['answer'] #GPT Response\n",
    "    \n",
    "#     response_eval = get_openai_response(system_prompt=system_prompt_eval, user_prompt=user_prompt_eval.format(patient=patient_problem, therapist=therapist_response))\n",
    "    \n",
    "#     openai_responses_eval_gpt.append(response_eval)\n",
    "\n",
    "# with open('openai_responses_eval_gpt.pkl', 'wb') as file:\n",
    "#     pickle.dump(openai_responses_eval_gpt, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('openai_responses_eval_gpt.pkl', 'rb') as file:\n",
    "    openai_responses_eval_gpt = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, response in enumerate(openai_responses_eval_gpt):\n",
    "    response = response.replace('json','').replace(\"```\",'')\n",
    "    try:\n",
    "        json_responses[index]['gpt_answer_eval'] = json.loads(response)\n",
    "    except:\n",
    "        print(index)\n",
    "        json_responses[index]['gpt_answer_eval'] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forming the visualizations using the evaluation criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_data = []\n",
    "llama_undr = []\n",
    "llama_intr_eff = []\n",
    "llama_collab = []\n",
    "llama_gd_disc = []\n",
    "llama_foc = []\n",
    "llama_strat = []\n",
    "gpt_undr = []\n",
    "gpt_intr_eff = []\n",
    "gpt_collab = []\n",
    "gpt_gd_disc = []\n",
    "gpt_foc = []\n",
    "gpt_strat = []\n",
    "\n",
    "for response in tqdm(json_responses):\n",
    "    topic_data.append(response['topic'])\n",
    "    \n",
    "    llama_undr.append(response['llama_answer_eval']['Understanding'])\n",
    "    llama_intr_eff.append(response['llama_answer_eval']['Interpersonal Effectiveness'])\n",
    "    llama_collab.append(response['llama_answer_eval']['Collaboration'])\n",
    "    llama_gd_disc.append(response['llama_answer_eval']['Guided Discovery'])\n",
    "    llama_foc.append(response['llama_answer_eval']['Focus'])\n",
    "    llama_strat.append(response['llama_answer_eval']['Strategy'])\n",
    "    \n",
    "    gpt_undr.append(response['gpt_answer_eval']['Understanding'])\n",
    "    gpt_intr_eff.append(response['gpt_answer_eval']['Interpersonal Effectiveness'])\n",
    "    gpt_collab.append(response['gpt_answer_eval']['Collaboration'])\n",
    "    gpt_gd_disc.append(response['gpt_answer_eval']['Guided Discovery'])\n",
    "    gpt_foc.append(response['gpt_answer_eval']['Focus'])\n",
    "    gpt_strat.append(response['gpt_answer_eval']['Strategy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.DataFrame({'llama_undr': llama_undr, 'gpt_undr': gpt_undr, 'llama_intr_eff': llama_intr_eff, 'gpt_intr_eff': gpt_intr_eff,\n",
    "                        'llama_collab': llama_collab, 'gpt_collab': gpt_collab, 'llama_gd_disc': llama_gd_disc, 'gpt_gd_disc': gpt_gd_disc,\n",
    "                        'llama_foc': llama_foc, 'gpt_foc': gpt_foc, 'llama_strat': llama_strat, 'gpt_strat': gpt_strat}, index=topic_data)\n",
    "\n",
    "eval_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_metrics = eval_df.agg('mean')\n",
    "avg_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "plt.bar(avg_metrics.index.to_list(), avg_metrics.values.tolist(), color=['r','b','r','b','r','b','r','b','r','b','r','b'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_groupby_avg = eval_df.groupby(eval_df.index).mean()\n",
    "topic_groupby_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,6))\n",
    "plt.bar(topic_groupby_avg.index.to_list(), topic_groupby_avg['llama_undr'].values.tolist())\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mental-health",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response Generation\n",
    "\n",
    "This file will generate the responses for CounselChat questions using 4 models\n",
    "\n",
    "1. Base GPT (gpt-4o)\n",
    "2. Fine-Tuned GPT (gpt-4o)\n",
    "3. Base LLaMA (LLaMA-3.2 3B Instruct)\n",
    "4. Fine-Tuned LLaMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the Processed CounselChat Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('processed_data/counselchat_top_votes_test.pkl', 'rb') as file:\n",
    "    dataset_top_votes_test = pickle.load(file)\n",
    "\n",
    "dataset_top_votes_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing a response generation dataframe to record all the answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_response_generation = dataset_top_votes_test\n",
    "df_response_generation.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI Configuration and Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../api.key\", 'r') as file:\n",
    "    openai_api_key = file.read()\n",
    "\n",
    "openai_client = OpenAI(api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_openai_response(user_prompt: str) -> str:\n",
    "        \n",
    "    completion = openai_client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert mental health professional trained to counsel and guide patients suffering from ill mental-health. Limit your response to a maximum of 250 words\"},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    openai_response = completion.choices[0].message.content\n",
    "    \n",
    "    return openai_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get GPT Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_responses_base = []\n",
    "for index, row in tqdm(dataset_top_votes_test.iterrows(), total=len(dataset_top_votes_test)):\n",
    "    question_input = row['question']\n",
    "    try:\n",
    "        gpt_resp = get_openai_response(user_prompt = question_input)\n",
    "        gpt_responses_base.append(gpt_resp)\n",
    "    except:\n",
    "        gpt_responses_base.append('gpt_resp')\n",
    "\n",
    "with open('response_generation_data/gpt_base_resp.pkl', 'wb') as file:\n",
    "    pickle.dump(gpt_responses_base, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('response_generation_data/gpt_base_resp.pkl', 'rb') as file:\n",
    "    gpt_responses_base = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_response_generation['gpt_responses_base'] = gpt_responses_base\n",
    "df_response_generation.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI Fine-Tuned Model Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_openai_response_finetuned(user_prompt: str) -> str:\n",
    "        \n",
    "    completion = openai_client.chat.completions.create(\n",
    "    model=\"ft:gpt-4o-2024-08-06:university-of-texas-at-austin:counselchat-train:BGJdvzQV\",\n",
    "    temperature=0,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert mental health professional trained to counsel and guide patients suffering from ill mental-health\"},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "    max_tokens=2048\n",
    "    )\n",
    "\n",
    "    openai_response = completion.choices[0].message.content\n",
    "    \n",
    "    return openai_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_responses_ft = []\n",
    "for index, row in tqdm(dataset_top_votes_test.iterrows(), total=len(dataset_top_votes_test)):\n",
    "    question_input = row['question']\n",
    "    try:\n",
    "        gpt_resp = get_openai_response_finetuned(user_prompt=question_input)\n",
    "        gpt_responses_ft.append(gpt_resp)\n",
    "    except:\n",
    "        gpt_responses_ft.append('')\n",
    "        \n",
    "with open('response_generation_data/gpt_ft_resp.pkl', 'wb') as file:\n",
    "    pickle.dump(gpt_responses_ft, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('response_generation_data/gpt_ft_resp.pkl', 'rb') as file:\n",
    "    gpt_responses_ft = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_response_generation['gpt_responses_ft'] = gpt_responses_ft\n",
    "df_response_generation.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferencing from LLaMA Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing the batches of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "question_list = dataset_top_votes_test['question'].to_list()\n",
    "batch_indices = np.arange(0, len(question_list), batch_size)\n",
    "if batch_indices[-1] != len(question_list):\n",
    "    batch_indices = np.append(batch_indices, len(question_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the base model from Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 2048 \n",
    "dtype = None # None for auto-detection.\n",
    "load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Llama-3.2-3B-Instruct\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    load_in_4bit=load_in_4bit,\n",
    "    dtype=dtype,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing batch Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llama_response_base(question_inputs: str):\n",
    "    \n",
    "    llama_inputs = [[{\"role\": \"system\", \"content\": \"You are an expert mental health professional trained to counsel and guide patients suffering from ill mental-health. Limit your response to a maximum of 250 words\"},\n",
    "                     {\"role\": \"user\", \"content\": question}] for question in question_inputs]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(llama_inputs, tokenize=False, add_generation_prompt=True)\n",
    "    \n",
    "    inputs = tokenizer(prompt, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    inputs = {key: val.to(model.device) for key, val in inputs.items()}\n",
    "    temp_texts = tokenizer.batch_decode(inputs['input_ids'], skip_special_tokens=True)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        **inputs, \n",
    "        max_new_tokens=max_seq_length,\n",
    "        num_return_sequences=1,\n",
    "        temperature=0.0\n",
    "    )\n",
    "\n",
    "    texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    texts = [i[len(temp_texts[idx]):] for idx, i in enumerate(texts)]\n",
    "    \n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing the Unsloth Fast Inference\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "llama_responses_base = []\n",
    "for i in tqdm(range(0, len(batch_indices) - 1)):\n",
    "    questions_input = question_list[batch_indices[i]:batch_indices[i+1]]\n",
    "    llama_resp = get_llama_response_base(questions_input)\n",
    "    llama_responses_base = llama_responses_base + llama_resp\n",
    "\n",
    "with open('response_generation_data/llama_responses_base.pkl', 'wb') as file:\n",
    "    pickle.dump(llama_responses_base, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('response_generation_data/llama_responses_base.pkl', 'rb') as file:\n",
    "    llama_responses_base = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_response_generation['llama_responses_base'] = llama_responses_base\n",
    "df_response_generation.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferencing from LLaMA Fine-Tune Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing the batches of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "question_list = dataset_top_votes_test['question'].to_list()\n",
    "batch_indices = np.arange(0, len(question_list), batch_size)\n",
    "if batch_indices[-1] != len(question_list):\n",
    "    batch_indices = np.append(batch_indices, len(question_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"llama32-sft-fine-tune-counselchat\"\n",
    "\n",
    "max_seq_length = 2048 \n",
    "dtype = None # None for auto-detection.\n",
    "load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_id,\n",
    "    max_seq_length = max_seq_length,\n",
    "    load_in_4bit=load_in_4bit,\n",
    "    dtype=dtype,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing batch Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llama_response_ft(question_inputs: str):\n",
    "    \n",
    "    llama_inputs = [[{\"role\": \"system\", \"content\": \"You are an expert mental health professional trained to counsel and guide patients suffering from ill mental-health.\"},\n",
    "                     {\"role\": \"user\", \"content\": question}] for question in question_inputs]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(llama_inputs, tokenize=False, add_generation_prompt=True)\n",
    "    \n",
    "    inputs = tokenizer(prompt, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    inputs = {key: val.to(model.device) for key, val in inputs.items()}\n",
    "    temp_texts = tokenizer.batch_decode(inputs['input_ids'], skip_special_tokens=True)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        **inputs, \n",
    "        max_new_tokens=max_seq_length,\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "\n",
    "    texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    texts = [i[len(temp_texts[idx]):] for idx, i in enumerate(texts)]\n",
    "    \n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing the Unsloth Fast Inference\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "llama_responses_ft = []\n",
    "for i in tqdm(range(0, len(batch_indices) - 1)):\n",
    "    questions_input = question_list[batch_indices[i]:batch_indices[i+1]]\n",
    "    llama_resp = get_llama_response_base(questions_input)\n",
    "    llama_responses_ft = llama_responses_ft + llama_resp\n",
    "\n",
    "with open('response_generation_data/llama_responses_ft.pkl', 'wb') as file:\n",
    "    pickle.dump(llama_responses_ft, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('response_generation_data/llama_responses_ft.pkl', 'rb') as file:\n",
    "    llama_responses_ft = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_response_generation['llama_responses_ft'] = llama_responses_ft\n",
    "df_response_generation.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mental-health-agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response Generation\n",
    "\n",
    "This file will generate the responses for CounselChat questions using 4 models\n",
    "\n",
    "1. Base GPT (gpt-4o)\n",
    "2. Fine-Tuned GPT (gpt-4o)\n",
    "3. Base LLaMA (LLaMA-3.2 3B Instruct)\n",
    "4. Fine-Tuned LLaMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the Processed CounselChat Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"nbertagnolli/counsel-chat\"\n",
    "dataset = load_dataset(dataset_name, split=\"all\")\n",
    "dataset = dataset.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df = dataset.to_pandas()\n",
    "dataset_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df_top_votes = dataset_df.groupby('questionID').apply(lambda x: x.sort_values('upvotes', ascending=False).iloc[0], include_groups=False).reset_index()\n",
    "dataset_df_top_votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df_top_votes['question'] = dataset_df_top_votes['questionText'] + \" \" + dataset_df_top_votes['questionTitle']\n",
    "dataset_df_top_votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df_final = dataset_df_top_votes[['topic', 'question', 'answerText']]\n",
    "dataset_df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI Configuration and Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../api.key\", 'r') as file:\n",
    "    openai_api_key = file.read()\n",
    "\n",
    "openai_client = OpenAI(api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_qa = 'You are an expert mental-health counsellor'\n",
    "user_prompt_qa = '''A patient is suffering from ill mental health. The patient writes the following thoughts on a social media platform:\n",
    "\n",
    "{question}\n",
    "\n",
    "You need to respond to the user in a way that improves their overall mental health. You must return response in a json serializable format as following {{response: response_text}}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_openai_response(system_prompt: str, user_prompt: str) -> str:\n",
    "        \n",
    "    completion = openai_client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    openai_response = completion.choices[0].message.content\n",
    "    \n",
    "    return openai_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get GPT Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt_responses = []\n",
    "# for index, row in tqdm(dataset_df_final.iterrows(), total=len(dataset_df_final)):\n",
    "#     question_input = row['question']\n",
    "#     gpt_resp = get_openai_response(system_prompt=system_prompt_qa, user_prompt_qa.format(question = question_input))\n",
    "#     try:\n",
    "#         gpt_answer = json.loads(gpt_resp.split(\"```\")[1].replace('json',''))['response']\n",
    "#         gpt_responses.append(gpt_answer)\n",
    "#     except:\n",
    "#         gpt_responses.append(gpt_resp)\n",
    "\n",
    "# with open('response_generation_data/openai_que_resp.pkl', 'wb') as file:\n",
    "#     pickle.dump(gpt_responses, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('response_generation_data/openai_que_resp.pkl', 'rb') as file:\n",
    "    gpt_responses = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df_final['gpt_responses'] = gpt_responses\n",
    "dataset_df_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI Fine-Tuned Model Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_openai_response_finetuned(system_prompt: str, user_prompt: str) -> str:\n",
    "        \n",
    "    completion = openai_client.chat.completions.create(\n",
    "    model=\"ft:gpt-4o-2024-08-06:university-of-texas-at-austin:counselchat-clean:BE3PqwuO\",\n",
    "    temperature=0,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "    max_tokens=2048\n",
    "    )\n",
    "\n",
    "    openai_response = completion.choices[0].message.content\n",
    "    \n",
    "    return openai_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt_responses_ft = []\n",
    "# system_prompt_qa = 'You are an expert mental health professional trained to counsel and guide patients suffering from ill mental-health'\n",
    "# for index, row in tqdm(dataset_df_final.iterrows(), total=len(dataset_df_final)):\n",
    "#     question_input = row['question']\n",
    "#     try:\n",
    "#         gpt_resp = get_openai_response_finetuned(system_prompt=system_prompt_qa, user_prompt=question_input)\n",
    "#         gpt_responses_ft.append(gpt_resp)\n",
    "#     except:\n",
    "#         gpt_responses_ft.append('')\n",
    "        \n",
    "# with open('response_generation_data/openai_ft_que_resp.pkl', 'wb') as file:\n",
    "#     pickle.dump(gpt_responses_ft, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('response_generation_data/openai_ft_que_resp.pkl', 'rb') as file:\n",
    "    gpt_responses_ft = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df_final['gpt_responses_ft'] = gpt_responses_ft\n",
    "dataset_df_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferencing from LLAMA Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hf_token.key', 'r') as f:\n",
    "    hf_token = f.read()\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-3B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.model_max_length = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\") # Must be float32 for MacBooks!\n",
    "model.config.pad_token_id = tokenizer.pad_token_id # Updating the model config to use the special pad token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llama_response(question_inputs: str):\n",
    "    \n",
    "    llama_inputs = [[{\"role\": \"user\", \"content\": question}] for question in question_inputs]\n",
    "\n",
    "    texts = tokenizer.apply_chat_template(llama_inputs, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(texts, padding=\"longest\", truncation=True, return_tensors=\"pt\")\n",
    "    inputs = {key: val.to(model.device) for key, val in inputs.items()}\n",
    "    temp_texts = tokenizer.batch_decode(inputs['input_ids'], skip_special_tokens=True)\n",
    "    \n",
    "    gen_tokens = model.generate(\n",
    "        **inputs, \n",
    "        max_new_tokens=2048, \n",
    "        pad_token_id=tokenizer.pad_token_id, \n",
    "        eos_token_id=terminators,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        # top_p=0.9\n",
    "    )\n",
    "\n",
    "    gen_text = tokenizer.batch_decode(gen_tokens, skip_special_tokens=True)\n",
    "    gen_text = [i[len(temp_texts[idx]):] for idx, i in enumerate(gen_text)]\n",
    "    \n",
    "    return gen_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "question_list = dataset_df_final['question'].to_list()\n",
    "batch_indices = np.arange(0, len(question_list), batch_size)\n",
    "if batch_indices[-1] != len(question_list):\n",
    "    batch_indices = np.append(batch_indices, len(question_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama_responses_base = []\n",
    "# for i in tqdm(range(0, len(batch_indices) - 1)):\n",
    "#     questions_input = question_list[batch_indices[i]:batch_indices[i+1]]\n",
    "#     llama_resp = get_llama_response(questions_input)\n",
    "#     llama_responses_base = llama_responses_base + llama_resp\n",
    "\n",
    "# with open('response_generation_data/llama_que_resp_base.pkl', 'wb') as file:\n",
    "#     pickle.dump(llama_responses_base, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('response_generation_data/llama_que_resp_base.pkl', 'rb') as file:\n",
    "    llama_responses_base = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df_final['llama_responses_base'] = llama_responses_base\n",
    "dataset_df_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferencing from LLaMA Fine-Tune Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"llama32-sft-fine-tune-counselchat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.model_max_length = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\") # Must be float32 for MacBooks!\n",
    "model.config.pad_token_id = tokenizer.pad_token_id # Updating the model config to use the special pad token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llama_response(question_inputs: str):\n",
    "    \n",
    "    llama_inputs = [[{\"role\": \"user\", \"content\": question}] for question in question_inputs]\n",
    "\n",
    "    texts = tokenizer.apply_chat_template(llama_inputs, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(texts, padding=\"longest\", truncation=True, return_tensors=\"pt\")\n",
    "    inputs = {key: val.to(model.device) for key, val in inputs.items()}\n",
    "    temp_texts = tokenizer.batch_decode(inputs['input_ids'], skip_special_tokens=True)\n",
    "    \n",
    "    gen_tokens = model.generate(\n",
    "        **inputs, \n",
    "        max_new_tokens=2048, \n",
    "        pad_token_id=tokenizer.pad_token_id, \n",
    "        eos_token_id=terminators,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        # top_p=0.9\n",
    "    )\n",
    "\n",
    "    gen_text = tokenizer.batch_decode(gen_tokens, skip_special_tokens=True)\n",
    "    gen_text = [i[len(temp_texts[idx]):] for idx, i in enumerate(gen_text)]\n",
    "    \n",
    "    return gen_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "question_list = dataset_df_final['question'].to_list()\n",
    "batch_indices = np.arange(0, len(question_list), batch_size)\n",
    "if batch_indices[-1] != len(question_list):\n",
    "    batch_indices = np.append(batch_indices, len(question_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama_responses = []\n",
    "# for i in tqdm(range(0, len(batch_indices) - 1)):\n",
    "#     questions_input = question_list[batch_indices[i]:batch_indices[i+1]]\n",
    "#     llama_resp = get_llama_response(questions_input)\n",
    "#     llama_responses = llama_responses + llama_resp\n",
    "\n",
    "# with open('response_generation_data/llama_que_resp.pkl', 'wb') as file:\n",
    "#     pickle.dump(llama_responses, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('response_generation_data/llama_que_resp.pkl', 'rb') as file:\n",
    "    llama_responses = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df_final['llama_responses'] = llama_responses\n",
    "dataset_df_final.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mental-health",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

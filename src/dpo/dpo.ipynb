{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DPO/IPO Links:\n",
    "\n",
    "https://github.com/eric-mitchell/direct-preference-optimization [Owner] \\\n",
    "https://huggingface.co/blog/pref-tuning \\\n",
    "https://github.com/huggingface/alignment-handbook \\\n",
    "https://github.com/dida-do/public/blob/master/fine-tuning_llm/train-dpo.py \\\n",
    "https://www.kaggle.com/code/aisuko/supervised-fine-tuning-llama2-with-dpo \\\n",
    "https://github.com/michaelnny/DPO-LLaMA \\\n",
    "https://plainenglish.io/community/direct-preference-optimization-dpo-a-simplified-approach-to-fine-tuning-large-language-models \\\n",
    "https://towardsdatascience.com/fine-tune-a-mistral-7b-model-with-direct-preference-optimization-708042745aac \\\n",
    "https://github.com/mlabonne/llm-course/blob/main/Fine_tune_a_Mistral_7b_model_with_DPO.ipynb \\\n",
    "https://huggingface.co/blog/dpo-trl \\\n",
    "https://discuss.huggingface.co/t/sfttrainer-class-and-training-arguements/85976/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install --upgrade \\\n",
    "“transformers==4.38.2”\\\n",
    "“datasets==2.16.1”\\\n",
    "“accelerate==0.26.1”\\\n",
    "“evaluate==0.4.1”\\\n",
    "“bitsandbytes==0.42.0”\\\n",
    "“trl==0.7.11”\\\n",
    "“peft==0.8.2”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLaMA2 with DPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from transformers.trainer import TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from peft import LoraConfig, TaskType, AutoPeftModelForCausalLM\n",
    "from trl.trainer import ConstantLengthDataset\n",
    "from trl import SFTTrainer, DPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/mn27889/miniconda3/envs/mental-health/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name = \"huggyllama/llama-7b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resolving data files: 100%|██████████| 72/72 [00:00<00:00, 378.00it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\n",
    "    path=\"lvwerra/stack-exchange-paired\",\n",
    "    split = \"train\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.train_test_split(test_size=0.005, seed=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = dataset['train']\n",
    "eval_data = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['qid', 'question', 'date', 'metadata', 'response_j', 'response_k'],\n",
       "    num_rows: 26667823\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If so how can it be done? By default L4 writes to a text file. I notice that Monolog can Log to Database on its [github](https://github.com/seldaek/monolog) page.\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0]['question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yep, You could create a listener to log everything in the routes.php\n",
      "\n",
      "```\n",
      "Event::listen('laravel.log', function($type,$message)\n",
      "{\n",
      "    $log = new Log();\n",
      "    $log->message = $message;\n",
      "    $log->type = $type;\n",
      "    $log->update;\n",
      "});\n",
      "\n",
      "```\n",
      "\n",
      "Or alternatively if you wanted to only log errors 400 and 500 Larvavel there is a Log event in the Routes.php file which listens to errors 404 and 500, you can write your own code in this event listener. So assuming you have a Model called Log defined,\n",
      "\n",
      "```\n",
      "Event::listen('404', function()\n",
      "{\n",
      "    $error = \"404: \" . URL::full();\n",
      "    Log::error($error);\n",
      "    $update = new Log();\n",
      "    $update->error = $error;\n",
      "    $update->update;\n",
      "    return Response::error('404');\n",
      "});\n",
      "\n",
      "Event::listen('500', function()\n",
      "{\n",
      "    Log::error($error);\n",
      "    $update = new Log();\n",
      "    $update->error = $error;\n",
      "    $update->update;\n",
      "    return Response::error('500');\n",
      "});\n",
      "\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0]['response_j'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In laravel 5 now it is `illuminate.log`\n",
      "\n",
      "Now it will be like \n",
      "\n",
      "```\n",
      "Event::listen('illuminate.log', function($type,$message)\n",
      "{\n",
      "    ....\n",
      "});\n",
      "\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0]['response_k'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sample_text(example):\n",
    "    text = f\"Question: {example['question']}\\n\\nAnswer: {example['response_j']}\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: If so how can it be done? By default L4 writes to a text file. I notice that Monolog can Log to Database on its [github](https://github.com/seldaek/monolog) page.\n",
      "\n",
      "Answer: Yep, You could create a listener to log everything in the routes.php\n",
      "\n",
      "```\n",
      "Event::listen('laravel.log', function($type,$message)\n",
      "{\n",
      "    $log = new Log();\n",
      "    $log->message = $message;\n",
      "    $log->type = $type;\n",
      "    $log->update;\n",
      "});\n",
      "\n",
      "```\n",
      "\n",
      "Or alternatively if you wanted to only log errors 400 and 500 Larvavel there is a Log event in the Routes.php file which listens to errors 404 and 500, you can write your own code in this event listener. So assuming you have a Model called Log defined,\n",
      "\n",
      "```\n",
      "Event::listen('404', function()\n",
      "{\n",
      "    $error = \"404: \" . URL::full();\n",
      "    Log::error($error);\n",
      "    $update = new Log();\n",
      "    $update->error = $error;\n",
      "    $update->update;\n",
      "    return Response::error('404');\n",
      "});\n",
      "\n",
      "Event::listen('500', function()\n",
      "{\n",
      "    Log::error($error);\n",
      "    $update = new Log();\n",
      "    $update->error = $error;\n",
      "    $update->update;\n",
      "    return Response::error('500');\n",
      "});\n",
      "\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(prepare_sample_text(train_data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chars_token_ratio(dataset, tokenizer, nb_examples=400):\n",
    "    '''\n",
    "    Estimate the average number of characters per token in the dataset\n",
    "    '''\n",
    "    \n",
    "    total_characters, total_tokens = 0, 0\n",
    "    for _, example in tqdm(zip(range(nb_examples), iter(dataset)), total=nb_examples):\n",
    "        text = prepare_sample_text(example)\n",
    "        total_characters += len(text)\n",
    "        if tokenizer.is_fast:\n",
    "            total_tokens += len(tokenizer(text).tokens())\n",
    "        else:\n",
    "            total_tokens += len(tokenizer.tokenize(text))\n",
    "    \n",
    "    return total_characters/total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_per_token = chars_token_ratio(train_data, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ConstantLengthDataset(\n",
    "    tokenizer,\n",
    "    train_data,\n",
    "    formatting_func=prepare_sample_text,\n",
    "    infinite=True,\n",
    "    seq_length=1024,\n",
    "    chars_per_token=chars_per_token\n",
    ")\n",
    "\n",
    "eval_dataset = ConstantLengthDataset(\n",
    "    tokenizer,\n",
    "    eval_data,\n",
    "    formatting_func=prepare_sample_text,\n",
    "    infinite=False,\n",
    "    seq_length=1024,\n",
    "    chars_per_token=chars_per_token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config = bnb_config,\n",
    "    device_map = \"auto\",\n",
    "    torch_dtype = torch.bfloat16,\n",
    "    trust_remote_code = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.config.use_cache=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    bias=\"none\",\n",
    "    task_type = TaskType.CAUSAL_LM\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args=TrainingArguments(\n",
    "    output_dir=\"./sft\",\n",
    "    max_steps=100,\n",
    "    logging_steps=10,\n",
    "    save_steps=10,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=True,\n",
    "    group_by_length=False,\n",
    "    learning_rate=1e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_steps=50,\n",
    "    weight_decay=0.05,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    fp16=True,\n",
    "    remove_unused_columns=False,\n",
    "    report_to=\"none\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_trainer=SFTTrainer(\n",
    "    model=base_model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    peft_config=peft_config,\n",
    "    packing=True,\n",
    "    max_seq_length=None,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sft_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoPeftModelForCausalLM.from_pretrained(\"./sft/checkpoint-100\", device_map=\"auto\", torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./sft/final_merged_checkpoint\", safe_serialization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct Preference Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_prompt_and_responses(samples):\n",
    "    return {\n",
    "        \"prompt\":[\n",
    "            \"Question:\"+question+\"\\n\\nAnswer:\" for question in samples[\"question\"]\n",
    "        ],\n",
    "        \"chosen\": samples[\"response_j\"],\n",
    "        \"rejected\": samples[\"response_k\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stack_exchange_paired(sanity_check=False, cache_dir=None, num_proc=24):\n",
    "    dataset=load_dataset(\n",
    "        \"lvwerra/stack-exchange-paired\",\n",
    "        split=\"train\",\n",
    "        cache_dir=cache_dir,\n",
    "    )\n",
    "    original_columns=dataset.column_names\n",
    "    \n",
    "    if sanity_check:\n",
    "        dataset=dataset.select(range(min(len(dataset), 1000)))\n",
    "    \n",
    "    return dataset.map(\n",
    "        return_prompt_and_responses,\n",
    "        batched=True,\n",
    "        num_proc=num_proc,\n",
    "        remove_columns=original_columns,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:07<00:00,  2.50s/it]\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.91s/it]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"./sft/final_merged_checkpoint\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "model.config.use_cache=False\n",
    "\n",
    "model_ref = AutoModelForCausalLM.from_pretrained(\n",
    "    \"./sft/final_merged_checkpoint\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    load_in_4bit=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/mn27889/miniconda3/envs/mental-health/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer_dpo=AutoTokenizer.from_pretrained(\"huggyllama/llama-7b\")\n",
    "tokenizer_dpo.pad_token=tokenizer_dpo.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resolving data files: 100%|██████████| 72/72 [00:00<00:00, 734.09it/s]\n",
      "Map (num_proc=24): 100%|██████████| 26801833/26801833 [03:28<00:00, 128560.57 examples/s] \n"
     ]
    }
   ],
   "source": [
    "train_dataset = get_stack_exchange_paired()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': \"Question:I have installed the Java 3D API on PC via the exe installer, which simply created a new directory with `j3dcore.jar`, `vecmath.jar`, `j3dutils.jar` in a lib sub-directory and `j3dcore-ogl.dll` in a bin sub-directory.\\n\\nNetbeans had no issues and my code compiled and executed smoothly, however once I built my project and tried to run it from the command prompt I got an `UnsatisfiedLinkError` saying that `no j3dcore-ogl in java.library.path`. \\n\\nGoogle came to the rescue and gave me 3 viable solutions:\\n\\n* by copying the dll file into my JRE's bin directory\\n* by adding the path of the dll file to the library path (`java -Djava.library.path=dllpath`)\\n* load the dll in the program with `System.load()` (I couldn't get this one to work, actually)\\n\\nMy question is: Is there an elegant solution to this problem, that I missed? \\n\\nIt seems tedious that for each different PC someone would like to use this program on, he'd have to either copy the dll or add it to the library path before it can run. (Side question: How come Netbeans didn't have a problem with the dll?)\\n\\nAnswer:\",\n",
       " 'chosen': \"> \\n> Making my Java program easily distributable\\n> \\n> \\n> \\n\\nIf you mean 'easy for the end user' look to [Java Web Start](https://stackoverflow.com/tags/java-web-start/info).\\n\\n---\\n\\nA passer-by asks:\\n\\n> \\n> Can you package the dll dependencies with Web Start? \\n> \\n> \\n> \\n\\nYes, but much, much better. You can package the natives for each platform in separate Jars, and supply them only to the platform that uses that native, even so far as partitioning the download between 32 & 64 bit versions of the natives.\\n\\nJWS puts the natives on the run-time class-path of the application, ready for loading in code.\\n\\nThis all happens automatically for the end user, they click a link, approve the trust dialog(s) when asked, and the application installs - possibly with desktop integration, and appears on screen like magic.\\n\\nJWS apps. that use natives need to be distributed as `all-permissions` security level, because the JVM cannot guarantee the actions of anything that 'goes native'.\",\n",
       " 'rejected': 'If you put the dlls in the same directory than you Jar, does it work?\\nIf yes, you could consider distributing it like this.'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 7435908/7435908 [00:34<00:00, 213449.10 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = train_dataset.filter(\n",
    "    lambda x: len(x[\"prompt\"])+len(x[\"chosen\"])<=1024 and len(x[\"prompt\"])+len(x[\"rejected\"])<=1024\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 4483004 examples [00:29, 150622.67 examples/s]\n",
      "Map (num_proc=24): 100%|██████████| 1000/1000 [00:00<00:00, 3277.29 examples/s]\n"
     ]
    }
   ],
   "source": [
    "eval_dataset = get_stack_exchange_paired(data_dir=\"data/evaluation\", sanity_check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 1000/1000 [00:00<00:00, 43812.06 examples/s]\n"
     ]
    }
   ],
   "source": [
    "eval_dataset = eval_dataset.filter(\n",
    "    lambda x: len(x[\"prompt\"])+len(x[\"chosen\"])<=1024 and len(x[\"prompt\"])+len(x[\"rejected\"])<=1024\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args=TrainingArguments(\n",
    "    output_dir=\"./dpo\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=1,\n",
    "    max_steps=1000,\n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=True,\n",
    "    group_by_length=False,\n",
    "    learning_rate=5e-4,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.05,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    bf16=True,\n",
    "    remove_unused_columns=False,\n",
    "    report_to=\"none\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config=LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=['q_proj','v_proj','k_proj','out_proj','fc_in','fc_out','wte',],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  33%|███▎      | 552611/1659503 [17:07<29:28, 626.06 examples/s]  "
     ]
    }
   ],
   "source": [
    "dpo_trainer=DPOTrainer(\n",
    "    model,\n",
    "    model_ref,\n",
    "    args=training_args,\n",
    "    beta=0.1,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer_dpo,\n",
    "    max_prompt_length=512,\n",
    "    max_length=1024,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpo_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpo_trainer.model.save_pretrained()\n",
    "dpo_trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mental-health-agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

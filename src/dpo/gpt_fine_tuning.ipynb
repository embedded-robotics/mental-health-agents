{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT Fine-Tuning\n",
    "\n",
    "This file will be used to create and validate the dataset to be used for the fine-tuning of GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the CounselChat Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"nbertagnolli/counsel-chat\"\n",
    "dataset = load_dataset(dataset_name, split=\"all\")\n",
    "dataset = dataset.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df = dataset.to_pandas()\n",
    "dataset_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df_top_votes = dataset_df.groupby('questionID').apply(lambda x: x.sort_values('upvotes', ascending=False).iloc[0], include_groups=False).reset_index()\n",
    "dataset_df_top_votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df_top_votes['question'] = dataset_df_top_votes['questionText'] + \" \" + dataset_df_top_votes['questionTitle']\n",
    "dataset_df_top_votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df_final = dataset_df_top_votes[['topic', 'question', 'answerText']]\n",
    "dataset_df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df_final = dataset_df_final.dropna().reset_index(drop=True)\n",
    "dataset_df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing emojis, unbreakable space and double spaces from answers/questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emojis(df_bios):\n",
    "    emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "    return re.sub(emoj, '', df_bios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df_final['question'] = dataset_df_final['question'].apply(remove_emojis)\n",
    "dataset_df_final['question'] = dataset_df_final['question'].apply(lambda x: x.replace('\\xa0', ' ').replace(u\"\\u2019\",\"'\").replace(u\"\\u00e9\",\"ee\").replace(\"\\n\",' ').replace(\"  \", \" \"))\n",
    "\n",
    "dataset_df_final['answerText'] = dataset_df_final['answerText'].apply(remove_emojis)\n",
    "dataset_df_final['answerText'] = dataset_df_final['answerText'].apply(lambda x: x.replace('\\xa0', ' ').replace(u\"\\u2019\",\"'\").replace(u\"\\u00e9\",\"ee\").replace(\"\\n\",' ').replace(\"  \", \" \"))\n",
    "\n",
    "dataset_df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing the data into the GPT based format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_fine_tune_data = []\n",
    "for index, row in dataset_df_final.iterrows():\n",
    "    system_prompt = \"You are an expert mental health professional trained to counsel and guide patients suffering from ill mental-health\"\n",
    "    user_content = row['question']\n",
    "    assistant_content = row['answerText']\n",
    "    \n",
    "    formatted_message = {\"messages\": [{\"role\": \"system\", \"content\": system_prompt},\n",
    "                                        {\"role\": \"user\", \"content\": user_content},\n",
    "                                        {\"role\": \"assistant\", \"content\": assistant_content}]}\n",
    "    \n",
    "    gpt_fine_tune_data.append(formatted_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the data in `jsonl` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"gpt_fine_tune_data.jsonl\"\n",
    "\n",
    "with open(file_path, 'w') as file:\n",
    "    for element in gpt_fine_tune_data:\n",
    "        json_line = json.dumps(element)\n",
    "        file.write(json_line + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifying the format using GPT Provided Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"gpt_fine_tune_data.jsonl\"\n",
    "\n",
    "# Load the dataset\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    dataset = [json.loads(line) for line in f]\n",
    "\n",
    "# Initial dataset stats\n",
    "print(\"Num examples:\", len(dataset))\n",
    "print(\"First example:\")\n",
    "for message in dataset[47][\"messages\"]:\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format error checks\n",
    "format_errors = defaultdict(int)\n",
    "\n",
    "for ex in dataset:\n",
    "    if not isinstance(ex, dict):\n",
    "        format_errors[\"data_type\"] += 1\n",
    "        continue\n",
    "        \n",
    "    messages = ex.get(\"messages\", None)\n",
    "    if not messages:\n",
    "        format_errors[\"missing_messages_list\"] += 1\n",
    "        continue\n",
    "        \n",
    "    for message in messages:\n",
    "        if \"role\" not in message or \"content\" not in message:\n",
    "            format_errors[\"message_missing_key\"] += 1\n",
    "        \n",
    "        if any(k not in (\"role\", \"content\", \"name\", \"function_call\", \"weight\") for k in message):\n",
    "            format_errors[\"message_unrecognized_key\"] += 1\n",
    "        \n",
    "        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\", \"function\"):\n",
    "            format_errors[\"unrecognized_role\"] += 1\n",
    "            \n",
    "        content = message.get(\"content\", None)\n",
    "        function_call = message.get(\"function_call\", None)\n",
    "        \n",
    "        if (not content and not function_call) or not isinstance(content, str):\n",
    "            format_errors[\"missing_content\"] += 1\n",
    "            print(content)\n",
    "    \n",
    "    if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
    "        format_errors[\"example_missing_assistant_message\"] += 1\n",
    "\n",
    "if format_errors:\n",
    "    print(\"Found errors:\")\n",
    "    for k, v in format_errors.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "else:\n",
    "    print(\"No errors found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uploading the training file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../api.key\", 'r') as file:\n",
    "    openai_api_key = file.read()\n",
    "\n",
    "openai_client = OpenAI(api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_client.files.create(\n",
    "  file=open(\"gpt_fine_tune_data.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mental-health",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

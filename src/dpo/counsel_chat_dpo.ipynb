{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/mn27889/miniconda3/envs/mental-health/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from transformers.trainer import TrainingArguments\n",
    "from tqdm import tqdm\n",
    "from peft import LoraConfig, TaskType, AutoPeftModelForCausalLM\n",
    "from trl.trainer import ConstantLengthDataset\n",
    "from trl import SFTTrainer, DPOTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the LLaMA Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/mn27889/miniconda3/envs/mental-health/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name = \"huggyllama/llama-7b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Counsel Chat Dataset in preference pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"nbertagnolli/counsel-chat\")\n",
    "question_id, question_id_index = np.unique(dataset['train']['questionID'], return_index=True)\n",
    "dataset_length = len(dataset['train']['questionID'])\n",
    "question_id_index = list(question_id_index)\n",
    "question_id_index.append(dataset_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = []\n",
    "preferred_answers = []\n",
    "rejected_answers = []\n",
    "\n",
    "for i in range(0, len(question_id_index)-1):\n",
    "    \n",
    "    index_val_first = int(question_id_index[i])\n",
    "    index_val_last = int(question_id_index[i+1]-1)\n",
    "    \n",
    "    questions.append(dataset[\"train\"][index_val_first]['questionTitle'])\n",
    "    preferred_answers.append(dataset[\"train\"][index_val_first]['answerText'])\n",
    "    rejected_answers.append(dataset[\"train\"][index_val_last]['answerText'])\n",
    "    \n",
    "\n",
    "counsel_data_pairs = {   \n",
    "                        'question': questions,\n",
    "                        'preferred_answer': preferred_answers,\n",
    "                        'rejected_answer': rejected_answers\n",
    "                    }\n",
    "\n",
    "counsel_dataset = Dataset.from_dict(counsel_data_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "counsel_dataset = counsel_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_data = counsel_dataset['train']\n",
    "test_data = counsel_dataset['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing constant length dataset for TRL trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sample_text(example):\n",
    "    text = f\"Question: {example['question']}\\n\\nCounsel Advice: {example['preferred_answer']}\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chars_token_ratio(dataset, tokenizer):\n",
    "    '''\n",
    "    Estimate the average number of characters per token in the dataset\n",
    "    '''\n",
    "    \n",
    "    total_characters, total_tokens = 0, 0\n",
    "    dataset_length = len(dataset['question'])\n",
    "    for _, example in tqdm(zip(range(dataset_length), iter(dataset)), total=dataset_length):\n",
    "        text = prepare_sample_text(example)\n",
    "        total_characters += len(text)\n",
    "        if tokenizer.is_fast:\n",
    "            total_tokens += len(tokenizer(text).tokens())\n",
    "        else:\n",
    "            total_tokens += len(tokenizer.tokenize(text))\n",
    "    \n",
    "    return total_characters/total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 846/846 [00:00<00:00, 1346.71it/s]\n"
     ]
    }
   ],
   "source": [
    "chars_per_token = chars_token_ratio(train_data, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ConstantLengthDataset(\n",
    "    tokenizer,\n",
    "    train_data,\n",
    "    formatting_func=prepare_sample_text,\n",
    "    infinite=True,\n",
    "    seq_length=1024,\n",
    "    chars_per_token=chars_per_token\n",
    ")\n",
    "\n",
    "test_dataset = ConstantLengthDataset(\n",
    "    tokenizer,\n",
    "    test_data,\n",
    "    formatting_func=prepare_sample_text,\n",
    "    infinite=False,\n",
    "    seq_length=1024,\n",
    "    chars_per_token=chars_per_token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/mn27889/miniconda3/envs/mental-health/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.91s/it]\n"
     ]
    }
   ],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config = bnb_config,\n",
    "    device_map = \"auto\",\n",
    "    # device_map = {\"\":0},\n",
    "    torch_dtype = torch.bfloat16,\n",
    "    trust_remote_code = False\n",
    ")\n",
    "\n",
    "base_model.config.use_cache=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    bias=\"none\",\n",
    "    task_type = TaskType.CAUSAL_LM\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args=TrainingArguments(\n",
    "    output_dir=\"counsel_data_sft\",\n",
    "    num_train_epochs=5,\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps = 25,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=16,\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=True,\n",
    "    group_by_length=False,\n",
    "    learning_rate=1e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_steps=50,\n",
    "    weight_decay=0.05,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    fp16=True,\n",
    "    remove_unused_columns=False,\n",
    "    report_to=\"none\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/mn27889/miniconda3/envs/mental-health/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:225: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "sft_trainer = SFTTrainer(\n",
    "        model=base_model,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        peft_config=peft_config,\n",
    "        packing=True,\n",
    "        max_seq_length=None,\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_args\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sft_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direct Preference Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_prompt_and_responses(samples):\n",
    "    return {\n",
    "        \"prompt\":[\n",
    "            \"Question: \" + question + \"\\n\\nCounsel Advice: \" for question in samples[\"question\"]\n",
    "        ],\n",
    "        \"chosen\": samples[\"preferred_answer\"],\n",
    "        \"rejected\": samples[\"rejected_answer\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 846/846 [00:00<00:00, 23445.18 examples/s]\n"
     ]
    }
   ],
   "source": [
    "original_columns = train_data.column_names\n",
    "\n",
    "dpo_train_data = train_data.map(\n",
    "                    return_prompt_and_responses,\n",
    "                    batched=True,\n",
    "                    remove_columns=original_columns,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 94/94 [00:00<00:00, 19034.64 examples/s]\n"
     ]
    }
   ],
   "source": [
    "original_columns = test_data.column_names\n",
    "\n",
    "dpo_test_data = test_data.map(\n",
    "                    return_prompt_and_responses,\n",
    "                    batched=True,\n",
    "                    remove_columns=original_columns,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.12s/it]\n"
     ]
    }
   ],
   "source": [
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    \"counsel_data_sft/checkpoint-135\",\n",
    "    quantization_config = bnb_config,\n",
    "    # device_map = \"auto\",\n",
    "    device_map = {\"\":0},\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    is_trainable=True\n",
    ")\n",
    "\n",
    "model.config.use_cache=False\n",
    "\n",
    "# model_ref = AutoPeftModelForCausalLM.from_pretrained(\n",
    "#     \"counsel_data_sft/checkpoint-135\",\n",
    "#     device_map=\"auto\",\n",
    "#     low_cpu_mem_usage=True,\n",
    "#     torch_dtype=torch.float16,\n",
    "#     load_in_4bit=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args=TrainingArguments(\n",
    "    output_dir=\"counsel_data_dpo\",\n",
    "    num_train_epochs=5,\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps = 1,\n",
    "    logging_steps = 1,\n",
    "    logging_dir=\"dpo_logs\",\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=True,\n",
    "    group_by_length=False,\n",
    "    learning_rate=1e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_steps=50,\n",
    "    weight_decay=0.05,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    fp16=True,\n",
    "    remove_unused_columns=False,\n",
    "    report_to=\"none\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/mn27889/miniconda3/envs/mental-health/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:249: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 94/94 [00:00<00:00, 940.67 examples/s]\n",
      "Map: 100%|██████████| 94/94 [00:00<00:00, 992.42 examples/s]\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "dpo_trainer = DPOTrainer(\n",
    "    model,\n",
    "    ref_model=None,\n",
    "    args=training_args,\n",
    "    beta=0.1,\n",
    "    train_dataset=dpo_test_data,\n",
    "    eval_dataset=dpo_test_data,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length = 1024,\n",
    "    max_prompt_length=1024,\n",
    "    peft_config=peft_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/mn27889/miniconda3/envs/mental-health/lib/python3.12/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/data/mn27889/miniconda3/envs/mental-health/lib/python3.12/site-packages/torch/utils/checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 8/15 31:44 < 37:01, 0.00 it/s, Epoch 2.33/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rewards/chosen</th>\n",
       "      <th>Rewards/rejected</th>\n",
       "      <th>Rewards/accuracies</th>\n",
       "      <th>Rewards/margins</th>\n",
       "      <th>Logps/rejected</th>\n",
       "      <th>Logps/chosen</th>\n",
       "      <th>Logits/rejected</th>\n",
       "      <th>Logits/chosen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.693100</td>\n",
       "      <td>0.693146</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>0.256944</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>-485.850006</td>\n",
       "      <td>-522.076599</td>\n",
       "      <td>-0.424849</td>\n",
       "      <td>-0.397616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.693100</td>\n",
       "      <td>0.692848</td>\n",
       "      <td>0.000385</td>\n",
       "      <td>-0.000213</td>\n",
       "      <td>0.427083</td>\n",
       "      <td>0.000597</td>\n",
       "      <td>-485.852142</td>\n",
       "      <td>-522.072754</td>\n",
       "      <td>-0.424836</td>\n",
       "      <td>-0.397603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.693100</td>\n",
       "      <td>0.692272</td>\n",
       "      <td>0.001246</td>\n",
       "      <td>-0.000516</td>\n",
       "      <td>0.447917</td>\n",
       "      <td>0.001762</td>\n",
       "      <td>-485.855133</td>\n",
       "      <td>-522.064148</td>\n",
       "      <td>-0.424833</td>\n",
       "      <td>-0.397601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.692000</td>\n",
       "      <td>0.691410</td>\n",
       "      <td>0.002414</td>\n",
       "      <td>-0.001095</td>\n",
       "      <td>0.447917</td>\n",
       "      <td>0.003509</td>\n",
       "      <td>-485.860962</td>\n",
       "      <td>-522.052490</td>\n",
       "      <td>-0.424816</td>\n",
       "      <td>-0.397584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.691900</td>\n",
       "      <td>0.690280</td>\n",
       "      <td>0.004218</td>\n",
       "      <td>-0.001591</td>\n",
       "      <td>0.447917</td>\n",
       "      <td>0.005808</td>\n",
       "      <td>-485.865875</td>\n",
       "      <td>-522.034424</td>\n",
       "      <td>-0.424826</td>\n",
       "      <td>-0.397599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.691300</td>\n",
       "      <td>0.688878</td>\n",
       "      <td>0.006387</td>\n",
       "      <td>-0.002267</td>\n",
       "      <td>0.447917</td>\n",
       "      <td>0.008654</td>\n",
       "      <td>-485.872650</td>\n",
       "      <td>-522.012695</td>\n",
       "      <td>-0.424827</td>\n",
       "      <td>-0.397605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.687900</td>\n",
       "      <td>0.687166</td>\n",
       "      <td>0.008939</td>\n",
       "      <td>-0.003208</td>\n",
       "      <td>0.447917</td>\n",
       "      <td>0.012148</td>\n",
       "      <td>-485.882080</td>\n",
       "      <td>-521.987244</td>\n",
       "      <td>-0.424791</td>\n",
       "      <td>-0.397574</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/mn27889/miniconda3/envs/mental-health/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/data/mn27889/miniconda3/envs/mental-health/lib/python3.12/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/data/mn27889/miniconda3/envs/mental-health/lib/python3.12/site-packages/torch/utils/checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/data/mn27889/miniconda3/envs/mental-health/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/data/mn27889/miniconda3/envs/mental-health/lib/python3.12/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/data/mn27889/miniconda3/envs/mental-health/lib/python3.12/site-packages/torch/utils/checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dpo_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpo_trainer.save_model(\"counsel_data_dpo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mental-health",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

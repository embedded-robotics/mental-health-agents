{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning LLaMA-3.2 3B Instruct\n",
    "\n",
    "This code will fine-tune the `LLaMA-3.2-3B-Instruct` model on the CounselChat dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/mn27889/miniconda3/envs/mental-health-agents/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pickle\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported, train_on_responses_only\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from datasets import Dataset, DatasetDict\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some links to be used for fine-tuning\n",
    "1. https://www.kdnuggets.com/fine-tuning-llama-using-unsloth\n",
    "2. https://www.analyticsvidhya.com/blog/2024/12/fine-tuning-llama-3-2-3b-for-rag/\n",
    "3. https://www.linkedin.com/pulse/step-guide-use-fine-tune-llama-32-dr-oualid-soula-xmnff/\n",
    "4. https://medium.com/@alexandros_chariton/how-to-fine-tune-llama-3-2-instruct-on-your-own-data-a-detailed-guide-e5f522f397d7 (check the data format since it uses generation_token at end of input data since that might not be needed)\n",
    "5. https://drlee.io/step-by-step-guide-fine-tuning-metas-llama-3-2-1b-model-f1262eda36c8\n",
    "6. https://huggingface.co/blog/ImranzamanML/fine-tuning-1b-llama-32-a-comprehensive-article\n",
    "7. https://blog.futuresmart.ai/fine-tune-llama-32-vision-language-model-on-custom-datasets\n",
    "8. https://github.com/meta-llama/llama-cookbook/blob/main/getting-started/finetuning/multigpu_finetuning.md\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the Counsel Chat Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>question</th>\n",
       "      <th>answerText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>relationships</td>\n",
       "      <td>I am currently suffering from erectile dysfunc...</td>\n",
       "      <td>Hi, First and foremost, I want to acknowledge ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>family-conflict</td>\n",
       "      <td>For the past week or so me and my boyfriend ha...</td>\n",
       "      <td>Forgetting one's emotions is impossible.Since ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>depression</td>\n",
       "      <td>I am in high school and have been facing anxie...</td>\n",
       "      <td>Hi Helena,I felt a bit sad when I read this. T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>anxiety</td>\n",
       "      <td>I'm concerned about my boyfriend. I suffer fro...</td>\n",
       "      <td>Hello! Thank you for your question. There are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spirituality</td>\n",
       "      <td>I'm a Christian teenage girl, and I have lost ...</td>\n",
       "      <td>Having sex with your boyfriend is and was a mi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             topic                                           question  \\\n",
       "0    relationships  I am currently suffering from erectile dysfunc...   \n",
       "1  family-conflict  For the past week or so me and my boyfriend ha...   \n",
       "2       depression  I am in high school and have been facing anxie...   \n",
       "3          anxiety  I'm concerned about my boyfriend. I suffer fro...   \n",
       "4     spirituality  I'm a Christian teenage girl, and I have lost ...   \n",
       "\n",
       "                                          answerText  \n",
       "0  Hi, First and foremost, I want to acknowledge ...  \n",
       "1  Forgetting one's emotions is impossible.Since ...  \n",
       "2  Hi Helena,I felt a bit sad when I read this. T...  \n",
       "3  Hello! Thank you for your question. There are ...  \n",
       "4  Having sex with your boyfriend is and was a mi...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('processed_data/counselchat_top_votes_train.pkl', 'rb') as file:\n",
    "    dataset_top_votes_train = pickle.load(file)\n",
    "\n",
    "dataset_top_votes_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>question</th>\n",
       "      <th>answerText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>relationships</td>\n",
       "      <td>I had to go to the emergency room today to get...</td>\n",
       "      <td>It is extremely frustrating when our significa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>marriage</td>\n",
       "      <td>What makes a healthy marriage last? What makes...</td>\n",
       "      <td>This is a fantastic question. In one sentence,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>relationships</td>\n",
       "      <td>I'm a female freshman in high school, and this...</td>\n",
       "      <td>First off, I think it is great that you are wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>intimacy</td>\n",
       "      <td>My wife and I are newly married, about 2 month...</td>\n",
       "      <td>You are newly married, you Have a hectic sched...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>legal-regulatory</td>\n",
       "      <td>I think I have depression, anxiety, bipolar di...</td>\n",
       "      <td>It can be difficult to get counseling if you d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              topic                                           question  \\\n",
       "0     relationships  I had to go to the emergency room today to get...   \n",
       "1          marriage  What makes a healthy marriage last? What makes...   \n",
       "2     relationships  I'm a female freshman in high school, and this...   \n",
       "3          intimacy  My wife and I are newly married, about 2 month...   \n",
       "4  legal-regulatory  I think I have depression, anxiety, bipolar di...   \n",
       "\n",
       "                                          answerText  \n",
       "0  It is extremely frustrating when our significa...  \n",
       "1  This is a fantastic question. In one sentence,...  \n",
       "2  First off, I think it is great that you are wi...  \n",
       "3  You are newly married, you Have a hectic sched...  \n",
       "4  It can be difficult to get counseling if you d...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('processed_data/counselchat_top_votes_test.pkl', 'rb') as file:\n",
    "    dataset_top_votes_test = pickle.load(file)\n",
    "\n",
    "dataset_top_votes_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the HuggingFace dataset using Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = Dataset.from_pandas(dataset_top_votes_train)\n",
    "dataset_test = Dataset.from_pandas(dataset_top_votes_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DatasetDict()\n",
    "dataset['train'] = dataset_train\n",
    "dataset['test'] = dataset_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['topic', 'question', 'answerText'],\n",
       "        num_rows: 690\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['topic', 'question', 'answerText'],\n",
       "        num_rows: 173\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.19: Fast Llama patching. Transformers: 4.50.2.\n",
      "   \\\\   /|    NVIDIA RTX A6000. Num GPUs = 1. Max memory: 47.413 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.4.1+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.0.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post1. FA2 = True]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Using bfloat16 full finetuning which cuts memory usage by 50%.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Llama-3.2-3B-Instruct\",\n",
    "    max_seq_length = 2048,\n",
    "    load_in_4bit = False, # 4 bit quantization to reduce memory\n",
    "    load_in_8bit = False, # [NEW!] A bit more accurate, uses 2x memory\n",
    "    full_finetuning = True, # [NEW!] We have full finetuning now!\n",
    "    dtype=None, #None for auto-detection. Can be torch.bfloat16 or torch.float16 (will be automatically detected)\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up the PEFT settings for the model\n",
    "\n",
    "https://huggingface.co/blog/damjan-k/rslora\\\n",
    "https://medium.com/@fartypantsham/what-rank-r-and-alpha-to-use-in-lora-in-llm-1b4f025fd133"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Full finetuning is enabled, so .get_peft_model has no effect\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 64, #max_full_rank=64 by default in FastLanguageModel\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 64, #scaling_factor = lora_alpha/r. If we select lora_alpha = 2 * r then it will multiply the adapter weights by 2 which can be un-ncessary\n",
    "    lora_dropout = 0.1,\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    use_rslora = True,\n",
    "    loftq_config = None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forming the chat template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to apply the chat template\n",
    "def format_chat_template(example):\n",
    "        \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert mental health professional trained to counsel and guide patients suffering from ill mental-health\"},\n",
    "        {\"role\": \"user\", \"content\": example['question']},\n",
    "        {\"role\": \"assistant\", \"content\": example['answerText']}\n",
    "    ]\n",
    "    \n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "    return {\"text\": prompt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 690/690 [00:00<00:00, 5035.10 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 173/173 [00:00<00:00, 6026.25 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_formatted = dataset.map(format_chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 30 Mar 2025\n",
      "\n",
      "You are an expert mental health professional trained to counsel and guide patients suffering from ill mental-health<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "I am currently suffering from erectile dysfunction and have tried Viagra, Cialis, etc. Nothing seemed to work. My girlfriend of 3 years is very sexually frustrated. I told her that it is okay for her to have sex with other men. Is that really okay? Is it okay for my girlfriend to have sex with other men since I can't sexually perform?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Hi, First and foremost, I want to acknowledge your efforts to gain (your) ideal erectile function. If the medications are not working and you have taken them as prescribed, I would encourage you to seek the help of a sex therapist as the dysfunction may be due to a psychological and/or relational issue rather than a physical/medical one. As for your question, only you can answer this. Is it OK? Are you OK with her sleeping with others? Have you thought through what this may look like, feel like, become for you and her? Opening up a relationship is a choice only the people in the relationship can answer. Even then, the answer may change at any point by either of you. I encourage you to also determine what the intention is underneath your telling your girlfriend she could sleep with others. Be clear with the intention and then together have continuous conversations about the expectations of opening up (i.e.: are there any kinds of sex that is off limits, areas of the body where touch or intimacy is not allowed, are uses of safer sex required or not, do you want to know the details or not, so forth). An excellent resource would be the book \"Opening Up\" by Tristan Taormino. I wish you the best of luck!Dr. Lily Zehner, MFT-C<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "print(dataset_formatted['train']['text'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing the TRL SFTTrainer and related Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Tokenizing [\"text\"] (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 690/690 [00:02<00:00, 329.36 examples/s]\n",
      "Unsloth: Tokenizing [\"text\"] (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 173/173 [00:01<00:00, 104.93 examples/s]\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "full_model_path = \"./llama32-sft-full-counselchat\" #use for full finetuning\n",
    "# peft_model_path = \"./llama32-sft-peft-counselchat\" #use for LoRA based fine-tuning\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "        output_dir=full_model_path,\n",
    "        per_device_train_batch_size=1,\n",
    "        per_device_eval_batch_size=2,\n",
    "        # gradient_accumulation_steps=4,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=0.1,\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=50,\n",
    "        save_strategy=\"epoch\",\n",
    "        warmup_steps = 5,\n",
    "        num_train_epochs = 3,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        # load_best_model_at_end=True,\n",
    "        # metric_for_best_model=\"eval_loss\",\n",
    "        # greater_is_better=False,\n",
    "        seed = 42,\n",
    "        report_to = \"none\",\n",
    "    )\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset=dataset_formatted[\"train\"],\n",
    "    eval_dataset=dataset_formatted[\"test\"],\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = 2048,\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer), #only use when using train_on_responses_only()\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['topic', 'question', 'answerText', 'text', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 690\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 30 Mar 2025\n",
      "\n",
      "You are an expert mental health professional trained to counsel and guide patients suffering from ill mental-health<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "I am currently suffering from erectile dysfunction and have tried Viagra, Cialis, etc. Nothing seemed to work. My girlfriend of 3 years is very sexually frustrated. I told her that it is okay for her to have sex with other men. Is that really okay? Is it okay for my girlfriend to have sex with other men since I can't sexually perform?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Hi, First and foremost, I want to acknowledge your efforts to gain (your) ideal erectile function. If the medications are not working and you have taken them as prescribed, I would encourage you to seek the help of a sex therapist as the dysfunction may be due to a psychological and/or relational issue rather than a physical/medical one. As for your question, only you can answer this. Is it OK? Are you OK with her sleeping with others? Have you thought through what this may look like, feel like, become for you and her? Opening up a relationship is a choice only the people in the relationship can answer. Even then, the answer may change at any point by either of you. I encourage you to also determine what the intention is underneath your telling your girlfriend she could sleep with others. Be clear with the intention and then together have continuous conversations about the expectations of opening up (i.e.: are there any kinds of sex that is off limits, areas of the body where touch or intimacy is not allowed, are uses of safer sex required or not, do you want to know the details or not, so forth). An excellent resource would be the book \"Opening Up\" by Tristan Taormino. I wish you the best of luck!Dr. Lily Zehner, MFT-C<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(trainer.train_dataset['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Only Focus on the `Response Part` for the generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=64): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 690/690 [00:01<00:00, 387.28 examples/s]\n",
      "Map (num_proc=64): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 173/173 [00:01<00:00, 121.00 examples/s]\n"
     ]
    }
   ],
   "source": [
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
    "    response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['topic', 'question', 'answerText', 'text', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 690\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " 13347,\n",
       " 11,\n",
       " 5629,\n",
       " 323,\n",
       " 43780,\n",
       " 11,\n",
       " 358,\n",
       " 1390,\n",
       " 311,\n",
       " 25670,\n",
       " 701,\n",
       " 9045,\n",
       " 311,\n",
       " 8895,\n",
       " 320,\n",
       " 22479,\n",
       " 8,\n",
       " 10728,\n",
       " 57217,\n",
       " 734,\n",
       " 13,\n",
       " 1442,\n",
       " 279,\n",
       " 31010,\n",
       " 527,\n",
       " 539,\n",
       " 3318,\n",
       " 323,\n",
       " 499,\n",
       " 617,\n",
       " 4529,\n",
       " 1124,\n",
       " 439,\n",
       " 32031,\n",
       " 11,\n",
       " 358,\n",
       " 1053,\n",
       " 15253,\n",
       " 499,\n",
       " 311,\n",
       " 6056,\n",
       " 279,\n",
       " 1520,\n",
       " 315,\n",
       " 264,\n",
       " 1877,\n",
       " 42863,\n",
       " 439,\n",
       " 279,\n",
       " 32403,\n",
       " 1253,\n",
       " 387,\n",
       " 4245,\n",
       " 311,\n",
       " 264,\n",
       " 24064,\n",
       " 323,\n",
       " 5255,\n",
       " 72283,\n",
       " 4360,\n",
       " 4856,\n",
       " 1109,\n",
       " 264,\n",
       " 7106,\n",
       " 14,\n",
       " 69216,\n",
       " 832,\n",
       " 13,\n",
       " 1666,\n",
       " 369,\n",
       " 701,\n",
       " 3488,\n",
       " 11,\n",
       " 1193,\n",
       " 499,\n",
       " 649,\n",
       " 4320,\n",
       " 420,\n",
       " 13,\n",
       " 2209,\n",
       " 433,\n",
       " 10619,\n",
       " 30,\n",
       " 8886,\n",
       " 499,\n",
       " 10619,\n",
       " 449,\n",
       " 1077,\n",
       " 21811,\n",
       " 449,\n",
       " 3885,\n",
       " 30,\n",
       " 12522,\n",
       " 499,\n",
       " 3463,\n",
       " 1555,\n",
       " 1148,\n",
       " 420,\n",
       " 1253,\n",
       " 1427,\n",
       " 1093,\n",
       " 11,\n",
       " 2733,\n",
       " 1093,\n",
       " 11,\n",
       " 3719,\n",
       " 369,\n",
       " 499,\n",
       " 323,\n",
       " 1077,\n",
       " 30,\n",
       " 41137,\n",
       " 709,\n",
       " 264,\n",
       " 5133,\n",
       " 374,\n",
       " 264,\n",
       " 5873,\n",
       " 1193,\n",
       " 279,\n",
       " 1274,\n",
       " 304,\n",
       " 279,\n",
       " 5133,\n",
       " 649,\n",
       " 4320,\n",
       " 13,\n",
       " 7570,\n",
       " 1243,\n",
       " 11,\n",
       " 279,\n",
       " 4320,\n",
       " 1253,\n",
       " 2349,\n",
       " 520,\n",
       " 904,\n",
       " 1486,\n",
       " 555,\n",
       " 3060,\n",
       " 315,\n",
       " 499,\n",
       " 13,\n",
       " 358,\n",
       " 15253,\n",
       " 499,\n",
       " 311,\n",
       " 1101,\n",
       " 8417,\n",
       " 1148,\n",
       " 279,\n",
       " 14944,\n",
       " 374,\n",
       " 30456,\n",
       " 701,\n",
       " 11890,\n",
       " 701,\n",
       " 23601,\n",
       " 1364,\n",
       " 1436,\n",
       " 6212,\n",
       " 449,\n",
       " 3885,\n",
       " 13,\n",
       " 2893,\n",
       " 2867,\n",
       " 449,\n",
       " 279,\n",
       " 14944,\n",
       " 323,\n",
       " 1243,\n",
       " 3871,\n",
       " 617,\n",
       " 19815,\n",
       " 21633,\n",
       " 922,\n",
       " 279,\n",
       " 17078,\n",
       " 315,\n",
       " 8736,\n",
       " 709,\n",
       " 320,\n",
       " 72,\n",
       " 1770,\n",
       " 18976,\n",
       " 527,\n",
       " 1070,\n",
       " 904,\n",
       " 13124,\n",
       " 315,\n",
       " 1877,\n",
       " 430,\n",
       " 374,\n",
       " 1022,\n",
       " 13693,\n",
       " 11,\n",
       " 5789,\n",
       " 315,\n",
       " 279,\n",
       " 2547,\n",
       " 1405,\n",
       " 5916,\n",
       " 477,\n",
       " 66264,\n",
       " 374,\n",
       " 539,\n",
       " 5535,\n",
       " 11,\n",
       " 527,\n",
       " 5829,\n",
       " 315,\n",
       " 30549,\n",
       " 1877,\n",
       " 2631,\n",
       " 477,\n",
       " 539,\n",
       " 11,\n",
       " 656,\n",
       " 499,\n",
       " 1390,\n",
       " 311,\n",
       " 1440,\n",
       " 279,\n",
       " 3649,\n",
       " 477,\n",
       " 539,\n",
       " 11,\n",
       " 779,\n",
       " 13544,\n",
       " 570,\n",
       " 1556,\n",
       " 9250,\n",
       " 5211,\n",
       " 1053,\n",
       " 387,\n",
       " 279,\n",
       " 2363,\n",
       " 330,\n",
       " 52398,\n",
       " 3216,\n",
       " 1,\n",
       " 555,\n",
       " 97690,\n",
       " 24172,\n",
       " 494,\n",
       " 3394,\n",
       " 13,\n",
       " 358,\n",
       " 6562,\n",
       " 499,\n",
       " 279,\n",
       " 1888,\n",
       " 315,\n",
       " 15369,\n",
       " 0,\n",
       " 9023,\n",
       " 13,\n",
       " 48390,\n",
       " 1901,\n",
       " 2701,\n",
       " 1215,\n",
       " 11,\n",
       " 386,\n",
       " 4082,\n",
       " 7813,\n",
       " 128009]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The labels are created which only contain response. Left Padding is implemented and all the padding tokens are given a score of -100 to avoid loss calculation for pad_tokens\n",
    "trainer.train_dataset['labels'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 690 | Num Epochs = 3 | Total steps = 2,070\n",
      "O^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 1\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 1 x 1) = 1\n",
      " \"-____-\"     Trainable parameters = 3,212,749,824/3,212,749,824 (100.00% trained)\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2070' max='2070' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2070/2070 15:58, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>207</td>\n",
       "      <td>4.931900</td>\n",
       "      <td>4.849617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>414</td>\n",
       "      <td>4.691300</td>\n",
       "      <td>4.703162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>621</td>\n",
       "      <td>4.742900</td>\n",
       "      <td>4.462102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>828</td>\n",
       "      <td>3.134000</td>\n",
       "      <td>4.491683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1035</td>\n",
       "      <td>3.183600</td>\n",
       "      <td>4.456414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1242</td>\n",
       "      <td>3.048100</td>\n",
       "      <td>4.213548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1449</td>\n",
       "      <td>1.910400</td>\n",
       "      <td>4.965687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1656</td>\n",
       "      <td>0.742200</td>\n",
       "      <td>5.107991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1863</td>\n",
       "      <td>0.884200</td>\n",
       "      <td>5.131706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2070</td>\n",
       "      <td>0.748900</td>\n",
       "      <td>5.203910</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the model and tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For full fine-tuning, current technique doesn't work well. Use the below technique to save the model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./llama32-sft-full-counselchat/tokenizer_config.json',\n",
       " './llama32-sft-full-counselchat/special_tokens_map.json',\n",
       " './llama32-sft-full-counselchat/tokenizer.json')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_model_path = \"./llama32-sft-full-counselchat\" #use for full finetuning\n",
    "trainer.model.save_pretrained(full_model_path)\n",
    "trainer.tokenizer.save_pretrained(full_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the LoRA into the base model and then save 16-bit version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model_path = \"./llama32-sft-peft-counselchat\"\n",
    "model.save_pretrained_merged(peft_model_path, tokenizer, save_method = \"merged_16bit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just save the LoRA Adapters without merging with base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained_merged(peft_model_path, tokenizer, save_method = \"lora\",) #WJust Save LoRA Adapters\n",
    "\n",
    "# # Or run the two below statements\n",
    "# model.save_pretrained(peft_model_path)\n",
    "# tokenizer.save_pretrained(peft_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model_path = \"./llama32-sft-full-counselchat\"\n",
    "# peft_model_path = \"./llama32-sft-peft-counselchat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = full_model_path,\n",
    "    max_seq_length = 2048,\n",
    "    load_in_4bit = False, # 4 bit quantization to reduce memory\n",
    "    load_in_8bit = False, # [NEW!] A bit more accurate, uses 2x memory\n",
    "    full_finetuning = True, # [NEW!] We have full finetuning now!\n",
    "    dtype=None, #None for auto-detection. Can be torch.bfloat16 or torch.float16 (will be automatically detected)\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=0\n",
    "\n",
    "print(dataset['train']['question'][idx])\n",
    "\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "messages = [{\"role\": \"system\", \"content\": \"You are an expert mental health professional trained to counsel and guide patients suffering from ill mental-health\"},\n",
    "    {\"role\": \"user\", \"content\": dataset['train']['question'][idx]}]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    \n",
    "inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=2048, num_return_sequences=1)\n",
    "\n",
    "text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(text.split(\"assistant\")[1])\n",
    "\n",
    "print('---------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mental-health-agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning LLaMA-3.2 3B Instruct\n",
    "\n",
    "This code will fine-tune the `LLaMA-3.2-3B-Instruct` model on the CounselChat dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/mn27889/miniconda3/envs/mental-health-agents/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pickle\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported, train_on_responses_only\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from datasets import Dataset, DatasetDict\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some links to be used for fine-tuning\n",
    "1. https://www.kdnuggets.com/fine-tuning-llama-using-unsloth\n",
    "2. https://www.analyticsvidhya.com/blog/2024/12/fine-tuning-llama-3-2-3b-for-rag/\n",
    "3. https://www.linkedin.com/pulse/step-guide-use-fine-tune-llama-32-dr-oualid-soula-xmnff/\n",
    "4. https://medium.com/@alexandros_chariton/how-to-fine-tune-llama-3-2-instruct-on-your-own-data-a-detailed-guide-e5f522f397d7 (check the data format since it uses generation_token at end of input data since that might not be needed)\n",
    "5. https://drlee.io/step-by-step-guide-fine-tuning-metas-llama-3-2-1b-model-f1262eda36c8\n",
    "6. https://huggingface.co/blog/ImranzamanML/fine-tuning-1b-llama-32-a-comprehensive-article\n",
    "7. https://blog.futuresmart.ai/fine-tune-llama-32-vision-language-model-on-custom-datasets\n",
    "8. https://github.com/meta-llama/llama-cookbook/blob/main/getting-started/finetuning/multigpu_finetuning.md\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the Counsel Chat Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>question</th>\n",
       "      <th>answerText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>relationships</td>\n",
       "      <td>I am currently suffering from erectile dysfunc...</td>\n",
       "      <td>Hi, First and foremost, I want to acknowledge ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>family-conflict</td>\n",
       "      <td>For the past week or so me and my boyfriend ha...</td>\n",
       "      <td>Forgetting one's emotions is impossible.Since ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>depression</td>\n",
       "      <td>I am in high school and have been facing anxie...</td>\n",
       "      <td>Hi Helena,I felt a bit sad when I read this. T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>anxiety</td>\n",
       "      <td>I'm concerned about my boyfriend. I suffer fro...</td>\n",
       "      <td>Hello! Thank you for your question. There are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spirituality</td>\n",
       "      <td>I'm a Christian teenage girl, and I have lost ...</td>\n",
       "      <td>Having sex with your boyfriend is and was a mi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             topic                                           question  \\\n",
       "0    relationships  I am currently suffering from erectile dysfunc...   \n",
       "1  family-conflict  For the past week or so me and my boyfriend ha...   \n",
       "2       depression  I am in high school and have been facing anxie...   \n",
       "3          anxiety  I'm concerned about my boyfriend. I suffer fro...   \n",
       "4     spirituality  I'm a Christian teenage girl, and I have lost ...   \n",
       "\n",
       "                                          answerText  \n",
       "0  Hi, First and foremost, I want to acknowledge ...  \n",
       "1  Forgetting one's emotions is impossible.Since ...  \n",
       "2  Hi Helena,I felt a bit sad when I read this. T...  \n",
       "3  Hello! Thank you for your question. There are ...  \n",
       "4  Having sex with your boyfriend is and was a mi...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('processed_data/counselchat_top_votes_train.pkl', 'rb') as file:\n",
    "    dataset_top_votes_train = pickle.load(file)\n",
    "\n",
    "dataset_top_votes_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>question</th>\n",
       "      <th>answerText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>relationships</td>\n",
       "      <td>I had to go to the emergency room today to get...</td>\n",
       "      <td>It is extremely frustrating when our significa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>marriage</td>\n",
       "      <td>What makes a healthy marriage last? What makes...</td>\n",
       "      <td>This is a fantastic question. In one sentence,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>relationships</td>\n",
       "      <td>I'm a female freshman in high school, and this...</td>\n",
       "      <td>First off, I think it is great that you are wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>intimacy</td>\n",
       "      <td>My wife and I are newly married, about 2 month...</td>\n",
       "      <td>You are newly married, you Have a hectic sched...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>legal-regulatory</td>\n",
       "      <td>I think I have depression, anxiety, bipolar di...</td>\n",
       "      <td>It can be difficult to get counseling if you d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              topic                                           question  \\\n",
       "0     relationships  I had to go to the emergency room today to get...   \n",
       "1          marriage  What makes a healthy marriage last? What makes...   \n",
       "2     relationships  I'm a female freshman in high school, and this...   \n",
       "3          intimacy  My wife and I are newly married, about 2 month...   \n",
       "4  legal-regulatory  I think I have depression, anxiety, bipolar di...   \n",
       "\n",
       "                                          answerText  \n",
       "0  It is extremely frustrating when our significa...  \n",
       "1  This is a fantastic question. In one sentence,...  \n",
       "2  First off, I think it is great that you are wi...  \n",
       "3  You are newly married, you Have a hectic sched...  \n",
       "4  It can be difficult to get counseling if you d...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('processed_data/counselchat_top_votes_test.pkl', 'rb') as file:\n",
    "    dataset_top_votes_test = pickle.load(file)\n",
    "\n",
    "dataset_top_votes_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the HuggingFace dataset using Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = Dataset.from_pandas(dataset_top_votes_train)\n",
    "dataset_test = Dataset.from_pandas(dataset_top_votes_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DatasetDict()\n",
    "dataset['train'] = dataset_train\n",
    "dataset['test'] = dataset_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['topic', 'question', 'answerText'],\n",
       "        num_rows: 690\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['topic', 'question', 'answerText'],\n",
       "        num_rows: 173\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.19: Fast Llama patching. Transformers: 4.50.2.\n",
      "   \\\\   /|    NVIDIA RTX A6000. Num GPUs = 1. Max memory: 47.413 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.4.1+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.0.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post1. FA2 = True]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Llama-3.2-3B-Instruct\",\n",
    "    max_seq_length = 2048,\n",
    "    load_in_4bit = False, # 4 bit quantization to reduce memory\n",
    "    load_in_8bit = True, # [NEW!] A bit more accurate, uses 2x memory\n",
    "    full_finetuning = False, # [NEW!] We have full finetuning now!\n",
    "    dtype=None, #None for auto-detection. Can be torch.bfloat16 or torch.float16 (will be automatically detected)\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up the PEFT settings for the model\n",
    "\n",
    "https://huggingface.co/blog/damjan-k/rslora\\\n",
    "https://medium.com/@fartypantsham/what-rank-r-and-alpha-to-use-in-lora-in-llm-1b4f025fd133"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Making `model.base_model.model.model` require gradients\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 64, #max_full_rank=64 by default in FastLanguageModel\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 64, #scaling_factor = lora_alpha/r. If we select lora_alpha = 2 * r then it will multiply the adapter weights by 2 which can be un-ncessary\n",
    "    lora_dropout = 0.1,\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    use_rslora = True,\n",
    "    loftq_config = None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forming the chat template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to apply the chat template\n",
    "def format_chat_template(example):\n",
    "        \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert mental health professional trained to counsel and guide patients suffering from ill mental-health\"},\n",
    "        {\"role\": \"user\", \"content\": example['question']},\n",
    "        {\"role\": \"assistant\", \"content\": example['answerText']}\n",
    "    ]\n",
    "    \n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "    return {\"text\": prompt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 690/690 [00:00<00:00, 3674.87 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 173/173 [00:00<00:00, 4142.04 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_formatted = dataset.map(format_chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 30 Mar 2025\n",
      "\n",
      "You are an expert mental health professional trained to counsel and guide patients suffering from ill mental-health<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "I am currently suffering from erectile dysfunction and have tried Viagra, Cialis, etc. Nothing seemed to work. My girlfriend of 3 years is very sexually frustrated. I told her that it is okay for her to have sex with other men. Is that really okay? Is it okay for my girlfriend to have sex with other men since I can't sexually perform?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Hi, First and foremost, I want to acknowledge your efforts to gain (your) ideal erectile function. If the medications are not working and you have taken them as prescribed, I would encourage you to seek the help of a sex therapist as the dysfunction may be due to a psychological and/or relational issue rather than a physical/medical one. As for your question, only you can answer this. Is it OK? Are you OK with her sleeping with others? Have you thought through what this may look like, feel like, become for you and her? Opening up a relationship is a choice only the people in the relationship can answer. Even then, the answer may change at any point by either of you. I encourage you to also determine what the intention is underneath your telling your girlfriend she could sleep with others. Be clear with the intention and then together have continuous conversations about the expectations of opening up (i.e.: are there any kinds of sex that is off limits, areas of the body where touch or intimacy is not allowed, are uses of safer sex required or not, do you want to know the details or not, so forth). An excellent resource would be the book \"Opening Up\" by Tristan Taormino. I wish you the best of luck!Dr. Lily Zehner, MFT-C<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "print(dataset_formatted['train']['text'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing the TRL SFTTrainer and related Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Tokenizing [\"text\"] (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 690/690 [00:02<00:00, 259.08 examples/s]\n",
      "Unsloth: Tokenizing [\"text\"] (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 173/173 [00:02<00:00, 83.07 examples/s] \n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "# full_model_path = \"./llama32-sft-full-counselchat\" #use for full finetuning\n",
    "peft_model_path = \"./llama32-sft-peft-counselchat\" #use for LoRA based fine-tuning\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "        output_dir=peft_model_path,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=2,\n",
    "        # gradient_accumulation_steps=4,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=0.1,\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=50,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=0.1,\n",
    "        warmup_steps = 5,\n",
    "        num_train_epochs = 3,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "        seed = 42,\n",
    "        report_to = \"none\",\n",
    "    )\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset=dataset_formatted[\"train\"],\n",
    "    eval_dataset=dataset_formatted[\"test\"],\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = 2048,\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer), #only use when using train_on_responses_only()\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['topic', 'question', 'answerText', 'text', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 690\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 30 Mar 2025\n",
      "\n",
      "You are an expert mental health professional trained to counsel and guide patients suffering from ill mental-health<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "I am currently suffering from erectile dysfunction and have tried Viagra, Cialis, etc. Nothing seemed to work. My girlfriend of 3 years is very sexually frustrated. I told her that it is okay for her to have sex with other men. Is that really okay? Is it okay for my girlfriend to have sex with other men since I can't sexually perform?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Hi, First and foremost, I want to acknowledge your efforts to gain (your) ideal erectile function. If the medications are not working and you have taken them as prescribed, I would encourage you to seek the help of a sex therapist as the dysfunction may be due to a psychological and/or relational issue rather than a physical/medical one. As for your question, only you can answer this. Is it OK? Are you OK with her sleeping with others? Have you thought through what this may look like, feel like, become for you and her? Opening up a relationship is a choice only the people in the relationship can answer. Even then, the answer may change at any point by either of you. I encourage you to also determine what the intention is underneath your telling your girlfriend she could sleep with others. Be clear with the intention and then together have continuous conversations about the expectations of opening up (i.e.: are there any kinds of sex that is off limits, areas of the body where touch or intimacy is not allowed, are uses of safer sex required or not, do you want to know the details or not, so forth). An excellent resource would be the book \"Opening Up\" by Tristan Taormino. I wish you the best of luck!Dr. Lily Zehner, MFT-C<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(trainer.train_dataset['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Only Focus on the `Response Part` for the generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=64): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 690/690 [00:02<00:00, 282.90 examples/s]\n",
      "Map (num_proc=64): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 173/173 [00:01<00:00, 89.44 examples/s] \n"
     ]
    }
   ],
   "source": [
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
    "    response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['topic', 'question', 'answerText', 'text', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 690\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " 13347,\n",
       " 11,\n",
       " 5629,\n",
       " 323,\n",
       " 43780,\n",
       " 11,\n",
       " 358,\n",
       " 1390,\n",
       " 311,\n",
       " 25670,\n",
       " 701,\n",
       " 9045,\n",
       " 311,\n",
       " 8895,\n",
       " 320,\n",
       " 22479,\n",
       " 8,\n",
       " 10728,\n",
       " 57217,\n",
       " 734,\n",
       " 13,\n",
       " 1442,\n",
       " 279,\n",
       " 31010,\n",
       " 527,\n",
       " 539,\n",
       " 3318,\n",
       " 323,\n",
       " 499,\n",
       " 617,\n",
       " 4529,\n",
       " 1124,\n",
       " 439,\n",
       " 32031,\n",
       " 11,\n",
       " 358,\n",
       " 1053,\n",
       " 15253,\n",
       " 499,\n",
       " 311,\n",
       " 6056,\n",
       " 279,\n",
       " 1520,\n",
       " 315,\n",
       " 264,\n",
       " 1877,\n",
       " 42863,\n",
       " 439,\n",
       " 279,\n",
       " 32403,\n",
       " 1253,\n",
       " 387,\n",
       " 4245,\n",
       " 311,\n",
       " 264,\n",
       " 24064,\n",
       " 323,\n",
       " 5255,\n",
       " 72283,\n",
       " 4360,\n",
       " 4856,\n",
       " 1109,\n",
       " 264,\n",
       " 7106,\n",
       " 14,\n",
       " 69216,\n",
       " 832,\n",
       " 13,\n",
       " 1666,\n",
       " 369,\n",
       " 701,\n",
       " 3488,\n",
       " 11,\n",
       " 1193,\n",
       " 499,\n",
       " 649,\n",
       " 4320,\n",
       " 420,\n",
       " 13,\n",
       " 2209,\n",
       " 433,\n",
       " 10619,\n",
       " 30,\n",
       " 8886,\n",
       " 499,\n",
       " 10619,\n",
       " 449,\n",
       " 1077,\n",
       " 21811,\n",
       " 449,\n",
       " 3885,\n",
       " 30,\n",
       " 12522,\n",
       " 499,\n",
       " 3463,\n",
       " 1555,\n",
       " 1148,\n",
       " 420,\n",
       " 1253,\n",
       " 1427,\n",
       " 1093,\n",
       " 11,\n",
       " 2733,\n",
       " 1093,\n",
       " 11,\n",
       " 3719,\n",
       " 369,\n",
       " 499,\n",
       " 323,\n",
       " 1077,\n",
       " 30,\n",
       " 41137,\n",
       " 709,\n",
       " 264,\n",
       " 5133,\n",
       " 374,\n",
       " 264,\n",
       " 5873,\n",
       " 1193,\n",
       " 279,\n",
       " 1274,\n",
       " 304,\n",
       " 279,\n",
       " 5133,\n",
       " 649,\n",
       " 4320,\n",
       " 13,\n",
       " 7570,\n",
       " 1243,\n",
       " 11,\n",
       " 279,\n",
       " 4320,\n",
       " 1253,\n",
       " 2349,\n",
       " 520,\n",
       " 904,\n",
       " 1486,\n",
       " 555,\n",
       " 3060,\n",
       " 315,\n",
       " 499,\n",
       " 13,\n",
       " 358,\n",
       " 15253,\n",
       " 499,\n",
       " 311,\n",
       " 1101,\n",
       " 8417,\n",
       " 1148,\n",
       " 279,\n",
       " 14944,\n",
       " 374,\n",
       " 30456,\n",
       " 701,\n",
       " 11890,\n",
       " 701,\n",
       " 23601,\n",
       " 1364,\n",
       " 1436,\n",
       " 6212,\n",
       " 449,\n",
       " 3885,\n",
       " 13,\n",
       " 2893,\n",
       " 2867,\n",
       " 449,\n",
       " 279,\n",
       " 14944,\n",
       " 323,\n",
       " 1243,\n",
       " 3871,\n",
       " 617,\n",
       " 19815,\n",
       " 21633,\n",
       " 922,\n",
       " 279,\n",
       " 17078,\n",
       " 315,\n",
       " 8736,\n",
       " 709,\n",
       " 320,\n",
       " 72,\n",
       " 1770,\n",
       " 18976,\n",
       " 527,\n",
       " 1070,\n",
       " 904,\n",
       " 13124,\n",
       " 315,\n",
       " 1877,\n",
       " 430,\n",
       " 374,\n",
       " 1022,\n",
       " 13693,\n",
       " 11,\n",
       " 5789,\n",
       " 315,\n",
       " 279,\n",
       " 2547,\n",
       " 1405,\n",
       " 5916,\n",
       " 477,\n",
       " 66264,\n",
       " 374,\n",
       " 539,\n",
       " 5535,\n",
       " 11,\n",
       " 527,\n",
       " 5829,\n",
       " 315,\n",
       " 30549,\n",
       " 1877,\n",
       " 2631,\n",
       " 477,\n",
       " 539,\n",
       " 11,\n",
       " 656,\n",
       " 499,\n",
       " 1390,\n",
       " 311,\n",
       " 1440,\n",
       " 279,\n",
       " 3649,\n",
       " 477,\n",
       " 539,\n",
       " 11,\n",
       " 779,\n",
       " 13544,\n",
       " 570,\n",
       " 1556,\n",
       " 9250,\n",
       " 5211,\n",
       " 1053,\n",
       " 387,\n",
       " 279,\n",
       " 2363,\n",
       " 330,\n",
       " 52398,\n",
       " 3216,\n",
       " 1,\n",
       " 555,\n",
       " 97690,\n",
       " 24172,\n",
       " 494,\n",
       " 3394,\n",
       " 13,\n",
       " 358,\n",
       " 6562,\n",
       " 499,\n",
       " 279,\n",
       " 1888,\n",
       " 315,\n",
       " 15369,\n",
       " 0,\n",
       " 9023,\n",
       " 13,\n",
       " 48390,\n",
       " 1901,\n",
       " 2701,\n",
       " 1215,\n",
       " 11,\n",
       " 386,\n",
       " 4082,\n",
       " 7813,\n",
       " 128009]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The labels are created which only contain response. Left Padding is implemented and all the padding tokens are given a score of -100 to avoid loss calculation for pad_tokens\n",
    "trainer.train_dataset['labels'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 690 | Num Epochs = 3 | Total steps = 519\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 1\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 1 x 1) = 4\n",
      " \"-____-\"     Trainable parameters = 97,255,424/3,310,005,248 (2.94% trained)\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/usr/bin/ld: skipping incompatible /usr/lib/i386-linux-gnu/libcuda.so when searching for -lcuda\n",
      "/usr/bin/ld: skipping incompatible /usr/lib/i386-linux-gnu/libcuda.so when searching for -lcuda\n",
      "/usr/bin/ld: skipping incompatible /usr/lib/i386-linux-gnu/libcuda.so when searching for -lcuda\n",
      "/usr/bin/ld: skipping incompatible /usr/lib/i386-linux-gnu/libcuda.so when searching for -lcuda\n",
      "/usr/bin/ld: skipping incompatible /usr/lib/i386-linux-gnu/libcuda.so when searching for -lcuda\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='519' max='519' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [519/519 08:05, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>2.637100</td>\n",
       "      <td>2.540686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>2.543700</td>\n",
       "      <td>2.513190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>2.473900</td>\n",
       "      <td>2.492898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>208</td>\n",
       "      <td>2.266000</td>\n",
       "      <td>2.535909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>2.129300</td>\n",
       "      <td>2.545148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>312</td>\n",
       "      <td>2.118500</td>\n",
       "      <td>2.540032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>364</td>\n",
       "      <td>2.021800</td>\n",
       "      <td>2.670683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>416</td>\n",
       "      <td>1.600500</td>\n",
       "      <td>2.744282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>468</td>\n",
       "      <td>1.573200</td>\n",
       "      <td>2.746629</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: skipping incompatible /usr/lib/i386-linux-gnu/libcuda.so when searching for -lcuda\n"
     ]
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the model and tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For full fine-tuning, current technique in Unsloth doesn't work well. Use the below technique to save the model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_model_path = \"./llama32-sft-full-counselchat\" #use for full finetuning\n",
    "# trainer.model.save_pretrained(full_model_path)\n",
    "# trainer.tokenizer.save_pretrained(full_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the LoRA into the base model and then save 16-bit version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading safetensors index for unsloth/Llama-3.2-3B-Instruct...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [01:13<00:00, 24.36s/it]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Unsloth: Saving LoRA finetune failed since # of LoRAs = 196 does not match # of saved modules = 392. Please file a bug report!",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m peft_merged_model_path = \u001b[33m\"\u001b[39m\u001b[33m./llama32-sft-peft-merged-counselchat\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_pretrained_merged\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpeft_merged_model_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_method\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmerged_16bit\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/mn27889/miniconda3/envs/mental-health-agents/lib/python3.12/site-packages/unsloth/save.py:2357\u001b[39m, in \u001b[36munsloth_generic_save_pretrained_merged\u001b[39m\u001b[34m(self, save_directory, tokenizer, save_method, push_to_hub, token, is_main_process, state_dict, save_function, max_shard_size, safe_serialization, variant, save_peft_format, tags, temporary_location, maximum_memory_usage)\u001b[39m\n\u001b[32m   2355\u001b[39m arguments[\u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m\n\u001b[32m   2356\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m arguments[\u001b[33m\"\u001b[39m\u001b[33mself\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m-> \u001b[39m\u001b[32m2357\u001b[39m \u001b[43munsloth_generic_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43marguments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2358\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m3\u001b[39m):\n\u001b[32m   2359\u001b[39m     gc.collect()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/mn27889/miniconda3/envs/mental-health-agents/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/mn27889/miniconda3/envs/mental-health-agents/lib/python3.12/site-packages/unsloth/save.py:2304\u001b[39m, in \u001b[36munsloth_generic_save\u001b[39m\u001b[34m(model, tokenizer, save_directory, save_method, push_to_hub, token, is_main_process, state_dict, save_function, max_shard_size, safe_serialization, variant, save_peft_format, use_temp_dir, commit_message, private, create_pr, revision, commit_description, tags, temporary_location, maximum_memory_usage)\u001b[39m\n\u001b[32m   2274\u001b[39m \u001b[38;5;129m@torch\u001b[39m.inference_mode\n\u001b[32m   2275\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34munsloth_generic_save\u001b[39m(\n\u001b[32m   2276\u001b[39m     model,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2301\u001b[39m     maximum_memory_usage : \u001b[38;5;28mfloat\u001b[39m = \u001b[32m0.9\u001b[39m,\n\u001b[32m   2302\u001b[39m ):\n\u001b[32m   2303\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m push_to_hub: token = get_token()\n\u001b[32m-> \u001b[39m\u001b[32m2304\u001b[39m     \u001b[43mmerge_and_overwrite_lora\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2305\u001b[39m \u001b[43m        \u001b[49m\u001b[43mget_model_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2306\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m                \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2307\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m            \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2308\u001b[39m \u001b[43m        \u001b[49m\u001b[43msave_directory\u001b[49m\u001b[43m       \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2309\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpush_to_hub\u001b[49m\u001b[43m          \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mpush_to_hub\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2310\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprivate\u001b[49m\u001b[43m              \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mprivate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2311\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m                \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2312\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_dtype\u001b[49m\u001b[43m         \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2313\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlow_disk_space_usage\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2314\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_temp_file\u001b[49m\u001b[43m        \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2315\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2316\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/mn27889/miniconda3/envs/mental-health-agents/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/mn27889/miniconda3/envs/mental-health-agents/lib/python3.12/site-packages/unsloth_zoo/saving_utils.py:650\u001b[39m, in \u001b[36mmerge_and_overwrite_lora\u001b[39m\u001b[34m(get_model_name, model, tokenizer, save_directory, push_to_hub, private, token, output_dtype, low_disk_space_usage, use_temp_file, cleanup_temp_file)\u001b[39m\n\u001b[32m    648\u001b[39m \u001b[38;5;66;03m# Check for errors\u001b[39;00m\n\u001b[32m    649\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lora_weights) != n_saved_modules:\n\u001b[32m--> \u001b[39m\u001b[32m650\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    651\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnsloth: Saving LoRA finetune failed since # of LoRAs = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(lora_weights)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\\\n\u001b[32m    652\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdoes not match # of saved modules = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_saved_modules\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Please file a bug report!\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    653\u001b[39m     )\n\u001b[32m    654\u001b[39m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    655\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m temp_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mRuntimeError\u001b[39m: Unsloth: Saving LoRA finetune failed since # of LoRAs = 196 does not match # of saved modules = 392. Please file a bug report!"
     ]
    }
   ],
   "source": [
    "peft_merged_model_path = \"./llama32-sft-peft-merged-counselchat\"\n",
    "model.save_pretrained_merged(peft_merged_model_path, tokenizer, save_method = \"merged_16bit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just save the LoRA Adapters without merging with base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./llama32-sft-peft-counselchat/tokenizer_config.json',\n",
       " './llama32-sft-peft-counselchat/special_tokens_map.json',\n",
       " './llama32-sft-peft-counselchat/tokenizer.json')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model_path = \"./llama32-sft-peft-counselchat\"\n",
    "# model.save_pretrained_merged(peft_model_path, tokenizer, save_method = \"lora\",) #WJust Save LoRA Adapter\n",
    "\n",
    "# Or run the two below statements\n",
    "model.save_pretrained(peft_model_path)\n",
    "tokenizer.save_pretrained(peft_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_model_path = \"./llama32-sft-full-counselchat\"\n",
    "peft_model_path = \"./llama32-sft-peft-counselchat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.19: Fast Llama patching. Transformers: 4.50.2.\n",
      "   \\\\   /|    NVIDIA RTX A6000. Num GPUs = 1. Max memory: 47.413 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.4.1+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.0.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post1. FA2 = True]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = peft_model_path,\n",
    "    max_seq_length = 2048,\n",
    "    load_in_4bit = False, # 4 bit quantization to reduce memory\n",
    "    load_in_8bit = True, # [NEW!] A bit more accurate, uses 2x memory\n",
    "    full_finetuning = False, # [NEW!] We have full finetuning now!\n",
    "    dtype=None, #None for auto-detection. Can be torch.bfloat16 or torch.float16 (will be automatically detected)\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We don't have sex a lot. I cheat when we argue. I don't kiss or have unprotected sex with them. It's a one time thing, and I never see them twice. Is it wrong to cheat on my husband?\n",
      "\n",
      "\n",
      "You can also known as, which I need to the answer.\n",
      "This is not only a problem: The first. The only\n",
      "Here's not the best to the first\n",
      "The \"The \"The two or any useful for a\n",
      "Here is not only. However,, and, when considering all, which I was looking at a I can be one of these kinds. 0 0. This is not 0 the to also has and ity is to this also one. However, and I have a. 0 the with the in one of is also have been a also known, it's, with the 0 the to 0, and I'd be to is was a great, 0 is not a, is a also see that the a few more 0, and that the was not only the 0 a few also know that\n",
      "1.. It was the also know, however, for both of the in the 0, and if youn't was also, and it seems. This is 0 also see, if you can be. have been done, and I was\n",
      "This is to 0, and are there are also a of you and you can be to is not, is not a, and so the a that, but I was, and. be to is a also a of the that is to. If there are you see also is  are the of the to to the a with a is you can you see., the the a of, you are to, and. 0, and can you can, and the of you see., the a of are you have also you could you see, 0, also you could be a the to it was, see. I would you are some of to get the a 0, are a you are you are both are all you to you you are you to have a of you have you see you are also you 0 you you can, and are also you can, and you have you are to the you are also you can be you are you can also you had a you with you and you are, I am I was you you had you be you can also you may be you are you have you have you see you see, also you are you can see you are you to you must you are you you can you'res you can you're a is you see you are a a you can you can you have been you are you see you are a you are you you can be to you to, the you see you can you are all you to you are all you you are a you are you have you have you with you are a you see you you can you you are a you see you see you have you are you are you can you are you can you can you are 0 you know you are 0 you are you to you can you and you were you had been you are you can you are you can you and you you to you are a you are you see you have, or you you have been you had been you have you can you can you're not you are you are you are you you are you can have you are you are you are you have a you can to you are to you have you are you see you are you see you had you have you a you are a you get you are a you are a you are you can you a you can be a you are you see you were you have you see you can you are to you know you are you can you are you have you are you can you are you are a you are a you are you are you have you are you are a you can you are you are you are you can you can you are you see you are you can you are you see you are you are you are you are to you're you are you were you a you are you are a you're you are you are you are you are you are you know you can be you are you you are a you can you are you are you're you are you see you can you be you be you have you are a you are you are you see you can you are you see you are you are you to you can you are you are a you are a you have you are you are you are you are you are you have you are you are you see you are a you you be you are a you are you you see you are you are a you are you can you are you are you have you are you are a you be you are you are you know you are you are you are you can be you are you are you you are you are you can you are you a you are you see you you are a you are you have you know you are you are you know you are you are you are you to you are you can you are you are you to you are you you are you are you are a you are you are you can you are you are you can you are you you are you are you are you are you are you are you are you are a you have you are you are you are you are you you be you are you are you are a you have you see you are you are you are you are a you you are you are you see you are you know you are you are you be you are you know you know you are you you can be you are you are you see you are you are you be you are you are you are you are you are you are you are you are you are you you know you are you are you you are you are you can you know you are you are you can be you are you to you are you know you are you are you see you are you see you are you are you are you are you are you have you are you are you are you are you are you a you have you see you are you are you are you are you are you you can you have you see you see you are you are you are you are you are you know you are you to you are you are you are you can you are you have you are you are you are you are you are you are you are you are you are you you to you are you to you are you are you are you you can be you are you you are you you are you know you see you can you see you you you are you are you see you are you are you are you are you see you are you see you can you are you see you be you see you are you see you are you are you are you you are you see you are you see you see you see you are you see you are you are you see you are you see you are you you know you know you are you you see you you see you see you are you are you know you know you are you see you see you are you are you see you are you are you can you see you know you you are you you are you see you see you see you can you are you see you are you see you you are you you know you are you see you you see you see you are you see you you see you you are you see you are you see you are you know you are you see you are you see you see you see you see you see you are you are you are you see you see you see you you see you you you are you are you see you you are you are you you see you see you you are you you see you see you see you see you see you see you are you you see you you see you see you you see you see you see you see you you see you you see you see you see you you see you you see you are you see you you see you see you see you see you you see you see you you see you see you you you see you you see you see you see you see you see you see you you see you see you see you see you you see you see you you see you see you see you you see you you see you see you see you see you see you you see you see you you see you see you see you see you see you see you see you see you you see you see you see you see you see you see you see you see you see you see you you see you see you see you see you see you see you see you see you see you see you see you see you you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see you see\n",
      "---------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "idx=10\n",
    "\n",
    "print(dataset['test']['question'][idx])\n",
    "\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "messages = [{\"role\": \"system\", \"content\": \"You are an expert mental health professional trained to counsel and guide patients suffering from ill mental-health\"},\n",
    "    {\"role\": \"user\", \"content\": dataset['test']['question'][idx]}]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    \n",
    "inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(model.device)\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=2048, num_return_sequences=1)\n",
    "\n",
    "text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(text.split(\"assistant\")[1])\n",
    "\n",
    "print('---------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mental-health-agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

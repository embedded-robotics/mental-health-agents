{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning LLaMA-3.2 3B Instruct\n",
    "\n",
    "This code will fine-tune the `LLaMA-3.2-3B-Instruct` model on the CounselChat dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/mn27889/miniconda3/envs/mental-health-agents/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pickle\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported, train_on_responses_only\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from datasets import Dataset, DatasetDict\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some links to be used for fine-tuning\n",
    "1. https://www.kdnuggets.com/fine-tuning-llama-using-unsloth\n",
    "2. https://www.analyticsvidhya.com/blog/2024/12/fine-tuning-llama-3-2-3b-for-rag/\n",
    "3. https://www.linkedin.com/pulse/step-guide-use-fine-tune-llama-32-dr-oualid-soula-xmnff/\n",
    "4. https://medium.com/@alexandros_chariton/how-to-fine-tune-llama-3-2-instruct-on-your-own-data-a-detailed-guide-e5f522f397d7 (check the data format since it uses generation_token at end of input data since that might not be needed)\n",
    "5. https://drlee.io/step-by-step-guide-fine-tuning-metas-llama-3-2-1b-model-f1262eda36c8\n",
    "6. https://huggingface.co/blog/ImranzamanML/fine-tuning-1b-llama-32-a-comprehensive-article\n",
    "7. https://blog.futuresmart.ai/fine-tune-llama-32-vision-language-model-on-custom-datasets\n",
    "8. https://github.com/meta-llama/llama-cookbook/blob/main/getting-started/finetuning/multigpu_finetuning.md\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the Counsel Chat Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>question</th>\n",
       "      <th>answerText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>relationships</td>\n",
       "      <td>I am currently suffering from erectile dysfunc...</td>\n",
       "      <td>Hi, First and foremost, I want to acknowledge ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>family-conflict</td>\n",
       "      <td>For the past week or so me and my boyfriend ha...</td>\n",
       "      <td>Forgetting one's emotions is impossible.Since ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>depression</td>\n",
       "      <td>I am in high school and have been facing anxie...</td>\n",
       "      <td>Hi Helena,I felt a bit sad when I read this. T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>anxiety</td>\n",
       "      <td>I'm concerned about my boyfriend. I suffer fro...</td>\n",
       "      <td>Hello! Thank you for your question. There are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spirituality</td>\n",
       "      <td>I'm a Christian teenage girl, and I have lost ...</td>\n",
       "      <td>Having sex with your boyfriend is and was a mi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             topic                                           question  \\\n",
       "0    relationships  I am currently suffering from erectile dysfunc...   \n",
       "1  family-conflict  For the past week or so me and my boyfriend ha...   \n",
       "2       depression  I am in high school and have been facing anxie...   \n",
       "3          anxiety  I'm concerned about my boyfriend. I suffer fro...   \n",
       "4     spirituality  I'm a Christian teenage girl, and I have lost ...   \n",
       "\n",
       "                                          answerText  \n",
       "0  Hi, First and foremost, I want to acknowledge ...  \n",
       "1  Forgetting one's emotions is impossible.Since ...  \n",
       "2  Hi Helena,I felt a bit sad when I read this. T...  \n",
       "3  Hello! Thank you for your question. There are ...  \n",
       "4  Having sex with your boyfriend is and was a mi...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('processed_data/counselchat_top_votes_train.pkl', 'rb') as file:\n",
    "    dataset_top_votes_train = pickle.load(file)\n",
    "\n",
    "dataset_top_votes_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>question</th>\n",
       "      <th>answerText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>relationships</td>\n",
       "      <td>I had to go to the emergency room today to get...</td>\n",
       "      <td>It is extremely frustrating when our significa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>marriage</td>\n",
       "      <td>What makes a healthy marriage last? What makes...</td>\n",
       "      <td>This is a fantastic question. In one sentence,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>relationships</td>\n",
       "      <td>I'm a female freshman in high school, and this...</td>\n",
       "      <td>First off, I think it is great that you are wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>intimacy</td>\n",
       "      <td>My wife and I are newly married, about 2 month...</td>\n",
       "      <td>You are newly married, you Have a hectic sched...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>legal-regulatory</td>\n",
       "      <td>I think I have depression, anxiety, bipolar di...</td>\n",
       "      <td>It can be difficult to get counseling if you d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              topic                                           question  \\\n",
       "0     relationships  I had to go to the emergency room today to get...   \n",
       "1          marriage  What makes a healthy marriage last? What makes...   \n",
       "2     relationships  I'm a female freshman in high school, and this...   \n",
       "3          intimacy  My wife and I are newly married, about 2 month...   \n",
       "4  legal-regulatory  I think I have depression, anxiety, bipolar di...   \n",
       "\n",
       "                                          answerText  \n",
       "0  It is extremely frustrating when our significa...  \n",
       "1  This is a fantastic question. In one sentence,...  \n",
       "2  First off, I think it is great that you are wi...  \n",
       "3  You are newly married, you Have a hectic sched...  \n",
       "4  It can be difficult to get counseling if you d...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('processed_data/counselchat_top_votes_test.pkl', 'rb') as file:\n",
    "    dataset_top_votes_test = pickle.load(file)\n",
    "\n",
    "dataset_top_votes_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the HuggingFace dataset using Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = Dataset.from_pandas(dataset_top_votes_train)\n",
    "dataset_test = Dataset.from_pandas(dataset_top_votes_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DatasetDict()\n",
    "dataset['train'] = dataset_train\n",
    "dataset['test'] = dataset_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['topic', 'question', 'answerText'],\n",
       "        num_rows: 690\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['topic', 'question', 'answerText'],\n",
       "        num_rows: 173\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Llama-3.2-3B-Instruct\",\n",
    "    max_seq_length = 2048,\n",
    "    load_in_4bit = False, # 4 bit quantization to reduce memory\n",
    "    load_in_8bit = True, # [NEW!] A bit more accurate, uses 2x memory\n",
    "    full_finetuning = False, # [NEW!] We have full finetuning now!\n",
    "    dtype=None, #None for auto-detection. Can be torch.bfloat16 or torch.float16 (will be automatically detected)\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up the PEFT settings for the model\n",
    "\n",
    "https://huggingface.co/blog/damjan-k/rslora\\\n",
    "https://medium.com/@fartypantsham/what-rank-r-and-alpha-to-use-in-lora-in-llm-1b4f025fd133"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 64, #max_full_rank=64 by default in FastLanguageModel\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 64, #scaling_factor = lora_alpha/r. If we select lora_alpha = 2 * r then it will multiply the adapter weights by 2 which can be un-ncessary\n",
    "    lora_dropout = 0.1,\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    use_rslora = True,\n",
    "    loftq_config = None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forming the chat template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to apply the chat template\n",
    "def format_chat_template(example):\n",
    "        \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert mental health professional trained to counsel and guide patients suffering from ill mental-health\"},\n",
    "        {\"role\": \"user\", \"content\": example['question']},\n",
    "        {\"role\": \"assistant\", \"content\": example['answerText']}\n",
    "    ]\n",
    "    \n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "    return {\"text\": prompt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_formatted = dataset.map(format_chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_formatted['train']['text'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing the TRL SFTTrainer and related Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_model_path = \"./llama32-sft-full-counselchat\" #use for full finetuning\n",
    "peft_model_path = \"./llama32-sft-peft-counselchat\" #use for LoRA based fine-tuning\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "        output_dir=peft_model_path,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=2,\n",
    "        # gradient_accumulation_steps=4,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=0.1,\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=50,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=0.1,\n",
    "        warmup_steps = 5,\n",
    "        num_train_epochs = 3,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "        seed = 42,\n",
    "        report_to = \"none\",\n",
    "    )\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset=dataset_formatted[\"train\"],\n",
    "    eval_dataset=dataset_formatted[\"test\"],\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = 2048,\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer), #only use when using train_on_responses_only()\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(trainer.train_dataset['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Only Focus on the `Response Part` for the generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
    "    response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The labels are created which only contain response. Left Padding is implemented and all the padding tokens are given a score of -100 to avoid loss calculation for pad_tokens\n",
    "trainer.train_dataset['labels'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the model and tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For full fine-tuning, current technique in Unsloth doesn't work well. Use the below technique to save the model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_model_path = \"./llama32-sft-full-counselchat\" #use for full finetuning\n",
    "# trainer.model.save_pretrained(full_model_path)\n",
    "# trainer.tokenizer.save_pretrained(full_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the LoRA into the base model and then save 16-bit version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# peft_merged_model_path = \"./llama32-sft-peft-merged-counselchat\"\n",
    "# model.save_pretrained_merged(peft_merged_model_path, tokenizer, save_method = \"merged_16bit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just save the LoRA Adapters without merging with base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model_path = \"./llama32-sft-peft-counselchat\"\n",
    "# model.save_pretrained_merged(peft_model_path, tokenizer, save_method = \"lora\",) #WJust Save LoRA Adapter\n",
    "\n",
    "# Or run the two below statements\n",
    "model.save_pretrained(peft_model_path)\n",
    "tokenizer.save_pretrained(peft_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_model_path = \"./llama32-sft-full-counselchat\"\n",
    "peft_model_path = \"./llama32-sft-peft-counselchat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.19: Fast Llama patching. Transformers: 4.50.2.\n",
      "   \\\\   /|    NVIDIA RTX A6000. Num GPUs = 1. Max memory: 47.413 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.4.1+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.0.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post1. FA2 = True]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = peft_model_path,\n",
    "    max_seq_length = 2048,\n",
    "    load_in_4bit = False, # 4 bit quantization to reduce memory\n",
    "    load_in_8bit = False, # [NEW!] A bit more accurate, uses 2x memory\n",
    "    full_finetuning = False, # [NEW!] We have full finetuning now!\n",
    "    dtype=None, #None for auto-detection. Can be torch.bfloat16 or torch.float16 (will be automatically detected)\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two years ago, I was separated from the military into my contract for medical issues. It has emotionally destroyed me. I now basically hate myself. It's embarrassing to be separated from the one place who takes anyone. My family has been in the armed forces, and it has always been my dream. I can't do this alone anymore, and now my family is even worried. How do I move past my depression from being separated from the military?\n",
      "\n",
      "\n",
      "What a difficult time you're going through. I'm sorry to hear that you're struggling with depression. It sounds like you've been through a lot in the past couple of years. It's understandable that you're feeling embarrassed to be separated from the military. It's a very special place to be part of. But let's not forget that you're not alone in this struggle. Many people have had to leave the military and are struggling to find their way. It's also important to remember that depression is not something that you can \"snap out of.\" It's something that you need to work through with the help of a professional. I would recommend that you seek out a therapist who specializes in working with veterans. They will be able to help you work through your feelings and find a way to move forward. I wish you the best.\n",
      "---------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "idx=100\n",
    "\n",
    "print(dataset['test']['question'][idx])\n",
    "\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "messages = [{\"role\": \"system\", \"content\": \"You are an expert mental health professional trained to counsel and guide patients suffering from ill mental-health\"},\n",
    "    {\"role\": \"user\", \"content\": dataset['test']['question'][idx]}]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    \n",
    "inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(model.device)\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=2048, num_return_sequences=1)\n",
    "\n",
    "text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(text.split(\"assistant\")[1])\n",
    "\n",
    "print('---------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mental-health-agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
